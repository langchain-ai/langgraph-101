{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Advanced Concepts: Middleware & Human-in-the-Loop\n",
        "\n",
        "Welcome to LangGraph Advanced Concepts! This notebook builds on the foundations from LangGraph 101 and introduces two powerful patterns for production agents.\n",
        "\n",
        "**What you'll learn:**\n",
        "- **Human-in-the-Loop** - Pause agents for human review and approval\n",
        "- **Middleware** - Modify agent behavior at key points in execution\n",
        "- **Tool Review** - Add approval workflows to sensitive tools\n",
        "- **Dynamic Behavior** - Adapt agent responses based on context\n",
        "\n",
        "**Prerequisites:** Complete `langgraph_101.ipynb` \n",
        "</br>\n",
        "</br>\n",
        "\n",
        "---\n",
        "</br>\n",
        "\n",
        "> **Note:** These patterns are essential for production agents where safety, compliance, and user control are critical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Let's quickly set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
        "\n",
        "# Initialize model\n",
        "model = init_chat_model(\"openai:gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Human-in-the-Loop with Interrupts\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Imagine you're building an agent that can send emails or make purchases. You don't want it to take these actions automatically - you want human approval first!\n",
        "\n",
        "**Human-in-the-loop** lets you:\n",
        "- Pause execution for review\n",
        "- Approve, reject, or edit actions\n",
        "- Add safety controls to sensitive operations\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. Agent encounters an `interrupt()` - execution pauses\n",
        "2. System surfaces information to human\n",
        "3. Human provides input (approve/reject/edit)\n",
        "4. Agent resumes with `Command(resume=...)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Simple Approval Workflow\n",
        "\n",
        "Let's start with a simple example - asking for approval before sending an email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool created successfully!\n",
            "Tool name: send_email\n",
            "Tool description: Send an email to a recipient.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.types import interrupt\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def send_email(to: str, subject: str, body: str) -> str:\n",
        "    \"\"\"Send an email to a recipient.\"\"\"\n",
        "    \n",
        "    # Pause for human approval\n",
        "    approval = interrupt({\n",
        "        \"action\": \"send_email\",\n",
        "        \"to\": to,\n",
        "        \"subject\": subject,\n",
        "        \"body\": body,\n",
        "        \"message\": \"Do you want to send this email?\"\n",
        "    })\n",
        "    \n",
        "    if approval.get(\"approved\"): # Will be true if accepted, false if declined\n",
        "        # In production, this would actually send the email\n",
        "        return f\" Email sent to {to} with subject '{subject}'\"\n",
        "    else:\n",
        "        return \"Email cancelled by user\"\n",
        "\n",
        "# Test the tool directly\n",
        "print(\"Tool created successfully!\")\n",
        "print(f\"Tool name: {send_email.name}\")\n",
        "print(f\"Tool description: {send_email.description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating an Agent with Human-in-the-Loop\n",
        "\n",
        "Now let's create an agent that uses this tool. **Remember:** Interrupts require a checkpointer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import uuid\n",
        "\n",
        "# Create checkpointer for persistence\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# Create agent with the email tool\n",
        "agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[send_email],\n",
        "    prompt=\"You are a helpful email assistant. When asked to send emails, use the send_email tool.\",\n",
        "    checkpointer=checkpointer  # Required for interrupts\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running Until Interrupt\n",
        "\n",
        "Let's run the agent and see it pause for approval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent paused for approval\n",
            "\n",
            "Interrupt details:\n",
            "  To: alice@example.com\n",
            "  Subject: Meeting Tomorrow\n",
            "  Body: Let's meet at 3pm.\n",
            "  Message: Do you want to send this email?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Create a unique thread for this conversation\n",
        "thread_id = str(uuid.uuid4())\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Run the agent and see it pause for approval\n",
        "result = agent.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Send an email to alice@example.com with subject 'Meeting Tomorrow' and body 'Let's meet at 3pm.'\")]\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Check if we hit an interrupt\n",
        "\n",
        "if \"__interrupt__\" in result:\n",
        "    print(\"Agent paused for approval\\n\")\n",
        "\n",
        "    interrupt_info = result[\"__interrupt__\"][0]\n",
        "\n",
        "    print(\"Interrupt details:\")\n",
        "    print(f\"  To: {interrupt_info.value['to']}\")\n",
        "    print(f\"  Subject: {interrupt_info.value['subject']}\")\n",
        "    print(f\"  Body: {interrupt_info.value['body']}\")\n",
        "    print(f\"  Message: {interrupt_info.value['message']}\")\n",
        "else:\n",
        "    print(\"Agent completed without interrupt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resuming with Approval\n",
        "\n",
        "Now let's approve the email and let the agent continue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final response:\n",
            "The email has been sent to alice@example.com with the subject \"Meeting Tomorrow\" and the body \"Let's meet at 3pm.\"\n"
          ]
        }
      ],
      "source": [
        "from langgraph.types import Command\n",
        "\n",
        "# Resume with approval\n",
        "result = agent.invoke(\n",
        "    Command(resume={\"approved\": True}),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Print the final response\n",
        "print(\"Final response:\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Try Rejecting the Email\n",
        "\n",
        "Run the cells again, but this time reject the email by passing `{\"approved\": False}`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final response:\n",
            "It seems like the email was canceled. Would you like me to try sending it again or assist you with something else?\n"
          ]
        }
      ],
      "source": [
        "# New thread for rejection example\n",
        "thread_id_2 = str(uuid.uuid4())\n",
        "config_2 = {\"configurable\": {\"thread_id\": thread_id_2}}\n",
        "\n",
        "# Run until interrupt\n",
        "result = agent.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Send an email to bob@example.com saying 'Hello!'\")]\n",
        "    },\n",
        "    config=config_2\n",
        ")\n",
        "\n",
        "# Resume with rejection\n",
        "result = agent.invoke(\n",
        "    Command(resume={\"approved\": False}),  # Reject the email\n",
        "    config=config_2\n",
        ")\n",
        "\n",
        "print(\"Final response:\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Advanced Pattern - Edit Before Execution\n",
        "\n",
        "Sometimes you want to **edit** the tool call, not just approve/reject it. Let's enhance our tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def send_email_v2(to: str, subject: str, body: str) -> str:\n",
        "    \"\"\"Send an email to a recipient.\"\"\"\n",
        "    \n",
        "    # Pause for human review\n",
        "    response = interrupt({\n",
        "        \"action\": \"send_email\",\n",
        "        \"to\": to,\n",
        "        \"subject\": subject,\n",
        "        \"body\": body,\n",
        "        \"message\": \"Review this email. You can approve, reject, or edit it.\"\n",
        "    })\n",
        "    \n",
        "    # Handle different response types\n",
        "    if response[\"type\"] == \"approve\":\n",
        "        return f\"Email sent to {to} with subject '{subject}'\"\n",
        "\n",
        "    elif response[\"type\"] == \"reject\":\n",
        "        return \"Email cancelled\"\n",
        "\n",
        "    elif response[\"type\"] == \"edit\":\n",
        "        # Use edited values\n",
        "        to = response.get(\"to\", to)\n",
        "        subject = response.get(\"subject\", subject)\n",
        "        body = response.get(\"body\", body)\n",
        "        return f\"\"\"Email sent with edits:\n",
        "                To: {to}\n",
        "                Subject: {subject}\n",
        "                Body: {body}\"\"\"\n",
        "    \n",
        "    return \"Unknown response\"\n",
        "\n",
        "# Create new agent with enhanced tool\n",
        "agent_v2 = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[send_email_v2],\n",
        "    prompt=\"You are a helpful email assistant.\",\n",
        "    checkpointer=MemorySaver()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paused for review...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run and edit the email\n",
        "thread_id_3 = str(uuid.uuid4())\n",
        "config_3 = {\"configurable\": {\"thread_id\": thread_id_3}}\n",
        "\n",
        "# Run until interrupt\n",
        "result = agent_v2.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Send an email to team@example.com about the meeting\")]\n",
        "    },\n",
        "    config=config_3\n",
        ")\n",
        "\n",
        "print(\"Paused for review...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets edit the email subject to make it URGENT meeting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final response:\n",
            "I have sent the email to team@example.com with the subject \"URGENT: Meeting Today at 2pm\" and included additional details in the body. If you need anything else, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Resume with edits\n",
        "result = agent_v2.invoke(\n",
        "    Command(resume={\n",
        "        \"type\": \"edit\",\n",
        "        \"subject\": \"URGENT: Meeting Today at 2pm\",  # We have edited the email subject\n",
        "        \"body\": \"This is the edited email body with more details.\"\n",
        "    }),\n",
        "    config=config_3\n",
        ")\n",
        "\n",
        "print(\"Final response:\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Introduction to Middleware\n",
        "\n",
        "**Middleware** provides fine-grained control over the agent loop. It lets you:\n",
        "- Inspect state before/after model calls\n",
        "- Modify model requests dynamically\n",
        "- Add custom logic at key execution points\n",
        "\n",
        "### The Agent Loop\n",
        "\n",
        "```\n",
        "Input --> [before_model] --> [modify_model_request] --> Model --> [after_model] --> Tools --> ...\n",
        "```\n",
        "\n",
        "Middleware hooks into this loop:\n",
        "- **`before_model`** - Runs before model execution, can update state\n",
        "- **`modify_model_request`** - Modifies the request (prompt, model, tools)\n",
        "- **`after_model`** - Runs after model execution, before tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Dynamic System Prompt\n",
        "\n",
        "Let's create middleware that changes the system prompt based on the user's role:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware.types import modify_model_request, AgentState, ModelRequest\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import TypedDict\n",
        "\n",
        "# Define context schema\n",
        "class Context(TypedDict):\n",
        "    user_role: str\n",
        "\n",
        "# Create middleware using decorator\n",
        "@modify_model_request\n",
        "def dynamic_prompt_middleware(request: ModelRequest, state: AgentState, runtime: Runtime[Context]) -> ModelRequest:\n",
        "    \"\"\"Adjust system prompt based on user role.\"\"\"\n",
        "    \n",
        "    user_role = runtime.context.get(\"user_role\", \"general\")\n",
        "    \n",
        "    if user_role == \"expert\":\n",
        "        request.system_prompt = \"You are an AI assistant for experts. Provide detailed technical responses with code examples.\"\n",
        "    elif user_role == \"beginner\":\n",
        "        request.system_prompt = \"You are an AI assistant for beginners. Explain concepts simply, avoid jargon.\"\n",
        "    else:\n",
        "        request.system_prompt = \"You are a helpful AI assistant.\"\n",
        "    \n",
        "    return request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def explain_concept(concept: str) -> str:\n",
        "    \"\"\"Explain a programming concept.\"\"\"\n",
        "    explanations = {\n",
        "        \"async\": \"Asynchronous programming allows code to run without blocking.\",\n",
        "        \"recursion\": \"Recursion is when a function calls itself.\"\n",
        "    }\n",
        "    return explanations.get(concept.lower(), \"Concept not found.\")\n",
        "\n",
        "# Create agent with middleware\n",
        "agent_with_middleware = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[explain_concept],\n",
        "    middleware=[dynamic_prompt_middleware],\n",
        "    context_schema=Context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Different User Roles\n",
        "\n",
        "Let's see how the agent responds differently based on user role:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "EXPERT USER\n",
            "==================================================\n",
            "Asynchronous programming is a programming paradigm that allows for the execution of operations without blocking the main thread of execution. This means that while one operation is waiting for a response (like reading from a database or making an API call), other operations can continue running. This approach is particularly useful in applications that require high performance and responsiveness, such as web applications, where a user interface cannot be stalled while waiting for a resource.\n",
            "\n",
            "### Key Features of Asynchronous Programming:\n",
            "\n",
            "1. **Non-blocking Calls**: Instead of waiting for a resource to become available, asynchronous programming enables a program to continue executing subsequent code.\n",
            "\n",
            "2. **Callbacks**: Functions can be executed once a task is complete, expressed through callback functions that handle the result of the asynchronous operation.\n",
            "\n",
            "3. **Promises**: A Promise is an object that represents a value which may be available now, or in the future, or never. It acts as a placeholder for an eventual result.\n",
            "\n",
            "4. **Async/Await**: Introduced in ES2017, async/await is a syntactic sugar built on top of Promises, making asynchronous code look much like synchronous code, thus improving readability.\n",
            "\n",
            "### Example of Asynchronous Programming\n",
            "\n",
            "Here's an example in JavaScript using `async/await`:\n",
            "\n",
            "```javascript\n",
            "// Simulating a function that returns a promise which resolves after 2 seconds\n",
            "function fetchData() {\n",
            "  return new Promise((resolve) => {\n",
            "    setTimeout(() => {\n",
            "      resolve(\"Data received\");\n",
            "    }, 2000);\n",
            "  });\n",
            "}\n",
            "\n",
            "// Async function to fetch data\n",
            "async function getData() {\n",
            "  console.log(\"Fetching data...\");\n",
            "\n",
            "  const result = await fetchData(); // Execution waits for fetchData to resolve\n",
            "  console.log(result); // This will run after the promise resolves\n",
            "\n",
            "  console.log(\"Data processing...\");\n",
            "}\n",
            "\n",
            "// Call the async function\n",
            "getData();\n",
            "\n",
            "console.log(\"This line runs while waiting for data to be fetched.\");\n",
            "```\n",
            "\n",
            "### Explanation of the Example:\n",
            "\n",
            "1. **fetchData Function**: Creates a promise that simulates data fetching, resolving after 2 seconds.\n",
            "2. **Async getData Function**: Uses `await` to pause execution until `fetchData` resolves.\n",
            "3. While `fetchData` is waiting, `\"This line runs while waiting for data to be fetched.\"` is logged to the console, illustrating that other code can run in the meantime.\n",
            "\n",
            "### Summary\n",
            "\n",
            "Asynchronous programming is crucial for creating responsive applications. By using concepts like callbacks, promises, and async/await, developers can write non-blocking code that handles tasks efficiently without freezing the user interface.\n",
            "\n",
            "==================================================\n",
            "BEGINNER USER\n",
            "==================================================\n",
            "Asynchronous programming allows parts of your code to run at the same time, without making other parts wait. Think of it like ordering a meal at a restaurant. You place your order and while waiting for your food, you can chat with friends instead of just sitting quietly. \n",
            "\n",
            "In programming, this means that while one task is waiting for a slow operation (like downloading a file or fetching data from the internet), your program can continue doing other things instead of pausing. This leads to faster and more efficient applications.\n"
          ]
        }
      ],
      "source": [
        "# Expert user\n",
        "print(\"=\" * 50)\n",
        "print(\"EXPERT USER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result = agent_with_middleware.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
        "    context={\"user_role\": \"expert\"}\n",
        ")\n",
        "print(result[\"messages\"][-1].content)\n",
        "print()\n",
        "\n",
        "# Beginner user\n",
        "print(\"=\" * 50)\n",
        "print(\"BEGINNER USER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result = agent_with_middleware.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
        "    context={\"user_role\": \"beginner\"}\n",
        ")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  ### Example 2: Custom Middleware - Request Logger\n",
        "\n",
        "  Middleware lets you hook into the agent loop and see what's happening at each step. This is incredibly useful for debugging and understanding how your agent works.\n",
        "\n",
        "  **The Agent Loop:**\n",
        "  User Input --> [before_model] --> [modify_model_request] --> Model --> [after_model] --> Tools --> ...\n",
        "\n",
        "  **What we'll build:**\n",
        "  A logger that prints information at each step:\n",
        "  - **Before model** - How many messages are in the conversation?\n",
        "  - **Model request** - Which model and tools are being used?\n",
        "  - **After model** - Did the model call a tool or give a final answer?\n",
        "\n",
        "  This is like adding debug `print()` statements, but in a clean, reusable way!\n",
        "\n",
        "  Let's create middleware that logs model requests for debugging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import AgentMiddleware\n",
        "from typing import Any\n",
        "\n",
        "class RequestLoggerMiddleware(AgentMiddleware):\n",
        "    \"\"\"Logs all model requests for debugging.\"\"\"\n",
        "    \n",
        "    name = \"request_logger\"\n",
        "    \n",
        "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
        "        \"\"\"Log before model execution.\"\"\"\n",
        "        message_count = len(state.get(\"messages\", []))\n",
        "        print(f\"[BEFORE MODEL] Processing {message_count} messages\")\n",
        "        return None  # Don't modify state\n",
        "    \n",
        "    def modify_model_request(self, request: ModelRequest, state: AgentState) -> ModelRequest:\n",
        "        \"\"\"Log the model request details.\"\"\"\n",
        "        print(f\"  [MODEL REQUEST]\")\n",
        "        print(f\"   Model: {request.model if hasattr(request, 'model') else 'default'}\")\n",
        "        print(f\"   Tools available: {len(request.tools) if request.tools else 0}\")\n",
        "        return request\n",
        "    \n",
        "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
        "        \"\"\"Log after model execution.\"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "            print(f\" [AFTER MODEL] Model requested {len(last_message.tool_calls)} tool call(s)\")\n",
        "        else:\n",
        "            print(f\" [AFTER MODEL] Model provided final response\")\n",
        "        return None  # Don't modify state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create agent with logger middleware\n",
        "agent_with_logger = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[explain_concept],\n",
        "    middleware=[RequestLoggerMiddleware()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  ### What to Expect\n",
        "\n",
        "  When we run the agent with the logger, you'll see the execution flow in real-time:\n",
        "\n",
        "  **First iteration:**\n",
        "  1. `[BEFORE MODEL]` - Shows how many messages we're starting with\n",
        "  2. `[MODEL REQUEST]` - Shows which model and tools are available\n",
        "  3. `[AFTER MODEL]` - The model decides to call the `explain_concept` tool\n",
        "\n",
        "  **Second iteration (after tool execution):**\n",
        "  1. `[BEFORE MODEL]` - Now we have more messages (including tool result)\n",
        "  2. `[MODEL REQUEST]` - Model info again\n",
        "  3. `[AFTER MODEL]` - Model provides the final answer (no more tools needed)\n",
        "\n",
        "  This gives you a detailed view into your agent's decision-making process\n",
        "\n",
        "  Let's run it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "RUNNING AGENT WITH LOGGER\n",
            "==================================================\n",
            "\n",
            "[BEFORE MODEL] Processing 1 messages\n",
            "  [MODEL REQUEST]\n",
            "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x137b29a70> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x137b28050> root_client=<openai.OpenAI object at 0x137b289d0> root_async_client=<openai.AsyncOpenAI object at 0x137b28180> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "   Tools available: 1\n",
            " [AFTER MODEL] Model requested 1 tool call(s)\n",
            "[BEFORE MODEL] Processing 3 messages\n",
            "  [MODEL REQUEST]\n",
            "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x137b29a70> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x137b28050> root_client=<openai.OpenAI object at 0x137b289d0> root_async_client=<openai.AsyncOpenAI object at 0x137b28180> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "   Tools available: 1\n",
            " [AFTER MODEL] Model provided final response\n",
            "\n",
            "==================================================\n",
            "FINAL RESPONSE\n",
            "==================================================\n",
            "Recursion is a programming concept where a function calls itself in order to solve a problem. This technique is often used to break down complex problems into simpler subproblems. \n",
            "\n",
            "A recursive function typically has two main components:\n",
            "\n",
            "1. **Base Case**: This is the condition under which the recursion stops. It prevents the function from calling itself indefinitely, which would lead to a stack overflow.\n",
            "\n",
            "2. **Recursive Case**: This is where the function calls itself with modified arguments to gradually move towards the base case.\n",
            "\n",
            "Recursion is commonly used in algorithms for tasks such as traversing data structures (like trees), searching, and sorting. However, it's important to be cautious with recursion, as excessive recursion can lead to performance issues or stack overflow errors.\n"
          ]
        }
      ],
      "source": [
        "# Run and observe the logs\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"RUNNING AGENT WITH LOGGER\")\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "result = agent_with_logger.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain recursion\"}]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"FINAL RESPONSE\")\n",
        "print(\"=\" * 50)\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Combining Middleware and Human-in-the-loop\n",
        "\n",
        "Let's combine human-in-the-loop AND middleware for a production-ready agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sensitive tool that needs approval\n",
        "@tool\n",
        "def delete_database(database_name: str) -> str:\n",
        "    \"\"\"Delete a database. THIS IS DANGEROUS!\"\"\"\n",
        "    \n",
        "    response = interrupt({\n",
        "        \"action\": \"delete_database\",\n",
        "        \"database_name\": database_name,\n",
        "        \"warning\": \"This will permanently delete the database!\",\n",
        "        \"message\": \"Are you absolutely sure?\"\n",
        "    })\n",
        "    \n",
        "    if response.get(\"confirmed\"):\n",
        "        return f\"Database '{database_name}' has been deleted (simulation)\"\n",
        "    else:\n",
        "        return \"Database deletion cancelled\"\n",
        "\n",
        "# Middleware to track dangerous operations\n",
        "class SafetyMiddleware(AgentMiddleware):\n",
        "    \"\"\"Add safety checks and logging.\"\"\"\n",
        "    \n",
        "    name = \"safety_checker\"\n",
        "    \n",
        "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
        "        \"\"\"Check for dangerous tool calls.\"\"\"\n",
        "        last_message = state[\"messages\"][-1]\n",
        "        \n",
        "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "            for tool_call in last_message.tool_calls:\n",
        "                if \"delete\" in tool_call[\"name\"].lower():\n",
        "                    print(\"   [SAFETY] Dangerous operation detected!\")\n",
        "                    print(f\"   Tool: {tool_call['name']}\")\n",
        "                    print(f\"   Args: {tool_call['args']}\")\n",
        "        \n",
        "        return None\n",
        "\n",
        "# Create production agent\n",
        "production_agent = create_agent(\n",
        "    model=\"openai:gpt-4o-mini\",\n",
        "    tools=[delete_database],\n",
        "    middleware=[SafetyMiddleware()],\n",
        "    checkpointer=MemorySaver()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  ### What to Expect: Layered Safety in Action\n",
        "\n",
        "  When we attempt a dangerous operation, you'll see **both** safety mechanisms activate:\n",
        "\n",
        "  **Layer 1 - Middleware Detection:**\n",
        "  - `[SAFETY] Dangerous operation detected!` - Middleware spots the delete operation\n",
        "  - Logs the tool name and arguments for audit trails\n",
        "\n",
        "  **Layer 2 - Human Approval (Interrupt):**\n",
        "  - Agent execution pauses at the `interrupt()`\n",
        "  - Warning message displayed to human reviewer\n",
        "  - Execution won't continue until explicit approval\n",
        "\n",
        "  **This is defense-in-depth:** Middleware monitors ALL operations, while interrupts enforce human approval for critical actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DANGEROUS OPERATION ATTEMPT\n",
            "==================================================\n",
            "\n",
            "   [SAFETY] Dangerous operation detected!\n",
            "   Tool: delete_database\n",
            "   Args: {'database_name': 'production_db'}\n",
            "\n",
            "  Human approval required:\n",
            "   This will permanently delete the database!\n",
            "   Database: production_db\n",
            "\n",
            "(In a real app, a human would review this before proceeding)\n"
          ]
        }
      ],
      "source": [
        "# Test the combined pattern\n",
        "thread_id_4 = str(uuid.uuid4())\n",
        "config_4 = {\"configurable\": {\"thread_id\": thread_id_4}}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"DANGEROUS OPERATION ATTEMPT\")\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Run until interrupt\n",
        "result = production_agent.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"Delete the production_db database\")]\n",
        "    },\n",
        "    config=config_4\n",
        ")\n",
        "\n",
        "if \"__interrupt__\" in result:\n",
        "    interrupt_info = result[\"__interrupt__\"][0]\n",
        "    print(\"\\n  Human approval required:\")\n",
        "    print(f\"   {interrupt_info.value['warning']}\")\n",
        "    print(f\"   Database: {interrupt_info.value['database_name']}\")\n",
        "\n",
        "print(\"\\n(In a real app, a human would review this before proceeding)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Human-in-the-Loop (Interrupts)\n",
        "-  Use `interrupt()` to pause execution\n",
        "-  Requires a `checkpointer` for persistence\n",
        "-  Resume with `Command(resume=value)`\n",
        "-  Perfect for approval workflows and sensitive operations\n",
        "\n",
        "### Middleware\n",
        "-  `@modify_model_request` - Adjust prompts, models, tools dynamically\n",
        "-  `before_model` / `after_model` - Add custom logic at key points\n",
        "-  Subclass `AgentMiddleware` for reusable components\n",
        "-  Perfect for logging, safety checks, dynamic behavior\n",
        "\n",
        "### When to Use What?\n",
        "\n",
        "**Use Interrupts when:**\n",
        "- You need human approval for actions\n",
        "- You want to review/edit tool calls\n",
        "- You need to validate user input\n",
        "\n",
        "**Use Middleware when:**\n",
        "- You need to modify agent behavior dynamically\n",
        "- You want to add logging/monitoring\n",
        "- You need to enforce policies (token limits, safety checks)\n",
        "- You want to personalize responses based on context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercise (Optional)\n",
        "\n",
        "Try building an agent that:\n",
        "1. Has a tool to make a purchase\n",
        "2. Uses middleware to check if the purchase amount is over $1000\n",
        "3. If over $1000, uses an interrupt to require approval\n",
        "4. If under $1000, processes automatically\n",
        "\n",
        "Hint: Combine `before_model` middleware with conditional `interrupt()` logic!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here!\n",
        "# Challenge: Build the purchase approval agent\n",
        "\n",
        "# @tool\n",
        "# def make_purchase(item: str, amount: float) -> str:\n",
        "#     ...\n",
        "\n",
        "# class PurchaseLimitMiddleware(AgentMiddleware):\n",
        "#     ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "You now have powerful tools for building production agents!\n",
        "\n",
        "**Continue your journey:**\n",
        "1.  Check out `multi_agent.ipynb` for multi-agent systems\n",
        "2.  Explore built-in middleware (Summarization, Anthropic Prompt Caching)\n",
        "3.  Build your own custom middleware for your use case\n",
        "4.  Add LangSmith for debugging and monitoring\n",
        "\n",
        "**Resources:**\n",
        "- [Middleware Documentation](https://docs.langchain.com/oss/python/langchain/middleware)\n",
        "- [Human-in-the-Loop Guide](https://docs.langchain.com/oss/python/langchain/human-in-the-loop)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "\n",
        "</br>\n",
        "</br>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
