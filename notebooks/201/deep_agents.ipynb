{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Agents: Building a Research Agent from Scratch\n",
        "\n",
        "Welcome to Deep Agents! This notebook will walk you through the core concepts of the Deep Agents framework by **progressively building a research agent** from scratch.\n",
        "\n",
        "**What you'll learn:**\n",
        "- What Deep Agents is and what it provides out of the box\n",
        "- How to add custom tools to extend agent capabilities\n",
        "- Understanding backends and storage abstraction\n",
        "- Task delegation with subagents and context isolation\n",
        "- Human-in-the-loop patterns for safety\n",
        "- Long-term memory for persistent storage\n",
        "- Middleware architecture and extensibility\n",
        "- Skills for reusable agent capabilities\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "> **Note:** This workshop requires a [Tavily API key](https://tavily.com) for web search functionality. Deep Agents is built on top of LangGraph, providing a powerful harness for building autonomous agents with filesystem access, planning, and delegation capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0: Setup & Installation\n",
        "\n",
        "First, let's install the necessary packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages \n",
        "# Run uv sync to install the packages or run:\n",
        "# !pip install deepagents tavily-python python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to Python path so we can import from utils module\n",
        "import sys\n",
        "from pathlib import Path\n",
        "project_root = Path().resolve().parent.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import model from centralized utils module\n",
        "from utils.models import model\n",
        "\n",
        "# Load environment variables for Tavily\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='LangSmith now uses UUID v7')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Your First Deep Agent (The Harness)\n",
        "\n",
        "<img src=\"../../images/deepAgentsDiag.png\" style=\"width: auto; height: 500px; border-radius: 8px;\" alt=\"Deep Agents Architecture\">\n",
        "\n",
        "Deep Agents functions as an **\"agent harness\"**â€”a framework built on a core tool-calling loop, but with pre-built tools and integrated capabilities for autonomous task execution.\n",
        "\n",
        "### What you get out of the box:\n",
        "\n",
        "- **Filesystem Tools** â€” `ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep`\n",
        "- **Planning Tool** â€” `write_todos` for task tracking\n",
        "- **Subagent Delegation** â€” `task()` tool for isolated work\n",
        "- **Large Tool Result Eviction** â€” Automatically offloads tool results >20k tokens to the filesystem\n",
        "- **Conversation Summarization** â€” Compresses history when approaching ~85% context capacity\n",
        "- **Dangling Tool Call Patching** â€” Fixes message history consistency automatically\n",
        "\n",
        "Let's create the most basic deep agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepagents import create_deep_agent\n",
        "\n",
        "# Create the most basic deep agent - just a model!\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\"\n",
        ")\n",
        "\n",
        "print(\"Deep agent created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the built-in filesystem tools\n",
        "\n",
        "Even without adding any custom tools, our agent already has filesystem capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask the agent to write and read a file\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called notes.md with the text 'Hello from Deep Agents!' then read it back to confirm.\"}]\n",
        "})\n",
        "\n",
        "# Print the final response\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the virtual filesystem contents (stored in result[\"files\"])\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“ VIRTUAL FILESYSTEM (in-memory, not on disk!)\")\n",
        "print(\"=\" * 50)\n",
        "for path, file_data in result.get(\"files\", {}).items():\n",
        "    print(f\"\\n  Path: '{path}'\")\n",
        "    print(\"  \" + \"-\" * 38)\n",
        "    # file_data contains 'content' (list of lines), 'created_at', 'modified_at'\n",
        "    if isinstance(file_data, dict) and \"content\" in file_data:\n",
        "        content = \"\\n\".join(file_data[\"content\"])\n",
        "        for line in content.split(\"\\n\"):\n",
        "            print(f\"  | {line}\")\n",
        "    else:\n",
        "        print(f\"  | {file_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- `create_deep_agent()` gives you filesystem + planning capabilities for free\n",
        "- No need to define basic file operations - they're built-in\n",
        "- Files are stored in agent state (we'll learn more about this in Part 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Adding Custom Tools\n",
        "\n",
        "While the built-in tools are powerful, we need **custom tools** to build a research agent. Let's add web search and strategic thinking capabilities.\n",
        "\n",
        "### Creating a Web Search Tool with Tavily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize Tavily client\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "\n",
        "@tool(parse_docstring=True)\n",
        "def tavily_search(query: str) -> str:\n",
        "    \"\"\"Search the web for information on a given query.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query to execute\n",
        "    \"\"\"\n",
        "    search_results = tavily_client.search(query, max_results=3, topic=\"general\")\n",
        "    \n",
        "    result_texts = []\n",
        "    for result in search_results.get(\"results\", []):\n",
        "        url = result[\"url\"]\n",
        "        title = result[\"title\"]\n",
        "        content = result.get(\"content\", \"No content available\")\n",
        "        result_text = f\"## {title}\\n**URL:** {url}\\n\\n{content}\\n\\n---\\n\"\n",
        "        result_texts.append(result_text)\n",
        "    \n",
        "    return f\"Found {len(result_texts)} result(s) for '{query}':\\n\\n{''.join(result_texts)}\"\n",
        "\n",
        "\n",
        "print(\"tavily_search tool created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add custom tools to our agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate the agent with our custom tools\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],  # Add our custom tools\n",
        "    system_prompt=\"\"\"You are a helpful research assistant.\n",
        "    \n",
        "Use tavily_search to find information on the web.\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Agent updated with custom tools!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the search capability\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for information about LangGraph and summarize what you find.\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Custom tools extend what your agent can do\n",
        "- The `@tool` decorator converts a function into a LangChain tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Understanding Backends\n",
        "\n",
        "<img src=\"../../images/deepAgentBackends.png\" style=\"width: auto; height: 500px; border-radius: 8px;\" alt=\"Backend Architecture\">\n",
        "\n",
        "Where do the agent's files actually go? **Backends** are pluggable storage systems that expose a filesystem surface to agents.\n",
        "\n",
        "### Four Backend Types:\n",
        "\n",
        "| Backend | Storage | Persistence | Use Case |\n",
        "|---------|---------|-------------|----------|\n",
        "| **StateBackend** | In-memory (agent state) | Single thread | Scratch pads, intermediate results |\n",
        "| **FilesystemBackend** | Local disk | Permanent | Direct file access (use with caution!) |\n",
        "| **StoreBackend** | LangGraph Store | Cross-thread | Long-term memories |\n",
        "| **CompositeBackend** | Routes to others | Mixed | Selective persistence |\n",
        "\n",
        "By default, `create_deep_agent()` uses **StateBackend** â€” files are stored in agent state and disappear when the thread ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langsmith import uuid7\n",
        "\n",
        "# Add a checkpointer so we can demonstrate persistence across turns\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Create a thread\n",
        "thread_id = str(uuid7())\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "print(f\"Thread ID: {thread_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write a file in this thread\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called /research_notes.md with 'My research findings go here'\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Files in state:\", list(result.get(\"files\", {}).keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In the same thread, the file persists\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Read the file `/research_notes.md`\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In a NEW thread, the file is gone (StateBackend is ephemeral)\n",
        "new_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"List all files with ls /\"}]\n",
        "}, config=new_config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FilesystemBackend - Writing to Real Disk\n",
        "\n",
        "When you need agents to work with **actual files on disk**, use `FilesystemBackend`. With `virtual_mode=True`, paths are sandboxed under `root_dir` for security.\n",
        "\n",
        "> âš ï¸ **Caution**: FilesystemBackend writes real files! Use `virtual_mode=True` to prevent path traversal attacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepagents.backends import FilesystemBackend\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create a temporary directory for our sandbox\n",
        "sandbox_dir = tempfile.mkdtemp(prefix=\"deepagents_sandbox_\")\n",
        "print(f\"Sandbox directory: `{sandbox_dir}`\")\n",
        "\n",
        "# Create a FilesystemBackend with virtual_mode=True (sandboxed)\n",
        "fs_backend = FilesystemBackend(root_dir=sandbox_dir, virtual_mode=True)\n",
        "\n",
        "# Create an agent that writes to real disk (sandboxed)\n",
        "agent_with_fs = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"You are a helpful assistant. Files you write go to the local filesystem. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    backend=fs_backend,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with FilesystemBackend created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Write a file through the agent\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_fs.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called notes.txt with 'Hello from FilesystemBackend!'\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Verify the file was actually written to disk!\n",
        "actual_path = os.path.join(sandbox_dir, \"notes.txt\")\n",
        "if os.path.exists(actual_path):\n",
        "    with open(actual_path, \"r\") as f:\n",
        "        print(f\"\\nâœ… File exists on disk at: `{actual_path}`\")\n",
        "        print(f\"   Content: {f.read()}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ File not found at: {actual_path}\")\n",
        "\n",
        "# List all files in the sandbox\n",
        "print(f\"\\nðŸ“ Files in sandbox ({sandbox_dir}):\")\n",
        "for f in os.listdir(sandbox_dir):\n",
        "    print(f\"   - {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CompositeBackend - Routing Paths to Different Backends\n",
        "\n",
        "`CompositeBackend` lets you route different paths to different backends. This is how you implement **hybrid storage** - some paths ephemeral, others persistent, others on disk.\n",
        "\n",
        "```\n",
        "/                 â†’ StateBackend (ephemeral scratch space)\n",
        "/memories/*       â†’ StoreBackend (persistent across threads)\n",
        "/workspace/*      â†’ FilesystemBackend (real disk)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepagents.backends import StateBackend, CompositeBackend\n",
        "\n",
        "# Create another sandbox for workspace files\n",
        "workspace_dir = tempfile.mkdtemp(prefix=\"deepagents_workspace_\")\n",
        "print(f\"Workspace directory: `{workspace_dir}`\")\n",
        "\n",
        "composite_backend = lambda rt: CompositeBackend(\n",
        "    default=StateBackend(rt),\n",
        "    routes={\n",
        "        # /workspace/* â†’ FilesystemBackend (real disk, sandboxed)\n",
        "        \"/workspace/\": FilesystemBackend(root_dir=workspace_dir, virtual_mode=True),\n",
        "    }\n",
        ")\n",
        "\n",
        "agent_composite = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant.\n",
        "\n",
        "STORAGE RULES:\n",
        "- Files in /workspace/* are saved to real disk (persistent)\n",
        "- All other files are ephemeral (disappear when thread ends)\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    backend=composite_backend,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with CompositeBackend created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Write to both locations and see the difference\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_composite.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Write two files:\n",
        "1. `/workspace/persistent.txt` with 'I will survive!'\n",
        "2. `/scratch.txt` with 'I am ephemeral'\n",
        "\n",
        "Then list both locations.\"\"\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Check what's on disk vs in state\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“ ON DISK (workspace_dir):\")\n",
        "for f in os.listdir(workspace_dir):\n",
        "    filepath = os.path.join(workspace_dir, f)\n",
        "    with open(filepath, \"r\") as file:\n",
        "        print(f\"   {f}: {file.read()}\")\n",
        "\n",
        "print(\"\\nðŸ“¦ IN VIRTUAL STATE:\")\n",
        "for path, data in result.get(\"files\", {}).items():\n",
        "    if isinstance(data, dict) and \"content\" in data:\n",
        "        content = \"\\n\".join(data[\"content\"])\n",
        "    else:\n",
        "        content = str(data)\n",
        "    print(f\"   `{path}`: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StoreBackend - For LangGraph Platform & Deployments\n",
        "\n",
        "`StoreBackend` uses LangGraph's `BaseStore` for persistent, cross-thread storage. This is the backend you'll use when:\n",
        "\n",
        "- Running `langgraph dev` locally (store is provided automatically)\n",
        "- Deploying to **LangSmith Deployments** (store is managed by the platform)\n",
        "\n",
        "The platform provides a persistent store, so your agent's `/memories/*` files survive across conversations and server restarts.\n",
        "\n",
        "> ðŸ’¡ **Note**: We'll demonstrate `StoreBackend` in Part 6 (Long-Term Memory) where we combine it with `CompositeBackend` for selective persistence.\n",
        "\n",
        "```python\n",
        "# When deployed to LangGraph Platform, the store is injected automatically\n",
        "# This is the pattern you'll use in production:\n",
        "from deepagents.backends import StateBackend, StoreBackend, CompositeBackend\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    backend=lambda rt: CompositeBackend(\n",
        "        default=StateBackend(rt),  # Everything else is ephemeral\n",
        "        routes={\n",
        "            # /memories/* persists across threads via the platform's store\n",
        "            \"/memories/\": StoreBackend(rt),\n",
        "        }\n",
        "    ),\n",
        "    store=InMemoryStore()  # Good for local dev; omit for LangSmith Deployment\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Remove temporary directories\n",
        "import shutil\n",
        "shutil.rmtree(sandbox_dir, ignore_errors=True)\n",
        "shutil.rmtree(workspace_dir, ignore_errors=True)\n",
        "print(\"âœ… Temporary directories cleaned up\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- **Backends** control where/how agent files are stored\n",
        "- **StateBackend** (default) is ephemeral - files disappear when thread ends\n",
        "- **FilesystemBackend** writes to real disk (use `virtual_mode=True` for sandboxing)\n",
        "- **CompositeBackend** routes different paths to different backends (hybrid storage)\n",
        "- **StoreBackend** is used for LangGraph Platform deployments (we'll use it in Part 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Part 4: Adding a Research Subagent\n",
        "\n",
        "<img src=\"../../images/deepAgentSubagents.png\" style=\"width: auto; height: 500px; border-radius: 8px;\" alt=\"Subagent Architecture\">\n",
        "\n",
        "As agents do more work, their context fills up with intermediate tool calls. **Subagents** solve this by isolating work in a separate context.\n",
        "\n",
        "### The Context Bloat Problem\n",
        "\n",
        "Without subagents:\n",
        "```\n",
        "User: Research AI agents\n",
        "â†’ search(\"AI agents overview\") â†’ 5000 tokens of results\n",
        "â†’ think(\"Found overview...\") â†’ 200 tokens\n",
        "â†’ search(\"AI agent frameworks\") â†’ 5000 tokens of results  \n",
        "â†’ think(\"Found frameworks...\") â†’ 200 tokens\n",
        "â†’ ... context bloats with every search\n",
        "```\n",
        "\n",
        "With subagents:\n",
        "```\n",
        "User: Research AI agents\n",
        "â†’ task(\"research-agent\", \"Research AI agents\")\n",
        "â†’ [Subagent does all searches in isolated context]\n",
        "â†’ Returns: \"Summary: AI agents are...\" (clean, compressed result)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get current date for the researcher\n",
        "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Define the researcher subagent\n",
        "RESEARCHER_INSTRUCTIONS = f\"\"\"You are a research assistant conducting research. Today's date is {current_date}.\n",
        "\n",
        "<Task>\n",
        "Use tools to gather information about the research topic.\n",
        "</Task>\n",
        "\n",
        "<Hard Limits>\n",
        "- Simple queries: Use 2-3 search tool calls maximum\n",
        "- Complex queries: Use up to 5 search tool calls maximum\n",
        "</Hard Limits>\n",
        "\n",
        "<Output Format>\n",
        "Structure your findings with:\n",
        "- Clear headings\n",
        "- Inline citations [1], [2], [3]\n",
        "- Sources section at the end\n",
        "</Output Format>\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        "\n",
        "research_subagent = {\n",
        "    \"name\": \"research-agent\",\n",
        "    \"description\": \"Delegate research tasks. Give one topic at a time.\",\n",
        "    \"system_prompt\": RESEARCHER_INSTRUCTIONS,\n",
        "    \"tools\": [tavily_search],\n",
        "}\n",
        "\n",
        "print(\"Research subagent defined!\")\n",
        "print(f\"  Name: {research_subagent['name']}\")\n",
        "print(f\"  Tools: {[t.name for t in research_subagent['tools']]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define orchestrator instructions\n",
        "ORCHESTRATOR_INSTRUCTIONS = \"\"\"You are a research coordinator.\n",
        "\n",
        "When asked to research a topic:\n",
        "1. Use write_todos to plan your research tasks\n",
        "2. Delegate research to the research-agent subagent using the task() tool\n",
        "3. NEVER search directly - always delegate to the research-agent\n",
        "4. Synthesize findings and write a report to /final_report.md\n",
        "\n",
        "The research-agent will handle all web searches and return summarized findings.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        "\n",
        "# Create agent with subagent\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=ORCHESTRATOR_INSTRUCTIONS,\n",
        "    subagents=[research_subagent],  # Add our research subagent\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with subagent created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test delegation\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Lightly research this week's intersesting news on AI agents\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content[:2000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Subagents isolate work in a separate context\n",
        "- Main agent only sees the final result, not intermediate searches\n",
        "- This keeps the main agent's context clean and focused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Middleware Deep Dive\n",
        "\n",
        "Deep Agents uses a **modular middleware architecture**. When you call `create_deep_agent()`, several middleware components are automatically attached.\n",
        "<img src=\"../../images/deepAgentMiddleware.png\" style=\"width: auto; height: 500px; border-radius: 8px;\" alt=\"Middleware\">\n",
        "\n",
        "### Always-On Middleware:\n",
        "\n",
        "| Middleware | Tools Provided | Purpose |\n",
        "|------------|---------------|----------|\n",
        "| **TodoListMiddleware** | `write_todos` | Task planning and tracking |\n",
        "| **FilesystemMiddleware** | `ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep` | File operations + **large tool result eviction** |\n",
        "| **SubAgentMiddleware** | `task` | Delegate work to subagents |\n",
        "| **SummarizationMiddleware** | *(none)* | Compresses conversation history at ~85% context capacity |\n",
        "| **PatchToolCallsMiddleware** | *(none)* | Fixes dangling tool calls in message history |\n",
        "\n",
        "### Conditional Middleware (added when configured):\n",
        "\n",
        "| Middleware | Trigger | Purpose |\n",
        "|------------|---------|----------|\n",
        "| **MemoryMiddleware** | `memory=[\"./AGENTS.md\"]` | Loads persistent context from AGENTS.md files |\n",
        "| **SkillsMiddleware** | `skills=[\"/skills/\"]` | Progressive disclosure of bundled capabilities |\n",
        "| **HumanInTheLoopMiddleware** | `interrupt_on={...}` | Human approval for sensitive operations |\n",
        "\n",
        "### Built-in Context Management Strategies\n",
        "\n",
        "Deep agents automatically manage context to stay within the model's token limits. There are three strategies, all configurable via `tool_token_limit_before_evict` and the model's `max_input_tokens`:\n",
        "\n",
        "<style>\n",
        ".ctx-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(500px, 1fr)); gap: 10px; margin: 10px 0 10px 0; }\n",
        ".ctx-card img { width: 1000px; border-radius: 8px; margin-bottom: 8px; }\n",
        ".ctx-card p { font-size: 0.92em; line-height: 1.5; margin: 0; }\n",
        ".ctx-card strong { display: block; margin-bottom: 4px; }\n",
        "</style>\n",
        "<div class=\"ctx-grid\">\n",
        "<div class=\"ctx-card\">\n",
        "<img src=\"../../images/Offloading%20Inputs%20LangChain.png\" alt=\"Offloading tool inputs\">\n",
        "<p><strong>Offload Large Inputs</strong>File write/edit tool calls leave the full content in conversation history. Since it's already on disk, it's redundant. At ~85% context capacity, deep agents truncate these older tool calls and replace them with a pointer to the file.</p>\n",
        "</div>\n",
        "<div class=\"ctx-card\">\n",
        "<img src=\"../../images/Offloading%20Results%20LangChain.png\" alt=\"Offloading tool results\">\n",
        "<p><strong>Offload Large Results</strong>When a tool result exceeds ~20k tokens, the deep agent saves it to the backend and swaps it with a file path reference + a 10-line preview. The agent can re-read or search the full content as needed.</p>\n",
        "</div>\n",
        "<div class=\"ctx-card\">\n",
        "<img src=\"../../images/LangChain%20Summarization.png\" alt=\"Conversation summarization\">\n",
        "<p><strong>Conversation Summarization</strong>When context hits ~85% of <code>max_input_tokens</code> and nothing is left to offload, the agent summarizes the history. The full messages are preserved to <code>/conversation_history/</code> on disk; a structured summary replaces them in working memory.</p>\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our agent already has all three middleware!\n",
        "# Let's see the planning capability in action\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Create a plan for researching machine learning frameworks. Use write_todos.\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the todos in state\n",
        "if \"todos\" in result:\n",
        "    print(\"ðŸ“‹ Agent's Todo List:\\n\")\n",
        "    for todo in result[\"todos\"]:\n",
        "        status_map = {\"completed\": \"âœ…\", \"in_progress\": \"ðŸ”„\", \"pending\": \"â¬š\"}\n",
        "        status = todo.get(\"status\", \"pending\")\n",
        "        icon = status_map.get(status, \"â¬š\")\n",
        "        content = todo.get(\"content\", str(todo))\n",
        "        print(f\"  {icon} {content}\")\n",
        "else:\n",
        "    print(\"No todos in state (agent may have used a different approach)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Middleware: Logging Tool Calls\n",
        "\n",
        "Beyond the built-in middleware, you can write your own. The simplest way is with **decorator-based middleware** using hooks like `@wrap_tool_call`, which wraps around every tool invocation.\n",
        "\n",
        "This is useful for observability â€” seeing exactly what the agent is doing in real time.\n",
        "\n",
        "Available hooks:\n",
        "\n",
        "| Hook | When it runs | Style |\n",
        "|------|-------------|-------|\n",
        "| `@before_agent` | Before agent starts (once) | Node |\n",
        "| `@before_model` | Before each LLM call | Node |\n",
        "| `@after_model` | After each LLM response | Node |\n",
        "| `@after_agent` | After agent completes (once) | Node |\n",
        "| `@wrap_model_call` | Around each LLM call | Wrap |\n",
        "| `@wrap_tool_call` | Around each tool call | Wrap |\n",
        "\n",
        "Let's create a simple `@wrap_tool_call` middleware that logs every tool the agent uses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import wrap_tool_call\n",
        "\n",
        "@wrap_tool_call\n",
        "def log_tool_calls(request, handler):\n",
        "    \"\"\"Log every tool call the agent makes.\"\"\"\n",
        "    tool_name = request.tool_call[\"name\"]\n",
        "    tool_args = request.tool_call[\"args\"]\n",
        "    print(f\"ðŸ”§ [Tool Call] {tool_name}\")\n",
        "    print(f\"   Args: {tool_args}\")\n",
        "\n",
        "    result = handler(request)\n",
        "\n",
        "    print(f\"âœ… [Tool Done] {tool_name}\\n\")\n",
        "    return result\n",
        "\n",
        "\n",
        "# Create an agent with our custom middleware\n",
        "agent_with_logging = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    middleware=[log_tool_calls],\n",
        "    checkpointer=MemorySaver(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the agent -- watch the tool call logs in the output!\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_logging.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph? create a short summary in your filesystem.\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"\\n--- Agent Response ---\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Middleware = pluggable capability modules that wrap the agent's model calls and tool calls\n",
        "- `create_deep_agent()` automatically adds TodoList, Filesystem, SubAgent, Summarization, and PatchToolCalls middleware\n",
        "- **Context management is built-in**: large tool results are evicted, conversation history is summarized\n",
        "- Writing custom middleware is straightforward â€” use decorator hooks like `@wrap_tool_call` for quick single-purpose middleware, or class-based `AgentMiddleware` for more complex cases\n",
        "- Pass custom middleware via `middleware=[...]` and it composes with all the built-in middleware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Human-in-the-Loop\n",
        "\n",
        "<img src=\"../../images/deepAgentHITL.png\" style=\"width: auto; height: 500px; border-radius: 8px;\" alt=\"Human-in-the-Loop Flow\">\n",
        "\n",
        "For sensitive operations, you may want a human to approve actions before they execute. Deep Agents has support for **interrupts** built-in via `HumanInTheLoopMiddleware`.\n",
        "\n",
        "### Built-in Decision Types:\n",
        "- **Approve** â€” Execute with proposed arguments\n",
        "- **Edit** â€” Modify arguments before execution\n",
        "- **Reject** â€” Skip the tool call entirely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally you can also customize which decisions are available for each tool, for example:\n",
        "```python\n",
        "interrupt_on = {\n",
        "    # Sensitive operations: allow all options\n",
        "    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n",
        "\n",
        "    # Moderate risk: approval or rejection only\n",
        "    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
        "\n",
        "    # Must approve (no rejection allowed)\n",
        "    \"critical_operation\": {\"allowed_decisions\": [\"approve\"]},\n",
        "}\n",
        "```\n",
        "\n",
        "#### For this demo, let's use the default interrupt decision types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create agent with interrupts on file writes\n",
        "agent_with_hitl = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    subagents=[research_subagent],\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_on={\n",
        "        \"write_file\": True,  # Interrupt before writing files\n",
        "        \"edit_file\": True,   # Interrupt before editing files\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Agent with HITL created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.types import Command\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "# This will trigger an interrupt when the agent tries to write a file\n",
        "result = agent_with_hitl.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called /test.md with 'Hello World'\"}]\n",
        "}, config=config)\n",
        "\n",
        "# Check if we hit an interrupt\n",
        "if result.get(\"__interrupt__\"):\n",
        "    print(\"ðŸ›‘ Interrupt triggered!\\n\")\n",
        "    interrupt_value = result[\"__interrupt__\"][0].value\n",
        "    action_requests = interrupt_value[\"action_requests\"]\n",
        "    review_configs = interrupt_value[\"review_configs\"]\n",
        "\n",
        "    for action, review in zip(action_requests, review_configs):\n",
        "        print(f\"  Tool: {action['name']}\")\n",
        "        print(f\"  Args: {action['args']}\")\n",
        "        print(f\"  Allowed decisions: {review['allowed_decisions']}\")\n",
        "else:\n",
        "    print(\"No interrupt (file was written)\")\n",
        "    print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resume with approval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if result.get(\"__interrupt__\"):\n",
        "    # Approve the write operation\n",
        "    result = agent_with_hitl.invoke(\n",
        "        Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
        "        config=config\n",
        "    )\n",
        "    print(\"âœ… Resumed with approval!\")\n",
        "    print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- HITL adds human oversight for risky operations\n",
        "- Configure which tools require approval with `interrupt_on`\n",
        "- A checkpointer is required for HITL to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Long-Term Memory\n",
        "\n",
        "<img src=\"../../images/deepAgentMemories.png\" style=\"width: auto; height:500px; border-radius: 8px;\" alt=\"Long-Term Memory\">\n",
        "\n",
        "So far, files disappear when threads end. **Long-term memory** uses a `CompositeBackend` to route certain paths to persistent storage.\n",
        "\n",
        "### Path Routing:\n",
        "- `/memories/*` â†’ **StoreBackend** (persistent across threads)\n",
        "- Everything else â†’ **StateBackend** (ephemeral)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.store.memory import InMemoryStore\n",
        "from deepagents.backends import StateBackend, StoreBackend, CompositeBackend\n",
        "\n",
        "# Create a store for persistent data\n",
        "store = InMemoryStore()\n",
        "\n",
        "# StoreBackend takes the runtime (rt) object - the store is passed to create_deep_agent() instead\n",
        "def backend_factory(rt):\n",
        "    \"\"\"Create a composite backend with path routing.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Files under /memories/ go to persistent store\n",
        "            \"/memories/\": StoreBackend(rt),\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Backend factory created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create agent with long-term memory\n",
        "agent_with_memory = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"\"\"You are a helpful research assistant with long-term memory.\n",
        "    \n",
        "IMPORTANT: Save important notes to /memories/ so they persist across conversations.\n",
        "For example: /memories/research_notes.md\n",
        "\n",
        "Regular files (not in /memories/) will disappear when the conversation ends.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    subagents=[research_subagent],\n",
        "    checkpointer=checkpointer,\n",
        "    backend=backend_factory,\n",
        "    store=store,  # Store is passed here, not to the backend directly\n",
        ")\n",
        "\n",
        "print(\"Agent with long-term memory created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 1: Save something to long-term memory\n",
        "thread1_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Save 'Important research finding: AI agents are evolving rapidly' to `/memories/findings.md`\"}]\n",
        "}, config=thread1_config)\n",
        "\n",
        "print(\"Thread 1:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 2: Access the memory from a different thread\n",
        "thread2_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Read me what's in the file `/memories/findings.md`\"}]\n",
        "}, config=thread2_config)\n",
        "\n",
        "print(\"Thread 2:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- `CompositeBackend` routes different paths to different storage backends\n",
        "- `/memories/*` persists across threads\n",
        "- Other files remain ephemeral (single thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Memory Patterns: Semantic, Episodic & Procedural\n",
        "\n",
        "Research in cognitive science (and the [CoALA paper](https://arxiv.org/abs/2309.02427)) identifies three types of long-term memory that map naturally to how agents store information:\n",
        "\n",
        "| Memory Type | What It Stores | Human Example | Agent Example |\n",
        "|-------------|---------------|---------------|---------------|\n",
        "| **Semantic** | Facts & knowledge | \"Paris is the capital of France\" | User preferences, project context |\n",
        "| **Episodic** | Past experiences | \"Last Tuesday I went hiking\" | Past research sessions, interaction logs |\n",
        "| **Procedural** | Instructions & rules | How to ride a bike | Coding standards, report formatting rules |\n",
        "\n",
        "We can map these to **filesystem paths** using `CompositeBackend` with multiple routes:\n",
        "\n",
        "```\n",
        "/memories/semantic/       -> Facts: user_preferences.md, project_context.md\n",
        "/memories/episodic/       -> Experiences: 2025-02-17_research.md\n",
        "/memories/procedural/     -> Rules: coding_standards.md, report_format.md\n",
        "/                         -> Ephemeral scratch space (StateBackend)\n",
        "```\n",
        "\n",
        "Since CompositeBackend uses **longest-prefix matching**, `/memories/semantic/` takes priority over a broader `/memories/` route."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a CompositeBackend with routes for each memory type\n",
        "memory_store = InMemoryStore()\n",
        "\n",
        "def advanced_backend_factory(rt):\n",
        "    \"\"\"Route different memory types to separate persistent paths.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Each memory type gets its own namespace in the store\n",
        "            \"/memories/semantic/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"semantic\")),\n",
        "            \"/memories/episodic/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"episodic\")),\n",
        "            \"/memories/procedural/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"procedural\")),\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Advanced backend factory created with 3 memory type routes!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an agent that understands the three memory types\n",
        "memory_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant with structured long-term memory.\n",
        "\n",
        "Your memory is organized into three types:\n",
        "- /memories/semantic/   -> Facts & knowledge (user preferences, project details)\n",
        "- /memories/episodic/   -> Past experiences (session logs, interaction summaries)  \n",
        "- /memories/procedural/ -> Instructions & rules (how to format reports, coding standards)\n",
        "\n",
        "When asked to remember something, save it to the appropriate memory type.\n",
        "Regular files (not in /memories/) are ephemeral and disappear after the conversation.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    checkpointer=checkpointer,\n",
        "    backend=advanced_backend_factory,\n",
        "    store=memory_store,\n",
        ")\n",
        "\n",
        "# Thread 1: Write to all three memory types\n",
        "config_t1 = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = memory_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Please save the following to the appropriate memory types:\n",
        "1. I prefer Python over JavaScript (this is a fact about me)\n",
        "2. In our last session, we researched LangGraph and found it useful (this is a past experience)\n",
        "3. Always use inline citations [1], [2] in research reports (this is a rule)\"\"\"}]\n",
        "}, config=config_t1)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 2: Verify memories persist across threads\n",
        "config_t2 = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = memory_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What do you remember about me? Check all memory types: semantic, episodic, and procedural.\"}]\n",
        "}, config=config_t2)\n",
        "\n",
        "print(\"From a NEW thread:\\n\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Namespace Scoping: Per-User vs Global Memory\n",
        "\n",
        "By default, `StoreBackend` stores files in the namespace `(assistant_id, \"filesystem\")` -- meaning all users of one assistant share the same memories. But what if you want **per-user isolation**?\n",
        "\n",
        "`StoreBackend` accepts a `namespace` parameter -- a callable that receives a `BackendContext` and returns a namespace tuple. This controls data isolation:\n",
        "\n",
        "| Scope | Namespace | Who Can See It |\n",
        "|-------|-----------|----------------|\n",
        "| **Default** | `(assistant_id, \"filesystem\")` | All users of one assistant |\n",
        "| **Per-user** | `(\"user\", user_id, \"filesystem\")` | Only that specific user |\n",
        "| **Global** | `(\"shared\", \"filesystem\")` | All users across all assistants |\n",
        "\n",
        "This lets you build agents where:\n",
        "- `/memories/user/` stores private per-user preferences (isolated by `user_id`)\n",
        "- `/memories/shared/` stores team guidelines visible to everyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a backend factory with per-user and shared namespaces\n",
        "from langgraph.config import get_config\n",
        "\n",
        "scoped_store = InMemoryStore()\n",
        "\n",
        "def scoped_backend_factory(rt):\n",
        "    \"\"\"Route user-private and shared memories to different namespaces.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Per-user memories: the namespace lambda runs at each store operation,\n",
        "            # so get_config() picks up the current invocation's user_id\n",
        "            \"/memories/user/\": StoreBackend(\n",
        "                rt, namespace=lambda ctx: (\n",
        "                    \"user\",\n",
        "                    get_config().get(\"configurable\", {}).get(\"user_id\", \"default\"),\n",
        "                    \"filesystem\"\n",
        "                )\n",
        "            ),\n",
        "            # Shared memories: same namespace for ALL users\n",
        "            \"/memories/shared/\": StoreBackend(\n",
        "                rt, namespace=lambda ctx: (\"shared\", \"filesystem\")\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "\n",
        "scoped_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant with scoped memory.\n",
        "\n",
        "MEMORY SCOPES:\n",
        "- /memories/user/    -> Private to the current user (only they can see it)\n",
        "- /memories/shared/  -> Shared across all users (everyone can see it)\n",
        "\n",
        "Save personal preferences to /memories/user/ and team guidelines to /memories/shared/.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    checkpointer=checkpointer,\n",
        "    backend=scoped_backend_factory,\n",
        "    store=scoped_store,\n",
        ")\n",
        "\n",
        "print(\"Scoped agent created with per-user and shared memory!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User A writes both private and shared memories\n",
        "config_alice = {\"configurable\": {\"thread_id\": str(uuid7()), \"user_id\": \"alice\"}}\n",
        "\n",
        "result = scoped_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Save these two things:\n",
        "1. To my private memory (/memories/user/): 'Alice prefers dark mode and Python'\n",
        "2. To shared memory (/memories/shared/): 'Team guideline: Always write unit tests'\"\"\"}]\n",
        "}, config=config_alice)\n",
        "\n",
        "print(\"Alice wrote:\\n\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User B tries to read both - can they see Alice's private memories?\n",
        "config_bob = {\"configurable\": {\"thread_id\": str(uuid7()), \"user_id\": \"bob\"}}\n",
        "\n",
        "result = scoped_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"List all files in `/memories/user/` and `/memories/shared/` to see what you can access.\"}]\n",
        "}, config=config_bob)\n",
        "\n",
        "print(\"Bob sees:\\n\", result[\"messages\"][-1].content)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Bob can see shared guidelines but NOT Alice's private preferences!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- **Three memory types** (semantic, episodic, procedural) map naturally to filesystem paths via `CompositeBackend` routes\n",
        "- **`namespace`** on `StoreBackend` controls data isolation -- per-user, per-assistant, or global\n",
        "- Longer route prefixes take precedence in `CompositeBackend` (e.g., `/memories/semantic/` over `/memories/`)\n",
        "- Pass `user_id` via config to scope memories per user: `config={\"configurable\": {\"user_id\": \"alice\"}}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: AGENTS.md & Skills\n",
        "\n",
        "So far we've been writing `system_prompt` strings directly in code. Deep Agents provides two file-based alternatives that are more powerful and follow best practices:\n",
        "\n",
        "### AGENTS.md: Persistent Identity & Instructions\n",
        "\n",
        "`AGENTS.md` files provide persistent context that is **always loaded** into the system prompt via the `memory=` parameter. This is where you put your agent's identity, workflow rules, and preferences. Another key benefit of AGENTS.md files, is that they're in simple markdown, which is easy for anyone to edit. \n",
        "\n",
        "The powerful part: **the agent can read AND edit its own AGENTS.md file**. This means the agent can update its own instructions based on feedback, essentially a self-modifiable system prompt.\n",
        "\n",
        "> **Tradeoff**: AGENTS.md is powerful for development and internal agents -- the agent can learn and self-improve by editing its own instructions. But in **production**, you'd want to move critical instructions to `system_prompt` (which the agent can't edit) to prevent the agent from accidentally breaking its own behavior. Think of AGENTS.md as great for the \"meta learning\" phase.\n",
        "\n",
        "### Skills: On-Demand Capabilities\n",
        "\n",
        "Skills are reusable capabilities bundled as `SKILL.md` files with frontmatter metadata. They use **progressive disclosure**:\n",
        "1. At startup, only the skill **name + description** (frontmatter) is loaded -- just a few tokens\n",
        "2. When the agent determines a skill is relevant to the current task, it reads the **full SKILL.md** content\n",
        "3. This keeps the prompt clean until the skill is actually needed\n",
        "\n",
        "### Comparison\n",
        "\n",
        "| Approach | Loaded When | Editable by Agent | Best For |\n",
        "|----------|-------------|-------------------|----------|\n",
        "| `system_prompt` | Always | No | Core identity, immutable rules |\n",
        "| `AGENTS.md` (memory) | Always | Yes | Workflow, preferences, learnable rules |\n",
        "| `SKILL.md` (skills) | On demand | No (read-only) | Task-specific instructions, templates |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our AGENTS.md -- the agent's identity and instructions\n",
        "agents_md_content = \"\"\"# Research Assistant\n",
        "\n",
        "You are an expert research assistant that can search the web, synthesize findings and produce polished reports and content.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Plan** -- Use `write_todos` to break the task into steps\n",
        "2. **Research** -- Search for information using tavily_search\n",
        "3. **Reflect** -- after each search reflect and analyze findings\n",
        "4. **Synthesize** -- Combine findings into a comprehensive report\n",
        "5. **Write** -- Save the final report to `/final_report.md`\n",
        "6. **Remember** -- Save key takeaways to `/memories/research_notes.md`\n",
        "\n",
        "## Rules\n",
        "\n",
        "- Use 2-3 searches maximum\n",
        "- Consolidate citations -- each unique URL gets one number [1], [2], [3]\n",
        "- End reports with a Sources section\n",
        "- Check for relevant skills when asked to create specific content formats\n",
        "\"\"\"\n",
        "\n",
        "print(\"AGENTS.md defined!\")\n",
        "print(agents_md_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now let's define our agent's Skills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LinkedIn post skill\n",
        "linkedin_skill_content = \"\"\"---\n",
        "name: linkedin-post\n",
        "description: Write a LinkedIn post based on research findings or a given topic. Use this skill when asked to create LinkedIn content, professional posts, or thought leadership pieces.\n",
        "---\n",
        "\n",
        "# LinkedIn Post Skill\n",
        "\n",
        "## Format\n",
        "- **Hook**: Start with a bold opening line that grabs attention (appears before the 'see more' cut)\n",
        "- **Body**: 3-5 short paragraphs, each 1-2 sentences\n",
        "- Use line breaks between paragraphs for readability\n",
        "- Include 1-2 relevant emojis per paragraph (don't overdo it)\n",
        "- End with a call-to-action or question to drive engagement\n",
        "- Add 3-5 relevant hashtags at the bottom\n",
        "\n",
        "## Tone\n",
        "- Professional but conversational\n",
        "- Share insights, not just information\n",
        "- Use 'I' statements and personal perspective where appropriate\n",
        "\n",
        "## Length\n",
        "- Ideal: 150-300 words\n",
        "- LinkedIn truncates after ~210 characters, so the first line must hook the reader\n",
        "\"\"\"\n",
        "\n",
        "print(\"LinkedIn post skill defined!\")\n",
        "print(f\"Skill name: linkedin-post\")\n",
        "print(f\"Length: {len(linkedin_skill_content)} chars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Twitter/X post skill\n",
        "twitter_skill_content = \"\"\"---\n",
        "name: twitter-post\n",
        "description: Write a Twitter/X post or thread based on research findings or a given topic. Use this skill when asked to create tweets, X posts, or social media threads.\n",
        "---\n",
        "\n",
        "# Twitter/X Post Skill\n",
        "\n",
        "## Single Tweet Format\n",
        "- Maximum 280 characters\n",
        "- Lead with the most compelling point\n",
        "- Use numbers or data when possible\n",
        "- 1-2 hashtags max (optional)\n",
        "\n",
        "## Thread Format (for longer content)\n",
        "- Tweet 1: Hook + preview (e.g., 'A thread on X:')\n",
        "- Tweets 2-N: One idea per tweet, numbered (1/, 2/, 3/)\n",
        "- Final tweet: Summary + call-to-action\n",
        "- 4-8 tweets is the sweet spot\n",
        "\n",
        "## Tone\n",
        "- Concise and punchy\n",
        "- Opinionated takes perform better than neutral summaries\n",
        "- Use plain language -- no corporate speak\n",
        "\"\"\"\n",
        "\n",
        "print(\"Twitter/X post skill defined!\")\n",
        "print(f\"Skill name: twitter-post\")\n",
        "print(f\"Length: {len(twitter_skill_content)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Put It All Together (AGENTS.md + Skills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an agent that uses AGENTS.md + skills\n",
        "# In a notebook, we seed the files via the `files` state key using create_file_data()\n",
        "# In production (langgraph dev / Studio), you'd use FilesystemBackend and load from disk\n",
        "from deepagents.backends.utils import create_file_data\n",
        "\n",
        "skill_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"You are an expert research assistant.\",\n",
        "    memory=[\"./AGENTS.md\"],         # Always loaded into system prompt\n",
        "    skills=[\"./skills/\"],            # Loaded on demand via progressive disclosure\n",
        "    checkpointer=checkpointer,\n",
        "    backend=backend_factory,\n",
        "    store=store,\n",
        ")\n",
        "\n",
        "# Seed the AGENTS.md and skill files into the virtual filesystem\n",
        "skill_files = {\n",
        "    \"/AGENTS.md\": create_file_data(agents_md_content),\n",
        "    \"/skills/linkedin-post/SKILL.md\": create_file_data(linkedin_skill_content),\n",
        "    \"/skills/twitter-post/SKILL.md\": create_file_data(twitter_skill_content),\n",
        "}\n",
        "\n",
        "print(\"Agent created with AGENTS.md + 2 skills!\")\n",
        "print(f\"  Memory: `./AGENTS.md` (always loaded)\")\n",
        "print(f\"  Skills: linkedin-post, twitter-post (loaded on demand)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Research a topic, then ask for a LinkedIn post\n",
        "# The agent will research first (no skill needed), then load the linkedin-post skill on demand\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = skill_agent.invoke(\n",
        "    {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Research what AI agents are, then write a LinkedIn post about your findings.\"}],\n",
        "        \"files\": skill_files\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Now ask for a tweet thread in the same conversation\n",
        "# The twitter-post skill will be loaded on demand\n",
        "result = skill_agent.invoke(\n",
        "    {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Now write a Twitter/X thread about the same topic.\"}]\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- **AGENTS.md** replaces hardcoded `system_prompt` strings -- loaded via `memory=`, editable by the agent\n",
        "- **Skills** provide on-demand capabilities via progressive disclosure -- only loaded when relevant\n",
        "- In a notebook, seed files via `files=` with `create_file_data()`; in production, use `FilesystemBackend` to load from disk\n",
        "- The agent can self-improve by editing its own AGENTS.md -- powerful for development, but lock down critical rules in `system_prompt` for production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: The Complete Research Agent\n",
        "\n",
        "Let's review what we built! Starting from a basic `create_deep_agent()` call, we progressively added:\n",
        "\n",
        "```\n",
        "Part 1: create_deep_agent(model)                    â†’ Basic filesystem agent\n",
        "Part 2: + tools=[tavily_search]                     â†’ Can search web\n",
        "Part 3: (understand backends)                       â†’ Same agent, understood storage\n",
        "Part 4: + subagents=[research_agent]                â†’ Can delegate research\n",
        "Part 5: + middleware=[log_tool_calls]               â†’ Log tool calls as they happen\n",
        "Part 6: + interrupt_on={...}, checkpointer          â†’ Human oversight\n",
        "Part 7: + backend (CompositeBackend)                â†’ Long-term memory\n",
        "Part 8: + memory (AGENTS.md) + skills (SKILL.md)    â†’ Self-modifiable identity + on-demand capabilities\n",
        "```\n",
        "\n",
        "Now let's build the final agent using the AGENTS.md + skills pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create our final research agent -- using AGENTS.md + skills (just like the Studio agent)\n",
        "final_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search],\n",
        "    system_prompt=\"You are an expert research assistant.\",\n",
        "    middleware=[log_tool_calls],\n",
        "    skills=[\"./skills/\"],            # LinkedIn + Twitter skills loaded on demand\n",
        "    memory=[\"./AGENTS.md\"],         # Identity + workflow from AGENTS.md\n",
        "    checkpointer=checkpointer,\n",
        "    backend=backend_factory,\n",
        "    store=store,\n",
        ")\n",
        "\n",
        "# Seed the files into the virtual filesystem\n",
        "final_agent_files = {\n",
        "    \"/AGENTS.md\": create_file_data(agents_md_content),\n",
        "    \"/skills/linkedin-post/SKILL.md\": create_file_data(linkedin_skill_content),\n",
        "    \"/skills/twitter-post/SKILL.md\": create_file_data(twitter_skill_content),\n",
        "}\n",
        "\n",
        "print(\"Final research agent created with AGENTS.md + skills!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a full research workflow -- research + LinkedIn post (exercises the skill!)\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "print(\"Starting research workflow...\\n\")\n",
        "\n",
        "result = final_agent.invoke(\n",
        "    {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Research what LangChain Deep Agents are, write a brief report, and then write a LinkedIn post about your findings.\"}],\n",
        "        \"files\": final_agent_files\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the virtual filesystem - did the agent write files?\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“ VIRTUAL FILESYSTEM\")\n",
        "print(\"=\" * 60)\n",
        "for path, file_data in result.get(\"files\", {}).items():\n",
        "    if isinstance(file_data, dict) and \"content\" in file_data:\n",
        "        content = \"\\n\".join(file_data[\"content\"])\n",
        "    else:\n",
        "        content = str(file_data)\n",
        "    print(f\"\\nðŸ“„ '{path}' ({len(content)} chars)\")\n",
        "    print(\"-\" * 40)\n",
        "    print(content[:500] + (\"...\" if len(content) > 500 else \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Deep Agents enables building complex agents incrementally\n",
        "- Each capability (tools, subagents, HITL, memory) is composable\n",
        "- The framework handles orchestration, you focus on capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Next Steps\n",
        "\n",
        "Congratulations! You've built a complete research agent using Deep Agents. Here's what you learned:\n",
        "\n",
        "| Concept | What It Does |\n",
        "|---------|-------------|\n",
        "| **Agent Harness** | Pre-built tools + context management |\n",
        "| **Custom Tools** | Extend capabilities (search) |\n",
        "| **Backends** | Control file storage -- ephemeral, disk, persistent, or hybrid |\n",
        "| **Subagents** | Context isolation for complex tasks |\n",
        "| **Middleware** | Pluggable capability modules (filesystem, summarization, todos, etc.) |\n",
        "| **Human-in-the-Loop** | Safety gates for sensitive operations |\n",
        "| **Long-Term Memory** | Persistent storage with path routing + namespace scoping |\n",
        "| **AGENTS.md** | File-based agent identity -- always loaded, editable by the agent |\n",
        "| **Skills** | On-demand capabilities via progressive disclosure (SKILL.md files) |\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Deep Agents Documentation](https://docs.langchain.com/oss/python/deepagents/)\n",
        "- [LangGraph Documentation](https://docs.langchain.com/oss/python/langgraph/)\n",
        "- [LangChain Academy](https://academy.langchain.com/)\n",
        "- [LangChain vs LangGraph vs Deep Agents](https://docs.langchain.com/oss/python/concepts/products)\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Run in Studio** - Check out `agents/deep_agent/` for a production-ready version with AGENTS.md and skills on disk\n",
        "2. **Deploy** - Use LangGraph Platform for production (`langgraph dev` or LangSmith Deployments)\n",
        "3. **Add Skills** - Create your own SKILL.md files for domain-specific capabilities\n",
        "4. **Customize Memory** - Use namespace scoping for per-user vs shared memory\n",
        "5. **Extend** - Build multi-agent systems with specialized subagents\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "**Happy building!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
