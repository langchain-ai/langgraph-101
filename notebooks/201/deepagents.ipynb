{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepAgents 101: Building a Research Agent from Scratch\n",
        "\n",
        "Welcome to DeepAgents 101! This notebook will walk you through the core concepts of the DeepAgents framework by **progressively building a research agent** from scratch.\n",
        "\n",
        "**What you'll learn:**\n",
        "- What DeepAgents is and what it provides out of the box\n",
        "- How to add custom tools to extend agent capabilities\n",
        "- Understanding backends and storage abstraction\n",
        "- Task delegation with subagents and context isolation\n",
        "- Human-in-the-loop patterns for safety\n",
        "- Long-term memory for persistent storage\n",
        "- Middleware architecture and extensibility\n",
        "- Skills for reusable agent capabilities\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "> **Note:** This workshop requires a [Tavily API key](https://tavily.com) for web search functionality. DeepAgents is built on top of LangGraph, providing a powerful harness for building autonomous agents with filesystem access, planning, and delegation capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0: Setup & Installation\n",
        "\n",
        "First, let's install the necessary packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages \n",
        "# Run uv sync to install the packages or run:\n",
        "# !pip install deepagents tavily-python python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to Python path so we can import from utils module\n",
        "import sys\n",
        "from pathlib import Path\n",
        "project_root = Path().resolve().parent.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import model from centralized utils module\n",
        "from utils.models import model\n",
        "\n",
        "# Load environment variables for Tavily\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='LangSmith now uses UUID v7')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Your First Deep Agent (The Harness)\n",
        "\n",
        "DeepAgents functions as an **\"agent harness\"**â€”a framework built on a core tool-calling loop, but with pre-built tools and integrated capabilities for autonomous task execution.\n",
        "\n",
        "![DeepAgents Architecture](../../images/deepAgentsDiag.png)\n",
        "\n",
        "### What you get out of the box:\n",
        "\n",
        "- **Filesystem Tools** - `ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep`\n",
        "- **Planning Tool** - `write_todos` for task tracking\n",
        "- **Subagent Delegation** - `task()` tool for isolated work\n",
        "- **Large Tool Result Eviction** - Automatically offloads tool results >20k tokens to the filesystem\n",
        "- **Conversation Summarization** - Compresses history when approaching ~85% context capacity\n",
        "- **Dangling Tool Call Patching** - Fixes message history consistency automatically\n",
        "\n",
        "Let's create the most basic deep agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep agent created successfully!\n"
          ]
        }
      ],
      "source": [
        "from deepagents import create_deep_agent\n",
        "\n",
        "# Create the most basic deep agent - just a model!\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\"\n",
        ")\n",
        "\n",
        "print(\"Deep agent created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the built-in filesystem tools\n",
        "\n",
        "Even without adding any custom tools, our agent already has filesystem capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent response: Perfect! âœ… I've successfully created the file `notes.md` with the text \"Hello from DeepAgents!\" and confirmed its contents by reading it back. The file now contains exactly what was requested.\n"
          ]
        }
      ],
      "source": [
        "# Ask the agent to write and read a file\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called notes.md with the text 'Hello from DeepAgents!' then read it back to confirm.\"}]\n",
        "})\n",
        "\n",
        "# Print the final response\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“ VIRTUAL FILESYSTEM (in-memory, not on disk!)\n",
            "==================================================\n",
            "\n",
            "  Path: '/notes.md'\n",
            "  --------------------------------------\n",
            "  | Hello from DeepAgents!\n"
          ]
        }
      ],
      "source": [
        "# Show the virtual filesystem contents (stored in result[\"files\"])\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“ VIRTUAL FILESYSTEM (in-memory, not on disk!)\")\n",
        "print(\"=\" * 50)\n",
        "for path, file_data in result.get(\"files\", {}).items():\n",
        "    print(f\"\\n  Path: '{path}'\")\n",
        "    print(\"  \" + \"-\" * 38)\n",
        "    # file_data contains 'content' (list of lines), 'created_at', 'modified_at'\n",
        "    if isinstance(file_data, dict) and \"content\" in file_data:\n",
        "        content = \"\\n\".join(file_data[\"content\"])\n",
        "        for line in content.split(\"\\n\"):\n",
        "            print(f\"  | {line}\")\n",
        "    else:\n",
        "        print(f\"  | {file_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- `create_deep_agent()` gives you filesystem + planning capabilities for free\n",
        "- No need to define basic file operations - they're built-in\n",
        "- Files are stored in agent state (we'll learn more about this in Part 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Adding Custom Tools\n",
        "\n",
        "While the built-in tools are powerful, we need **custom tools** to build a research agent. Let's add web search and strategic thinking capabilities.\n",
        "\n",
        "### Creating a Web Search Tool with Tavily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tavily_search tool created!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize Tavily client\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "\n",
        "@tool(parse_docstring=True)\n",
        "def tavily_search(query: str) -> str:\n",
        "    \"\"\"Search the web for information on a given query.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query to execute\n",
        "    \"\"\"\n",
        "    search_results = tavily_client.search(query, max_results=3, topic=\"general\")\n",
        "    \n",
        "    result_texts = []\n",
        "    for result in search_results.get(\"results\", []):\n",
        "        url = result[\"url\"]\n",
        "        title = result[\"title\"]\n",
        "        content = result.get(\"content\", \"No content available\")\n",
        "        result_text = f\"## {title}\\n**URL:** {url}\\n\\n{content}\\n\\n---\\n\"\n",
        "        result_texts.append(result_text)\n",
        "    \n",
        "    return f\"Found {len(result_texts)} result(s) for '{query}':\\n\\n{''.join(result_texts)}\"\n",
        "\n",
        "\n",
        "print(\"tavily_search tool created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Strategic Thinking Tool\n",
        "\n",
        "Research requires reflection between searches. The `think_tool` creates deliberate pauses for quality decision-making:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "think_tool created!\n"
          ]
        }
      ],
      "source": [
        "@tool(parse_docstring=True)\n",
        "def think_tool(reflection: str) -> str:\n",
        "    \"\"\"Tool for strategic reflection on research progress.\n",
        "    \n",
        "    Use this after each search to analyze results and plan next steps.\n",
        "    \n",
        "    Args:\n",
        "        reflection: Your detailed reflection on research progress\n",
        "    \"\"\"\n",
        "    return f\"Reflection recorded: {reflection}\"\n",
        "\n",
        "\n",
        "print(\"think_tool created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add custom tools to our agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent updated with custom tools!\n"
          ]
        }
      ],
      "source": [
        "# Recreate the agent with our custom tools\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],  # Add our custom tools\n",
        "    system_prompt=\"\"\"You are a helpful research assistant.\n",
        "    \n",
        "Use tavily_search to find information on the web.\n",
        "After each search, use think_tool to reflect on what you found and plan next steps.\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Agent updated with custom tools!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Summary: LangGraph\n",
            "\n",
            "**LangGraph** is an open-source framework developed by LangChain that enables developers to build and manage AI agent workflows using a graph-based architecture. Here are the key points:\n",
            "\n",
            "### **What It Is**\n",
            "- A low-level orchestration framework designed for building, managing, and deploying long-running, stateful agents and workflows\n",
            "- Built on top of LangChain to improve creation of cyclical graphs needed for agent runtimes\n",
            "- Combines large language models (LLMs) with graph-based architectures for more structured AI workflows\n",
            "\n",
            "### **Core Concepts**\n",
            "- **Graph-based structure**: Workflows are organized as interconnected nodes and edges, where each node represents a task or processing step\n",
            "- **State management**: Maintains state throughout agent execution with annotated state definitions\n",
            "- **Modularity**: Developers can create reusable components and combine multiple nodes into powerful, dynamic AI processes\n",
            "\n",
            "### **Use Cases**\n",
            "- Simple chatbots\n",
            "- Complex multi-agent systems\n",
            "- Long-running, stateful workflows\n",
            "- Agent-based AI applications requiring sophisticated control flow\n",
            "\n",
            "### **Key Features**\n",
            "- Transparent, developer-friendly design for building advanced AI systems\n",
            "- Scalable architecture for handling complex agent interactions\n",
            "- Integration with LangChain products and tools\n",
            "- Visual prototyping capabilities through LangGraph Studio\n",
            "- Widely adopted by companies like Klarna, Replit, and Elastic\n",
            "\n",
            "### **How It Works**\n",
            "Developers define workflows by:\n",
            "1. Creating state definitions with typed fields\n",
            "2. Adding nodes (functions or callable objects) to process data\n",
            "3. Connecting nodes with edges based on logic\n",
            "4. Compiling the graph for execution\n",
            "5. Streaming data through the workflow for agent execution\n",
            "\n",
            "LangGraph essentially provides a structured, graph-based alternative to traditional sequential or conditional workflow patterns, making it easier to build sophisticated AI agents with clear, manageable architectures.\n"
          ]
        }
      ],
      "source": [
        "# Test the search capability\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for information about LangGraph and summarize what you find.\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Custom tools extend what your agent can do\n",
        "- The `@tool` decorator converts a function into a LangChain tool\n",
        "- The `think_tool` creates deliberate pauses for reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Understanding Backends\n",
        "\n",
        "Where do the agent's files actually go? **Backends** are pluggable storage systems that expose a filesystem surface to agents.\n",
        "\n",
        "![Backend Architecture](../../images/deepAgentBackends.png)\n",
        "\n",
        "### Four Backend Types:\n",
        "\n",
        "| Backend | Storage | Persistence | Use Case |\n",
        "|---------|---------|-------------|----------|\n",
        "| **StateBackend** | In-memory (agent state) | Single thread | Scratch pads, intermediate results |\n",
        "| **FilesystemBackend** | Local disk | Permanent | Direct file access (use with caution!) |\n",
        "| **StoreBackend** | LangGraph Store | Cross-thread | Long-term memories |\n",
        "| **CompositeBackend** | Routes to others | Mixed | Selective persistence |\n",
        "\n",
        "By default, `create_deep_agent()` uses **StateBackend** - files are stored in agent state and disappear when the thread ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thread ID: 019c6a0c-ab92-7062-8296-d9052e9e8427\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langsmith import uuid7\n",
        "\n",
        "# Add a checkpointer so we can demonstrate persistence across turns\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Create a thread\n",
        "thread_id = str(uuid7())\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "print(f\"Thread ID: {thread_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in state: ['/research_notes.md']\n"
          ]
        }
      ],
      "source": [
        "# Write a file in this thread\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called /research_notes.md with 'My research findings go here'\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Files in state:\", list(result.get(\"files\", {}).keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's the content of `/research_notes.md`:\n",
            "\n",
            "```\n",
            "My research findings go here\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# In the same thread, the file persists\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Read the file /research_notes.md\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The root directory `/` appears to be empty. This could mean:\n",
            "\n",
            "1. **No files or directories are present** at the root level in this environment\n",
            "2. **The filesystem is newly initialized** with no content yet\n",
            "3. **The environment may have access restrictions** that prevent listing files\n",
            "\n",
            "Would you like me to:\n",
            "- Check a specific subdirectory if you know of one?\n",
            "- Search for files matching a certain pattern using `glob`?\n",
            "- Help you create or navigate to a specific location?\n"
          ]
        }
      ],
      "source": [
        "# In a NEW thread, the file is gone (StateBackend is ephemeral)\n",
        "new_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"List all files with ls /\"}]\n",
        "}, config=new_config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FilesystemBackend - Writing to Real Disk\n",
        "\n",
        "When you need agents to work with **actual files on disk**, use `FilesystemBackend`. With `virtual_mode=True`, paths are sandboxed under `root_dir` for security.\n",
        "\n",
        "> âš ï¸ **Caution**: FilesystemBackend writes real files! Use `virtual_mode=True` to prevent path traversal attacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sandbox directory: /var/folders/92/t95nn86d3lv1qmkxxkkmrw1w0000gn/T/deepagents_sandbox_t891kaw_\n",
            "Agent with FilesystemBackend created!\n"
          ]
        }
      ],
      "source": [
        "from deepagents.backends import FilesystemBackend\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create a temporary directory for our sandbox\n",
        "sandbox_dir = tempfile.mkdtemp(prefix=\"deepagents_sandbox_\")\n",
        "print(f\"Sandbox directory: {sandbox_dir}\")\n",
        "\n",
        "# Create a FilesystemBackend with virtual_mode=True (sandboxed)\n",
        "fs_backend = FilesystemBackend(root_dir=sandbox_dir, virtual_mode=True)\n",
        "\n",
        "# Create an agent that writes to real disk (sandboxed)\n",
        "agent_with_fs = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"You are a helpful assistant. Files you write go to the local filesystem. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    backend=fs_backend,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with FilesystemBackend created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent response: Done! I've created a file called `notes.txt` in the root directory with the content \"Hello from FilesystemBackend!\"\n",
            "\n",
            "âœ… File exists on disk at: /var/folders/92/t95nn86d3lv1qmkxxkkmrw1w0000gn/T/deepagents_sandbox_t891kaw_/notes.txt\n",
            "   Content: Hello from FilesystemBackend!\n",
            "\n",
            "ðŸ“ Files in sandbox (/var/folders/92/t95nn86d3lv1qmkxxkkmrw1w0000gn/T/deepagents_sandbox_t891kaw_):\n",
            "   - notes.txt\n"
          ]
        }
      ],
      "source": [
        "# Test: Write a file through the agent\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_fs.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called notes.txt with 'Hello from FilesystemBackend!'\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Verify the file was actually written to disk!\n",
        "actual_path = os.path.join(sandbox_dir, \"notes.txt\")\n",
        "if os.path.exists(actual_path):\n",
        "    with open(actual_path, \"r\") as f:\n",
        "        print(f\"\\nâœ… File exists on disk at: {actual_path}\")\n",
        "        print(f\"   Content: {f.read()}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ File not found at: {actual_path}\")\n",
        "\n",
        "# List all files in the sandbox\n",
        "print(f\"\\nðŸ“ Files in sandbox ({sandbox_dir}):\")\n",
        "for f in os.listdir(sandbox_dir):\n",
        "    print(f\"   - {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CompositeBackend - Routing Paths to Different Backends\n",
        "\n",
        "`CompositeBackend` lets you route different paths to different backends. This is how you implement **hybrid storage** - some paths ephemeral, others persistent, others on disk.\n",
        "\n",
        "```\n",
        "/                 â†’ StateBackend (ephemeral scratch space)\n",
        "/memories/*       â†’ StoreBackend (persistent across threads)\n",
        "/workspace/*      â†’ FilesystemBackend (real disk)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace directory: /var/folders/92/t95nn86d3lv1qmkxxkkmrw1w0000gn/T/deepagents_workspace_2uucliqj\n",
            "Agent with CompositeBackend created!\n"
          ]
        }
      ],
      "source": [
        "from deepagents.backends import StateBackend, CompositeBackend\n",
        "\n",
        "# Create another sandbox for workspace files\n",
        "workspace_dir = tempfile.mkdtemp(prefix=\"deepagents_workspace_\")\n",
        "print(f\"Workspace directory: {workspace_dir}\")\n",
        "\n",
        "composite_backend = lambda rt: CompositeBackend(\n",
        "    default=StateBackend(rt),\n",
        "    routes={\n",
        "        # /workspace/* â†’ FilesystemBackend (real disk, sandboxed)\n",
        "        \"/workspace/\": FilesystemBackend(root_dir=workspace_dir, virtual_mode=True),\n",
        "    }\n",
        ")\n",
        "\n",
        "agent_composite = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant.\n",
        "\n",
        "STORAGE RULES:\n",
        "- Files in /workspace/* are saved to real disk (persistent)\n",
        "- All other files are ephemeral (disappear when thread ends)\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    backend=composite_backend,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with CompositeBackend created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent response: Perfect! Both files have been created successfully:\n",
            "\n",
            "**File Summary:**\n",
            "- âœ… `/workspace/persistent.txt` - Created with content \"I will survive!\" (persistent - saved to real disk)\n",
            "- âœ… `/scratch.txt` - Created with content \"I am ephemeral\" (ephemeral - will disappear when thread ends)\n",
            "\n",
            "**Directory Listings:**\n",
            "- `/workspace/` contains: `persistent.txt`\n",
            "- `/` (root) contains: `scratch.txt` and `workspace/` directory\n",
            "\n",
            "==================================================\n",
            "ðŸ“ ON DISK (workspace_dir):\n",
            "   persistent.txt: I will survive!\n",
            "\n",
            "ðŸ“¦ IN VIRTUAL STATE:\n",
            "   /scratch.txt: I am ephemeral\n"
          ]
        }
      ],
      "source": [
        "# Test: Write to both locations and see the difference\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_composite.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Write two files:\n",
        "1. /workspace/persistent.txt with 'I will survive!'\n",
        "2. /scratch.txt with 'I am ephemeral'\n",
        "\n",
        "Then list both locations.\"\"\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(\"Agent response:\", result[\"messages\"][-1].content)\n",
        "\n",
        "# Check what's on disk vs in state\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“ ON DISK (workspace_dir):\")\n",
        "for f in os.listdir(workspace_dir):\n",
        "    filepath = os.path.join(workspace_dir, f)\n",
        "    with open(filepath, \"r\") as file:\n",
        "        print(f\"   {f}: {file.read()}\")\n",
        "\n",
        "print(\"\\nðŸ“¦ IN VIRTUAL STATE:\")\n",
        "for path, data in result.get(\"files\", {}).items():\n",
        "    if isinstance(data, dict) and \"content\" in data:\n",
        "        content = \"\\n\".join(data[\"content\"])\n",
        "    else:\n",
        "        content = str(data)\n",
        "    print(f\"   {path}: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StoreBackend - For LangGraph Platform & Deployments\n",
        "\n",
        "`StoreBackend` uses LangGraph's `BaseStore` for persistent, cross-thread storage. This is the backend you'll use when:\n",
        "\n",
        "- Running `langgraph dev` locally (store is provided automatically)\n",
        "- Deploying to **LangSmith Deployments** (store is managed by the platform)\n",
        "\n",
        "The platform provides a persistent store, so your agent's `/memories/*` files survive across conversations and server restarts.\n",
        "\n",
        "> ðŸ’¡ **Note**: We'll demonstrate `StoreBackend` in Part 6 (Long-Term Memory) where we combine it with `CompositeBackend` for selective persistence.\n",
        "\n",
        "```python\n",
        "# When deployed to LangGraph Platform, the store is injected automatically\n",
        "# This is the pattern you'll use in production:\n",
        "from deepagents.backends import StateBackend, StoreBackend, CompositeBackend\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "agent = create_deep_agent(\n",
        "    backend=lambda rt: CompositeBackend(\n",
        "        default=StateBackend(rt),  # Everything else is ephemeral\n",
        "        routes={\n",
        "            # /memories/* persists across threads via the platform's store\n",
        "            \"/memories/\": StoreBackend(rt),\n",
        "        }\n",
        "    ),\n",
        "    store=InMemoryStore()  # Good for local dev; omit for LangSmith Deployment\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Temporary directories cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Cleanup: Remove temporary directories\n",
        "import shutil\n",
        "shutil.rmtree(sandbox_dir, ignore_errors=True)\n",
        "shutil.rmtree(workspace_dir, ignore_errors=True)\n",
        "print(\"âœ… Temporary directories cleaned up\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- **Backends** control where/how agent files are stored\n",
        "- **StateBackend** (default) is ephemeral - files disappear when thread ends\n",
        "- **FilesystemBackend** writes to real disk (use `virtual_mode=True` for sandboxing)\n",
        "- **CompositeBackend** routes different paths to different backends (hybrid storage)\n",
        "- **StoreBackend** is used for LangGraph Platform deployments (we'll use it in Part 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Adding a Research Subagent\n",
        "\n",
        "As agents do more work, their context fills up with intermediate tool calls. **Subagents** solve this by isolating work in a separate context.\n",
        "\n",
        "![Subagent Architecture](../../images/deepAgentSubagents.png)\n",
        "\n",
        "### The Context Bloat Problem\n",
        "\n",
        "Without subagents:\n",
        "```\n",
        "User: Research AI agents\n",
        "â†’ search(\"AI agents overview\") â†’ 5000 tokens of results\n",
        "â†’ think(\"Found overview...\") â†’ 200 tokens\n",
        "â†’ search(\"AI agent frameworks\") â†’ 5000 tokens of results  \n",
        "â†’ think(\"Found frameworks...\") â†’ 200 tokens\n",
        "â†’ ... context bloats with every search\n",
        "```\n",
        "\n",
        "With subagents:\n",
        "```\n",
        "User: Research AI agents\n",
        "â†’ task(\"research-agent\", \"Research AI agents\")\n",
        "â†’ [Subagent does all searches in isolated context]\n",
        "â†’ Returns: \"Summary: AI agents are...\" (clean, compressed result)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Research subagent defined!\n",
            "  Name: research-agent\n",
            "  Tools: ['tavily_search', 'think_tool']\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get current date for the researcher\n",
        "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Define the researcher subagent\n",
        "RESEARCHER_INSTRUCTIONS = f\"\"\"You are a research assistant conducting research. Today's date is {current_date}.\n",
        "\n",
        "<Task>\n",
        "Use tools to gather information about the research topic.\n",
        "</Task>\n",
        "\n",
        "<Hard Limits>\n",
        "- Simple queries: Use 2-3 search tool calls maximum\n",
        "- Complex queries: Use up to 5 search tool calls maximum\n",
        "- After each search, use think_tool to reflect on findings\n",
        "</Hard Limits>\n",
        "\n",
        "<Output Format>\n",
        "Structure your findings with:\n",
        "- Clear headings\n",
        "- Inline citations [1], [2], [3]\n",
        "- Sources section at the end\n",
        "</Output Format>\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        "\n",
        "research_subagent = {\n",
        "    \"name\": \"research-agent\",\n",
        "    \"description\": \"Delegate research tasks. Give one topic at a time.\",\n",
        "    \"system_prompt\": RESEARCHER_INSTRUCTIONS,\n",
        "    \"tools\": [tavily_search, think_tool],\n",
        "}\n",
        "\n",
        "print(\"Research subagent defined!\")\n",
        "print(f\"  Name: {research_subagent['name']}\")\n",
        "print(f\"  Tools: {[t.name for t in research_subagent['tools']]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent with subagent created!\n"
          ]
        }
      ],
      "source": [
        "# Define orchestrator instructions\n",
        "ORCHESTRATOR_INSTRUCTIONS = \"\"\"You are a research coordinator.\n",
        "\n",
        "When asked to research a topic:\n",
        "1. Use write_todos to plan your research tasks\n",
        "2. Delegate research to the research-agent subagent using the task() tool\n",
        "3. NEVER search directly - always delegate to the research-agent\n",
        "4. Synthesize findings and write a report to /final_report.md\n",
        "\n",
        "The research-agent will handle all web searches and return summarized findings.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\"\n",
        "\n",
        "# Create agent with subagent\n",
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],\n",
        "    system_prompt=ORCHESTRATOR_INSTRUCTIONS,\n",
        "    subagents=[research_subagent],  # Add our research subagent\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "print(\"Agent with subagent created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test delegation\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Lightly research this week's intersesting news on AI agents\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content[:2000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Subagents isolate work in a separate context\n",
        "- Main agent only sees the final result, not intermediate searches\n",
        "- This keeps the main agent's context clean and focused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Human-in-the-Loop\n",
        "\n",
        "For sensitive operations, you may want a human to approve actions before they execute. DeepAgents supports **interrupts** for human-in-the-loop workflows.\n",
        "\n",
        "![Human-in-the-Loop Flow](../../images/deepAgentHITL.png)\n",
        "\n",
        "### Decision Types:\n",
        "- **Approve** - Execute with proposed arguments\n",
        "- **Edit** - Modify arguments before execution\n",
        "- **Reject** - Skip the tool call entirely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create agent with interrupts on file writes\n",
        "agent_with_hitl = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],\n",
        "    system_prompt=\"You are a helpful research assistant. When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\",\n",
        "    subagents=[research_subagent],\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_on={\n",
        "        \"write_file\": True,  # Interrupt before writing files\n",
        "        \"edit_file\": True,   # Interrupt before editing files\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Agent with HITL created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.types import Command\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "# This will trigger an interrupt when the agent tries to write a file\n",
        "result = agent_with_hitl.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a file called /test.md with 'Hello World'\"}]\n",
        "}, config=config)\n",
        "\n",
        "# Check if we hit an interrupt\n",
        "if result.get(\"__interrupt__\"):\n",
        "    print(\"ðŸ›‘ Interrupt triggered!\\n\")\n",
        "    interrupt_value = result[\"__interrupt__\"][0].value\n",
        "    action_requests = interrupt_value[\"action_requests\"]\n",
        "    review_configs = interrupt_value[\"review_configs\"]\n",
        "\n",
        "    for action, review in zip(action_requests, review_configs):\n",
        "        print(f\"  Tool: {action['name']}\")\n",
        "        print(f\"  Args: {action['args']}\")\n",
        "        print(f\"  Allowed decisions: {review['allowed_decisions']}\")\n",
        "else:\n",
        "    print(\"No interrupt (file was written)\")\n",
        "    print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume with approval\n",
        "if result.get(\"__interrupt__\"):\n",
        "    # Approve the write operation\n",
        "    result = agent_with_hitl.invoke(\n",
        "        Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
        "        config=config\n",
        "    )\n",
        "    print(\"âœ… Resumed with approval!\")\n",
        "    print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- HITL adds human oversight for risky operations\n",
        "- Configure which tools require approval with `interrupt_on`\n",
        "- A checkpointer is required for HITL to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Long-Term Memory\n",
        "\n",
        "So far, files disappear when threads end. **Long-term memory** uses a `CompositeBackend` to route certain paths to persistent storage.\n",
        "\n",
        "![Long-Term Memory](../../images/deepAgentMemories.png)\n",
        "\n",
        "### Path Routing:\n",
        "- `/memories/*` â†’ **StoreBackend** (persistent across threads)\n",
        "- Everything else â†’ **StateBackend** (ephemeral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.store.memory import InMemoryStore\n",
        "from deepagents.backends import StateBackend, StoreBackend, CompositeBackend\n",
        "\n",
        "# Create a store for persistent data\n",
        "store = InMemoryStore()\n",
        "\n",
        "# StoreBackend takes the runtime (rt) object - the store is passed to create_deep_agent() instead\n",
        "def backend_factory(rt):\n",
        "    \"\"\"Create a composite backend with path routing.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Files under /memories/ go to persistent store\n",
        "            \"/memories/\": StoreBackend(rt),\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Backend factory created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create agent with long-term memory\n",
        "agent_with_memory = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],\n",
        "    system_prompt=\"\"\"You are a helpful research assistant with long-term memory.\n",
        "    \n",
        "IMPORTANT: Save important notes to /memories/ so they persist across conversations.\n",
        "For example: /memories/research_notes.md\n",
        "\n",
        "Regular files (not in /memories/) will disappear when the conversation ends.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    subagents=[research_subagent],\n",
        "    checkpointer=checkpointer,\n",
        "    backend=backend_factory,\n",
        "    store=store,  # Store is passed here, not to the backend directly\n",
        ")\n",
        "\n",
        "print(\"Agent with long-term memory created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 1: Save something to long-term memory\n",
        "thread1_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Save 'Important research finding: AI agents are evolving rapidly' to /memories/findings.md\"}]\n",
        "}, config=thread1_config)\n",
        "\n",
        "print(\"Thread 1:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 2: Access the memory from a different thread\n",
        "thread2_config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Read the file /memories/findings.md\"}]\n",
        "}, config=thread2_config)\n",
        "\n",
        "print(\"Thread 2:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- `CompositeBackend` routes different paths to different storage backends\n",
        "- `/memories/*` persists across threads\n",
        "- Other files remain ephemeral (single thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Memory Patterns: Semantic, Episodic & Procedural\n",
        "\n",
        "Research in cognitive science (and the [CoALA paper](https://arxiv.org/abs/2309.02427)) identifies three types of long-term memory that map naturally to how agents store information:\n",
        "\n",
        "| Memory Type | What It Stores | Human Example | Agent Example |\n",
        "|-------------|---------------|---------------|---------------|\n",
        "| **Semantic** | Facts & knowledge | \"Paris is the capital of France\" | User preferences, project context |\n",
        "| **Episodic** | Past experiences | \"Last Tuesday I went hiking\" | Past research sessions, interaction logs |\n",
        "| **Procedural** | Instructions & rules | How to ride a bike | Coding standards, report formatting rules |\n",
        "\n",
        "We can map these to **filesystem paths** using `CompositeBackend` with multiple routes:\n",
        "\n",
        "```\n",
        "/memories/semantic/       -> Facts: user_preferences.md, project_context.md\n",
        "/memories/episodic/       -> Experiences: 2025-02-17_research.md\n",
        "/memories/procedural/     -> Rules: coding_standards.md, report_format.md\n",
        "/                         -> Ephemeral scratch space (StateBackend)\n",
        "```\n",
        "\n",
        "Since CompositeBackend uses **longest-prefix matching**, `/memories/semantic/` takes priority over a broader `/memories/` route."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a CompositeBackend with routes for each memory type\n",
        "memory_store = InMemoryStore()\n",
        "\n",
        "def advanced_backend_factory(rt):\n",
        "    \"\"\"Route different memory types to separate persistent paths.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Each memory type gets its own namespace in the store\n",
        "            \"/memories/semantic/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"semantic\")),\n",
        "            \"/memories/episodic/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"episodic\")),\n",
        "            \"/memories/procedural/\": StoreBackend(rt, namespace=lambda ctx: (\"memories\", \"procedural\")),\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Advanced backend factory created with 3 memory type routes!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an agent that understands the three memory types\n",
        "memory_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant with structured long-term memory.\n",
        "\n",
        "Your memory is organized into three types:\n",
        "- /memories/semantic/   -> Facts & knowledge (user preferences, project details)\n",
        "- /memories/episodic/   -> Past experiences (session logs, interaction summaries)  \n",
        "- /memories/procedural/ -> Instructions & rules (how to format reports, coding standards)\n",
        "\n",
        "When asked to remember something, save it to the appropriate memory type.\n",
        "Regular files (not in /memories/) are ephemeral and disappear after the conversation.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    checkpointer=checkpointer,\n",
        "    backend=advanced_backend_factory,\n",
        "    store=memory_store,\n",
        ")\n",
        "\n",
        "# Thread 1: Write to all three memory types\n",
        "config_t1 = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = memory_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Please save the following to the appropriate memory types:\n",
        "1. I prefer Python over JavaScript (this is a fact about me)\n",
        "2. In our last session, we researched LangGraph and found it useful (this is a past experience)\n",
        "3. Always use inline citations [1], [2] in research reports (this is a rule)\"\"\"}]\n",
        "}, config=config_t1)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thread 2: Verify memories persist across threads\n",
        "config_t2 = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = memory_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What do you remember about me? Check all memory types: semantic, episodic, and procedural.\"}]\n",
        "}, config=config_t2)\n",
        "\n",
        "print(\"From a NEW thread:\\n\")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Namespace Scoping: Per-User vs Global Memory\n",
        "\n",
        "By default, `StoreBackend` stores files in the namespace `(assistant_id, \"filesystem\")` -- meaning all users of one assistant share the same memories. But what if you want **per-user isolation**?\n",
        "\n",
        "`StoreBackend` accepts a `namespace` parameter -- a callable that receives a `BackendContext` and returns a namespace tuple. This controls data isolation:\n",
        "\n",
        "| Scope | Namespace | Who Can See It |\n",
        "|-------|-----------|----------------|\n",
        "| **Default** | `(assistant_id, \"filesystem\")` | All users of one assistant |\n",
        "| **Per-user** | `(\"user\", user_id, \"filesystem\")` | Only that specific user |\n",
        "| **Global** | `(\"shared\", \"filesystem\")` | All users across all assistants |\n",
        "\n",
        "This lets you build agents where:\n",
        "- `/memories/user/` stores private per-user preferences (isolated by `user_id`)\n",
        "- `/memories/shared/` stores team guidelines visible to everyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a backend factory with per-user and shared namespaces\n",
        "from langgraph.config import get_config\n",
        "\n",
        "scoped_store = InMemoryStore()\n",
        "\n",
        "def scoped_backend_factory(rt):\n",
        "    \"\"\"Route user-private and shared memories to different namespaces.\"\"\"\n",
        "    return CompositeBackend(\n",
        "        default=StateBackend(rt),\n",
        "        routes={\n",
        "            # Per-user memories: the namespace lambda runs at each store operation,\n",
        "            # so get_config() picks up the current invocation's user_id\n",
        "            \"/memories/user/\": StoreBackend(\n",
        "                rt, namespace=lambda ctx: (\n",
        "                    \"user\",\n",
        "                    get_config().get(\"configurable\", {}).get(\"user_id\", \"default\"),\n",
        "                    \"filesystem\"\n",
        "                )\n",
        "            ),\n",
        "            # Shared memories: same namespace for ALL users\n",
        "            \"/memories/shared/\": StoreBackend(\n",
        "                rt, namespace=lambda ctx: (\"shared\", \"filesystem\")\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "\n",
        "scoped_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    system_prompt=\"\"\"You are a helpful assistant with scoped memory.\n",
        "\n",
        "MEMORY SCOPES:\n",
        "- /memories/user/    -> Private to the current user (only they can see it)\n",
        "- /memories/shared/  -> Shared across all users (everyone can see it)\n",
        "\n",
        "Save personal preferences to /memories/user/ and team guidelines to /memories/shared/.\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    checkpointer=checkpointer,\n",
        "    backend=scoped_backend_factory,\n",
        "    store=scoped_store,\n",
        ")\n",
        "\n",
        "print(\"Scoped agent created with per-user and shared memory!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User A writes both private and shared memories\n",
        "config_alice = {\"configurable\": {\"thread_id\": str(uuid7()), \"user_id\": \"alice\"}}\n",
        "\n",
        "result = scoped_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Save these two things:\n",
        "1. To my private memory (/memories/user/): 'Alice prefers dark mode and Python'\n",
        "2. To shared memory (/memories/shared/): 'Team guideline: Always write unit tests'\"\"\"}]\n",
        "}, config=config_alice)\n",
        "\n",
        "print(\"Alice wrote:\\n\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User B tries to read both - can they see Alice's private memories?\n",
        "config_bob = {\"configurable\": {\"thread_id\": str(uuid7()), \"user_id\": \"bob\"}}\n",
        "\n",
        "result = scoped_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"List all files in /memories/user/ and /memories/shared/ to see what you can access.\"}]\n",
        "}, config=config_bob)\n",
        "\n",
        "print(\"Bob sees:\\n\", result[\"messages\"][-1].content)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Bob can see shared guidelines but NOT Alice's private preferences!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- **Three memory types** (semantic, episodic, procedural) map naturally to filesystem paths via `CompositeBackend` routes\n",
        "- **`namespace`** on `StoreBackend` controls data isolation -- per-user, per-assistant, or global\n",
        "- Longer route prefixes take precedence in `CompositeBackend` (e.g., `/memories/semantic/` over `/memories/`)\n",
        "- Pass `user_id` via config to scope memories per user: `config={\"configurable\": {\"user_id\": \"alice\"}}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Middleware Deep Dive\n",
        "\n",
        "DeepAgents uses a **modular middleware architecture**. When you call `create_deep_agent()`, several middleware components are automatically attached.\n",
        "\n",
        "![Middleware Architecture](../../images/deepAgentMiddleware.png)\n",
        "\n",
        "### Always-On Middleware:\n",
        "\n",
        "| Middleware | Tools Provided | Purpose |\n",
        "|------------|---------------|----------|\n",
        "| **TodoListMiddleware** | `write_todos` | Task planning and tracking |\n",
        "| **FilesystemMiddleware** | `ls`, `read_file`, `write_file`, `edit_file`, `glob`, `grep` | File operations + **large tool result eviction** |\n",
        "| **SubAgentMiddleware** | `task` | Delegate work to subagents |\n",
        "| **SummarizationMiddleware** | *(none)* | Compresses conversation history at ~85% context capacity |\n",
        "| **PatchToolCallsMiddleware** | *(none)* | Fixes dangling tool calls in message history |\n",
        "\n",
        "### Conditional Middleware (added when configured):\n",
        "\n",
        "| Middleware | Trigger | Purpose |\n",
        "|------------|---------|----------|\n",
        "| **MemoryMiddleware** | `memory=[\"./AGENTS.md\"]` | Loads persistent context from AGENTS.md files |\n",
        "| **SkillsMiddleware** | `skills=[\"/skills/\"]` | Progressive disclosure of bundled capabilities |\n",
        "| **HumanInTheLoopMiddleware** | `interrupt_on={...}` | Human approval for sensitive operations |\n",
        "\n",
        "### Context Management: Two Key Mechanisms\n",
        "\n",
        "**1. Large Tool Result Eviction** (FilesystemMiddleware):\n",
        "When a tool returns more than ~20k tokens, the result is automatically evicted to `/large_tool_results/` and replaced with a truncated preview. This prevents a single search result from blowing up the context window.\n",
        "\n",
        "**2. Conversation Summarization** (SummarizationMiddleware):\n",
        "When the conversation approaches ~85% of the model's context window, old messages are offloaded to `/conversation_history/` and replaced with a summary. The full history is preserved on disk so the agent can read it back if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our agent already has all three middleware!\n",
        "# Let's see the planning capability in action\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Create a todo list for researching machine learning frameworks. Use write_todos.\"}]\n",
        "}, config=config)\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the todos in state\n",
        "if \"todos\" in result:\n",
        "    print(\"ðŸ“‹ Agent's Todo List:\\n\")\n",
        "    for todo in result[\"todos\"]:\n",
        "        status_map = {\"completed\": \"âœ…\", \"in_progress\": \"ðŸ”„\", \"pending\": \"â¬š\"}\n",
        "        status = todo.get(\"status\", \"pending\")\n",
        "        icon = status_map.get(status, \"â¬š\")\n",
        "        content = todo.get(\"content\", str(todo))\n",
        "        print(f\"  {icon} {content}\")\n",
        "else:\n",
        "    print(\"No todos in state (agent may have used a different approach)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Middleware = pluggable capability modules that wrap the agent's model calls and tool calls\n",
        "- `create_deep_agent()` automatically adds TodoList, Filesystem, SubAgent, Summarization, and PatchToolCalls middleware\n",
        "- **Context management is built-in**: large tool results are evicted, conversation history is summarized\n",
        "- You can customize or add your own middleware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Skills (Bonus)\n",
        "\n",
        "**Skills** are reusable agent capabilities that bundle context and instructions. They follow a progressive disclosure pattern - the agent only loads skill details when needed.\n",
        "\n",
        "### Skill Structure:\n",
        "```\n",
        "skills/\n",
        "â””â”€â”€ my-skill/\n",
        "    â””â”€â”€ SKILL.md    # Frontmatter + instructions\n",
        "```\n",
        "\n",
        "### When to use Skills vs Tools:\n",
        "- **Skills**: Bundle extensive context, reference materials, instructions\n",
        "- **Tools**: Single-purpose functions for specific actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a simple skill as a file\n",
        "research_skill_content = \"\"\"---\n",
        "name: research-methodology\n",
        "description: Guidelines for conducting effective research\n",
        "---\n",
        "\n",
        "# Research Methodology Skill\n",
        "\n",
        "## Workflow\n",
        "1. **Define the question** - Be specific about what you're researching\n",
        "2. **Broad search first** - Start with overview queries\n",
        "3. **Reflect** - Use think_tool after each search\n",
        "4. **Narrow down** - Get specific based on findings\n",
        "5. **Synthesize** - Combine findings into a coherent report\n",
        "\n",
        "## Citation Format\n",
        "- Use inline citations: [1], [2], [3]\n",
        "- Include a Sources section at the end\n",
        "\n",
        "## Quality Checks\n",
        "- Do I have 3+ credible sources?\n",
        "- Have I answered the original question?\n",
        "- Are my findings consistent across sources?\n",
        "\"\"\"\n",
        "\n",
        "print(\"Skill content defined!\")\n",
        "print(research_skill_content[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skills can be passed as files to the agent via the \"files\" state key\n",
        "# The files dict expects FileData objects (not raw strings)\n",
        "from deepagents.backends.utils import create_file_data\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "result = agent_with_memory.invoke(\n",
        "    {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Read the research methodology skill and tell me the key steps.\"}],\n",
        "        \"files\": {\n",
        "            \"/skills/research-methodology/SKILL.md\": create_file_data(research_skill_content)\n",
        "        }\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- Skills bundle context and instructions for reuse\n",
        "- Progressive disclosure keeps the agent's prompt focused\n",
        "- Skills are defined as `SKILL.md` files with frontmatter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: The Complete Research Agent\n",
        "\n",
        "Let's review what we built! Starting from a basic `create_deep_agent()` call, we progressively added:\n",
        "\n",
        "```\n",
        "Part 1: create_deep_agent(model)                    â†’ Basic filesystem agent\n",
        "Part 2: + tools=[tavily_search, think_tool]         â†’ Can search web\n",
        "Part 3: (understand backends)                       â†’ Same agent, understood storage\n",
        "Part 4: + subagents=[research_agent]                â†’ Can delegate research\n",
        "Part 5: + interrupt_on={...}, checkpointer          â†’ Human oversight\n",
        "Part 6: + backend (CompositeBackend)                â†’ Long-term memory\n",
        "Part 7: (understand middleware)                     â†’ Same agent, understood internals\n",
        "Part 8: + skills                                    â†’ Reusable capabilities\n",
        "```\n",
        "\n",
        "Now let's run a full research workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create our final research agent with all capabilities\n",
        "final_agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[tavily_search, think_tool],\n",
        "    system_prompt=\"\"\"You are an expert research assistant.\n",
        "\n",
        "## Workflow\n",
        "1. Use write_todos to plan your research\n",
        "2. Search for information using tavily_search\n",
        "3. Reflect on findings using think_tool\n",
        "4. Write a final report to /final_report.md\n",
        "5. Save key takeaways to /memories/research_notes.md for future reference\n",
        "\n",
        "## Rules\n",
        "- Use 2-3 searches maximum\n",
        "- After each search, use think_tool to reflect\n",
        "- Consolidate citations (each unique URL gets one number)\n",
        "- End reports with a Sources section\n",
        "\n",
        "When referencing file paths, use backtick formatting like `path/file.md` instead of markdown links.\n",
        "\"\"\",\n",
        "    checkpointer=checkpointer,\n",
        "    backend=backend_factory,\n",
        "    store=store,\n",
        ")\n",
        "\n",
        "print(\"Final research agent created with all capabilities!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a full research workflow\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid7())}}\n",
        "\n",
        "print(\"Starting research workflow...\\n\")\n",
        "\n",
        "result = final_agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Research what LangChain Deep Agents is and write a brief report.\"}]},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the virtual filesystem - did the agent write files?\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“ VIRTUAL FILESYSTEM\")\n",
        "print(\"=\" * 60)\n",
        "for path, file_data in result.get(\"files\", {}).items():\n",
        "    if isinstance(file_data, dict) and \"content\" in file_data:\n",
        "        content = \"\\n\".join(file_data[\"content\"])\n",
        "    else:\n",
        "        content = str(file_data)\n",
        "    print(f\"\\nðŸ“„ {path} ({len(content)} chars)\")\n",
        "    print(\"-\" * 40)\n",
        "    print(content[:500] + (\"...\" if len(content) > 500 else \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaway:\n",
        "- DeepAgents enables building complex agents incrementally\n",
        "- Each capability (tools, subagents, HITL, memory) is composable\n",
        "- The framework handles orchestration, you focus on capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Next Steps\n",
        "\n",
        "Congratulations! You've built a complete research agent using DeepAgents. Here's what you learned:\n",
        "\n",
        "| Concept | What It Does |\n",
        "|---------|-------------|\n",
        "| **Agent Harness** | Pre-built tools + context management |\n",
        "| **Custom Tools** | Extend capabilities (search, think) |\n",
        "| **Backends** | Control file storage (ephemeral vs persistent) |\n",
        "| **Subagents** | Context isolation for complex tasks |\n",
        "| **Human-in-the-Loop** | Safety gates for sensitive operations |\n",
        "| **Long-Term Memory** | Persistent storage with path routing |\n",
        "| **Middleware** | Pluggable capability modules |\n",
        "| **Skills** | Reusable bundled capabilities |\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [DeepAgents Documentation](https://docs.langchain.com/oss/python/deepagents/)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [LangChain Academy](https://academy.langchain.com/)\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "1. **Customize** - Add your own tools and subagents\n",
        "2. **Deploy** - Use LangGraph Platform for production\n",
        "3. **Extend** - Build multi-agent systems with specialized agents\n",
        "4. **Integrate** - Connect to databases, APIs, and other services\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "**Happy building!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
