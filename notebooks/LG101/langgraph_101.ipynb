{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph 101: Building Your First Agent\n",
    "\n",
    "Welcome to LangGraph 101! This notebook will walk you through the core concepts of building agents with LangChain and LangGraph.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to interact with language models\n",
    "- Working with messages and conversation\n",
    "- Adding tools to extend LLM capabilities\n",
    "- Building an agent that can reason and act\n",
    "- Adding memory to maintain context\n",
    "- Streaming responses for better UX\n",
    "<br> \n",
    "<br> \n",
    "---\n",
    "<br> \n",
    "\n",
    "> **Note:** This tutorial uses LangChain v1 (alpha), which provides the easiest way to start building with LLMs. LangChain agents are built on top of LangGraph, providing durable execution, streaming, human-in-the-loop, and persistence out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup & Installation\n",
    "\n",
    "First, let's install the necessary packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install --pre -U langchain langchain-anthropic langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# We'll use OpenAI in this tutorial, but you can swap to any provider!\n",
    "# Supported providers: OpenAI, Anthropic, Google, Cohere, and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Your First LLM Call\n",
    "\n",
    "LangChain provides a **standard model interface** that works across all providers. This means you can easily swap between OpenAI, Anthropic, Google, and other providers without changing your code.\n",
    "\n",
    "Let's start by initializing a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangChain is an openâ€‘source framework for building applications that use large language models (LLMs). It provides the building blocks you need to connect LLMs to data, tools, and multi-step workflows, so you can create robust AI assistants, chatbots, and automation.\n",
      "\n",
      "Key ideas and components:\n",
      "- Chains: sequences of prompts and LLM calls to perform multi-step reasoning.\n",
      "- Prompt templates: reusable prompts that plug in variables.\n",
      "- Agents and tools: agents decide actions and can call external tools or APIs (e.g., search, calculators, code execution, database queries).\n",
      "- Memory: store and reuse context from previous interactions (conversation history, summaries).\n",
      "- Vector stores: manage embeddings for retrieval-augmented generation (RAG) over documents or data sources.\n",
      "- Prompts, callbacks, and evaluation: structured ways to manage prompts, track events, and assess outputs.\n",
      "- Language support: LangChain has Python and JavaScript/TypeScript versions.\n",
      "\n",
      "Typical use cases:\n",
      "- Chatbots and virtual assistants\n",
      "- Question answering over documents or the web\n",
      "- Automating workflows that require planning and tool use\n",
      "- RAG pipelines combining LLMs with external data sources\n",
      "- Code generation, QA, and data extraction tasks\n",
      "\n",
      "Quick note:\n",
      "- LangChain is a framework, not a model. You still need an LLM (e.g., OpenAI, Cohere) and any external tools or data sources you want to integrate.\n",
      "\n",
      "If youâ€™d like, I can show a tiny example (in Python or JS) to illustrate a simple chain or an agent with a tool.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize a chat model - you can easily swap providers!\n",
    "# Examples: \"openai:gpt-5-nano\", \"anthropic:claude-3-7-sonnet-latest\", \"google:gemini-2.0-flash\"\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")\n",
    "\n",
    "# Make your first call!\n",
    "response = model.invoke(\"What is LangChain?\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `init_chat_model()` gives you a standardized interface to any LLM provider\n",
    "- `.invoke()` sends a message and returns a response\n",
    "- No provider lock-in - swap models easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Messages\n",
    "\n",
    "**Messages** are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both content and metadata.\n",
    "\n",
    "There are different message types:\n",
    "- **SystemMessage** - Instructions for how the model should behave\n",
    "- **HumanMessage** - User input\n",
    "- **AIMessage** - Model responses\n",
    "- **ToolMessage** - Results from tool executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "An agent is something that can perceive its environment, make decisions, and take actions to achieve goals.\n",
      "\n",
      "In AI terms:\n",
      "- Itâ€™s often a program or robot.\n",
      "- It observes the world with sensors (or inputs), reasons about what to do, and acts with actuators (or outputs).\n",
      "\n",
      "Key ideas:\n",
      "- Autonomy: it can run on its own without constant instructions.\n",
      "- Perception: it observes the environment.\n",
      "- Action: it can affect the environment.\n",
      "- Goal or objective: it tries to do something specific, sometimes optimizing a task or outcome.\n",
      "- Sometimes learning or adapting over time.\n",
      "\n",
      "Types and examples:\n",
      "- Software agents: web crawlers, spam filters, chatbots.\n",
      "- Robotic agents: a vacuum robot, a drone.\n",
      "- Multi-agent systems: several agents that coordinate or compete.\n",
      "\n",
      "Simple examples:\n",
      "- Thermostat: senses temperature, aims to keep it at a setpoint, turns the heater on/off.\n",
      "- Vacuum robot: senses dirt and obstacles, aims to clean, moves and suctions accordingly.\n",
      "\n",
      "If youâ€™re asking in a particular context (AI, gaming, economics, etc.), tell me and Iâ€™ll tailor the explanation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Create a conversation with different message types\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant that explains technical concepts simply.\"),\n",
    "    HumanMessage(content=\"What is an agent?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversations\n",
    "\n",
    "Messages make it easy to maintain conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hereâ€™s a concrete example of an agent:\n",
      "\n",
      "Example: Smart garden irrigation controller\n",
      "\n",
      "- Perception (sensors/inputs): soil moisture sensors, current time, weather forecast, valve status.\n",
      "- Decision-making (reasoning): based on moisture level and forecast, decide whether to water and for how long.\n",
      "- Action (actuators): open/close irrigation valves for a set duration.\n",
      "- Goal: keep the garden plants healthy while saving water.\n",
      "- Autonomy: runs on its own after setup; doesnâ€™t need constant instructions.\n",
      "- Learning/adaptation: can adjust moisture targets for different plants or seasons based on past results.\n",
      "\n",
      "How it works in practice:\n",
      "- If the soil is dry and rain isnâ€™t forecasted, the agent turns on the sprinklers for 15 minutes.\n",
      "- If the soil is already moist or rain is expected soon, it does nothing.\n",
      "- Over time, it learns that certain plants need more water and adjusts targets accordingly.\n",
      "\n",
      "If you want, I can tailor a different example to a domain youâ€™re interested in (gaming, economics, robotics, etc.).\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "messages.append(response)  # Add AI response to history\n",
    "messages.append(HumanMessage(content=\"Can you give me an example?\"))\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Messages represent the conversation history\n",
    "- SystemMessage sets the model's behavior\n",
    "- Build multi-turn conversations by appending messages to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adding Tools - Extending LLM Capabilities\n",
    "\n",
    "LLMs are great at language, but they can't access external data or perform actions. **Tools** extend their capabilities. You can give an LLM a list of tools, and when it needs one, it will specify which tool to call. Your job is to execute the tool and feed the results back to the LLM so it can decide what to do next.\n",
    "\n",
    "You can create a tool just by writing a Python function with a clear description. LangChain's `@tool` decorator handles formatting the function's information in the LLM's desired format.  \n",
    "\n",
    "Let's create some simple tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature_fahrenheit\": 69.2, \"weather_code\": 3}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Basic hardcoded tool\n",
    "@tool\n",
    "def search_movies(genre: str) -> str:\n",
    "    \"\"\"Search for movies by genre.\"\"\"\n",
    "    # In a real app, this would query a movie database\n",
    "    movies = {\n",
    "        \"sci-fi\": \"Dune, Interstellar, Blade Runner 2049\",\n",
    "        \"comedy\": \"The Grand Budapest Hotel, Superbad, Knives Out\",\n",
    "        \"action\": \"Mad Max: Fury Road, John Wick, Mission Impossible\"\n",
    "    }\n",
    "    return movies.get(genre.lower(), \"No movies found for that genre\")\n",
    "\n",
    "# More realistic tool that calls an API\n",
    "@tool\n",
    "def get_weather(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Get current temperature in Fahrenheit and weather code for given coordinates.\n",
    "\n",
    "    Args:\n",
    "        latitude: Latitude coordinate\n",
    "        longitude: Longitude coordinate\n",
    "\n",
    "    Returns:\n",
    "        JSON string with temperature_fahrenheit and weather_code (do not include the code in your response, translate it to plain English)\n",
    "    \"\"\"\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current\": \"temperature_2m,weather_code\",\n",
    "        \"temperature_unit\": \"fahrenheit\"\n",
    "    }\n",
    "\n",
    "    weather = requests.get(url, params=params).json()[\"current\"]\n",
    "    temperature = weather[\"temperature_2m\"]\n",
    "    weather_code = weather[\"weather_code\"]\n",
    "    result = {\n",
    "        \"temperature_fahrenheit\": temperature,\n",
    "        \"weather_code\": weather_code\n",
    "    }\n",
    "\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "\n",
    "# Test a tool directly with SF's coordinates\n",
    "print(get_weather.invoke({\"latitude\": 37.77, \"longitude\": 122.42}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling (Function Calling)\n",
    "\n",
    "Now let's give these tools to the model using `.bind_tools()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [{'name': 'get_weather', 'args': {'latitude': 47.6062, 'longitude': -122.3321}, 'id': 'call_Yls32daVNVqXWri2Vh0PJpdA', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to the model\n",
    "tools = [get_weather, search_movies]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "message = \"What's the weather like in Seattle?\"\n",
    "\n",
    "# The model can now decide to call tools\n",
    "response = model_with_tools.invoke(message)\n",
    "\n",
    "# Check if the model wants to call a tool\n",
    "print(\"Tool calls:\", response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns a **tool call** request with:\n",
    "- `name`: Which tool to call\n",
    "- `args`: Arguments to pass to the tool\n",
    "- `id`: Unique identifier for tracking\n",
    "\n",
    "Let's execute the tool and continue the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Right now in Seattle: about 59.6Â°F (roughly 60Â°F) with clear skies. Want me to check the forecast for the rest of the day or upcoming days?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Execute the tool call\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    \n",
    "    # Call the actual tool\n",
    "    if tool_call[\"name\"] == \"get_weather\":\n",
    "        result = get_weather.invoke(tool_call[\"args\"])\n",
    "\n",
    "    elif tool_call[\"name\"] == \"search_movies\":\n",
    "        result = search_movies.invoke(tool_call[\"args\"])\n",
    "    \n",
    "    # Create a ToolMessage with the result\n",
    "    tool_message = ToolMessage(\n",
    "        content=result,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    \n",
    "    # Continue the conversation with the tool result\n",
    "    final_response = model_with_tools.invoke([\n",
    "        HumanMessage(content=message),\n",
    "        response,\n",
    "        tool_message\n",
    "    ])\n",
    "    \n",
    "    final_response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Tools are Python functions decorated with `@tool`\n",
    "- Good descriptions help the model know when to use each tool\n",
    "- Tool calling flow: Model requests tool â†’ Execute tool â†’ Return result â†’ Model synthesizes final response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building Your First Agent with `create_agent()`\n",
    "\n",
    "Manually defining a specific sequence of LLM calls and tool calls is tedious and inflexible. Instead, we can use an **agent** that runs this loop:\n",
    "1. Model decides which tool to call (if any)\n",
    "2. Tool gets executed\n",
    "3. Result goes back to model\n",
    "4. Repeat until task is complete\n",
    "\n",
    "LangChain makes this easy with `create_agent()` - **build an agent in ~10 lines of code!**\n",
    "The prebuilt agent handles running the loop described above - you just specify the system prompt and tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the weather in NYC? Also recommend some sci-fi movies.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_t2gVr2a6AIXaJ7evnjH3xci7)\n",
      " Call ID: call_t2gVr2a6AIXaJ7evnjH3xci7\n",
      "  Args:\n",
      "    latitude: 40.7128\n",
      "    longitude: -74.006\n",
      "  search_movies (call_53nsf9Pqfnm4hBxV3Tkpp6Hg)\n",
      " Call ID: call_53nsf9Pqfnm4hBxV3Tkpp6Hg\n",
      "  Args:\n",
      "    genre: Sci-Fi\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "{\"temperature_fahrenheit\": 79.5, \"weather_code\": 0}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_movies\n",
      "\n",
      "Dune, Interstellar, Blade Runner 2049\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here you go:\n",
      "\n",
      "- NYC weather: About 79.5Â°F with clear skies right now.\n",
      "\n",
      "- Sci-fi movie picks: \n",
      "  - Dune (2021)\n",
      "  - Interstellar (2014)\n",
      "  - Blade Runner 2049 (2017)\n",
      "\n",
      "Want more options (e.g., space opera, cyberpunk, or hard sci-fi) or want to know where to stream any of these?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Create an agent with tools\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[get_weather, search_movies],\n",
    "    system_prompt=\"You are a helpful assistant that can check weather and recommend movies.\"\n",
    ")\n",
    "\n",
    "# Use the agent\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather in NYC? Also recommend some sci-fi movies.\")]\n",
    "})\n",
    "\n",
    "# Print the final response\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The agent automatically:\n",
    "1. Analyzed the user's request\n",
    "2. Called `get_weather(\"NYC\")`\n",
    "3. Called `search_movies(\"sci-fi\")`\n",
    "4. Synthesized the results into a natural response\n",
    "\n",
    "Let's visualize the agent's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x1297c2660>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `create_agent()` builds a complete agent in ~10 lines\n",
    "- The agent automatically handles the reasoning â†’ action â†’ observation loop\n",
    "- Built on LangGraph for production features (persistence, streaming, human-in-the-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Adding Memory & State\n",
    "\n",
    "Right now, each agent invocation is independent. Let's add **memory** so the agent can maintain context across multiple interactions.\n",
    "\n",
    "LangGraph uses **checkpointers** to save and restore state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Nice to meet you, Alice! Since you love sci-fi movies, would you like me to find some recommendations for you?\n",
      "\n",
      "Response 2: Your name is Alice, and you love sci-fi movies. Would you like me to find some sci-fi movie recommendations for you?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "# Create a checkpointer for memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Create an agent with memory\n",
    "agent_with_memory = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=[get_weather, search_movies],\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# Create a thread for this conversation\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# First interaction\n",
    "result1 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice and I love sci-fi movies.\"}]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Response 1:\", result1[\"messages\"][-1].content)\n",
    "\n",
    "# Second interaction - the agent remembers!\n",
    "result2 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name and what movies do I like?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(\"\\nResponse 2:\", result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding State & Threads\n",
    "\n",
    "- **State**: The agent's \"memory\" - includes message history and any custom data\n",
    "- **Thread**: A conversation session identified by `thread_id`\n",
    "- **Checkpointer**: Saves state after each step, enabling memory and error recovery\n",
    "\n",
    "Each thread is independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New thread response: I don't know your name. If you'd like to share it with me, feel free!\n"
     ]
    }
   ],
   "source": [
    "# New thread - agent won't remember Alice\n",
    "new_thread_id = str(uuid.uuid4())\n",
    "new_config = {\"configurable\": {\"thread_id\": new_thread_id}}\n",
    "\n",
    "result3 = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config=new_config\n",
    ")\n",
    "print(\"New thread response:\", result3[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Checkpointers enable memory across interactions\n",
    "- Thread IDs separate different conversations\n",
    "- State persists automatically - no manual state management needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Streaming for Better UX\n",
    "\n",
    "LLMs can take a while to respond. **Streaming** shows progress in real-time, dramatically improving user experience.\n",
    "\n",
    "LangChain supports multiple streaming modes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Agent Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming agent steps:\n",
      "\n",
      "Step: agent\n",
      "   Tool call: get_weather\n",
      "\n",
      "Step: tools\n",
      "   Content: {\"temperature_fahrenheit\": 82.5, \"weather_code\": 0}\n",
      "\n",
      "Step: agent\n",
      "   Content: Boston right now is about 82.5Â°F with clear skies. Would you like a movie recommendation to match th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream agent progress with stream_mode=\"updates\"\n",
    "print(\"Streaming agent steps:\\n\")\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node_name, data in chunk.items():\n",
    "        print(f\"Step: {node_name}\")\n",
    "        if \"messages\" in data:\n",
    "            message = data[\"messages\"][-1]\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                print(f\"   Tool call: {message.tool_calls[0]['name']}\")\n",
    "            elif hasattr(message, 'content'):\n",
    "                print(f\"   Content: {message.content[:100]}...\" if len(message.content) > 100 else f\"   Content: {message.content}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming LLM Tokens\n",
    "\n",
    "For a ChatGPT-like experience, stream tokens as they're generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming tokens:\n",
      "\n",
      "Could you clarify which LangGraph you mean (e.g., a software library, a research project, or a specific product) so I can describe it accurately in one sentence?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream tokens with stream_mode=\"messages\"\n",
    "print(\"Streaming tokens:\\n\")\n",
    "\n",
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about LangGraph in one sentence.\"}]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # Only print content from the agent node\n",
    "    if metadata.get('langgraph_node') == 'agent':\n",
    "        # Get text from content blocks\n",
    "        for block in token.content_blocks:\n",
    "            if block.get('type') == 'text' and block.get('text'):\n",
    "                print(block['text'], end='', flush=True)\n",
    "\n",
    "print(\"\\n\")  # New line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- `stream_mode=\"updates\"` - See each agent step (useful for debugging)\n",
    "- `stream_mode=\"messages\"` - Stream LLM tokens (ChatGPT-like UX)\n",
    "- Streaming is built-in - no extra setup required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Putting It All Together - A Practical Example\n",
    "\n",
    "Let's build a more realistic agent that combines everything we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PERSONAL ASSISTANT DEMO\n",
      "==================================================\n",
      "\n",
      "User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\n",
      "\n",
      "Assistant: Hi Alice! Nice to meet you.\n",
      "\n",
      "I pulled up your saved preferences: you love sci-fi movies and you prefer warm-weather destinations.\n",
      "\n",
      "I tried to fetch a tailored pick from my recommender, but it didnâ€™t return a specific title this time. In the meantime, here are some strong sci-fi picks that fit a warm/tropical vibe:\n",
      "\n",
      "- Avatar (2009) â€” lush, tropical world with stunning visuals and big sci-fi adventure.\n",
      "- Jurassic Park (1993) â€” a classic on a tropical island; perfect blend of science fiction and thrilling action.\n",
      "- Jurassic World (2015) â€” modern take on the island-resort idea with plenty of sun-soaked settings.\n",
      "- Dune (2021) â€” desert heat and epic sci-fi drama for a grand, sun-baked atmosphere.\n",
      "\n",
      "Would you like me to check streaming availability for any of these, or tailor further (e.g., more action-heavy, lighter tone, or runtime preferences)?\n",
      "\n",
      "User: Also, what's the weather like in San Francisco?\n",
      "\n",
      "Assistant: Hi Alice! Hereâ€™s the latest for San Francisco:\n",
      "\n",
      "- Temperature: about 76Â°F (75.7Â°F), so pleasantly warm.\n",
      "- Sky: clear skies (sunny).\n",
      "\n",
      "Want a short-term forecast (next few hours or days) or a detailed 7-day outlook? I can pull that up. And if youâ€™d like, I can tailor movie recaps as well (e.g., more action-packed sci-fi with warm-weather vibes or lighter tone).\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create more realistic tools\n",
    "@tool\n",
    "def get_user_preferences(user_id: str) -> str:\n",
    "    \"\"\"Get a user's saved preferences.\"\"\"\n",
    "    # Simulate a user database\n",
    "    preferences = {\n",
    "        \"alice\": \"Loves sci-fi movies, prefers warm weather destinations\",\n",
    "        \"bob\": \"Enjoys comedy films, likes cold climates for travel\"\n",
    "    }\n",
    "    return preferences.get(user_id.lower(), \"No preferences found\")\n",
    "\n",
    "@tool\n",
    "def book_recommendation(genre: str, user_preferences: str = \"\") -> str:\n",
    "    \"\"\"Get personalized movie recommendations based on genre and user preferences.\"\"\"\n",
    "    recommendations = {\n",
    "        \"sci-fi\": \"Based on your preferences, try: Arrival, Ex Machina, or The Martian\",\n",
    "        \"comedy\": \"Based on your preferences, try: The Big Lebowski, Anchorman, or Bridesmaids\"\n",
    "    }\n",
    "    return recommendations.get(genre.lower(), \"No recommendations available\")\n",
    "\n",
    "# Create a helpful assistant agent\n",
    "assistant = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[get_weather, get_user_preferences, book_recommendation],\n",
    "    system_prompt=\"\"\"You are a helpful personal assistant. \n",
    "    \n",
    "    You can:\n",
    "    - Check weather for any city\n",
    "    - Look up user preferences\n",
    "    - Recommend movies based on preferences\n",
    "    \n",
    "    Always be friendly and personalize your responses based on user preferences.\"\"\",\n",
    "    checkpointer=MemorySaver()\n",
    ")\n",
    "\n",
    "# Demo conversation\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PERSONAL ASSISTANT DEMO\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Interaction 1\n",
    "print(\"User: Hi, I'm Alice. Can you check my preferences and recommend a movie?\\n\")\n",
    "result = assistant.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I'm Alice. Can you check my preferences and recommend a movie?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"Assistant: {result['messages'][-1].content}\\n\")\n",
    "\n",
    "# Interaction 2\n",
    "print(\"User: Also, what's the weather like in San Francisco?\\n\")\n",
    "result = assistant.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Also, what's the weather like in San Francisco?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"Assistant: {result['messages'][-1].content}\\n\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Next Steps - Exploring LangGraph Primitives\n",
    "\n",
    "We've been using `create_agent()`, which is built on **LangGraph**. LangGraph gives you full control over agent behavior using three core primitives:\n",
    "\n",
    "### Core LangGraph Concepts:\n",
    "\n",
    "1. **State** \n",
    "   - Shared data structure passed between nodes\n",
    "   - Represents the agent's \"memory\"\n",
    "   - Can include messages, custom data, etc.\n",
    "\n",
    "2. **Nodes** \n",
    "   - Python functions that process state\n",
    "   - Each node performs a specific task\n",
    "   - Examples: call LLM, execute tool, validate input\n",
    "\n",
    "3. **Edges** \n",
    "   - Define flow between nodes\n",
    "   - Can be normal (always go to next node)\n",
    "   - Or conditional (decide based on logic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use `create_agent()` vs custom LangGraph?\n",
    "\n",
    "**Use `create_agent()` when:**\n",
    "- Building standard ReAct-style agents\n",
    "- You need quick prototyping\n",
    "- Default behavior works for your use case\n",
    "\n",
    "**Use custom LangGraph when:**\n",
    "- You need custom control flow (e.g., approval workflows)\n",
    "- Building multi-agent systems\n",
    "- Implementing human-in-the-loop patterns\n",
    "- Complex state management requirements\n",
    "\n",
    "For more advanced patterns, check out:\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "- The `multi_agent.ipynb` notebook in this repo (LangGraph 201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've learned the core concepts of building agents with LangChain and LangGraph:\n",
    "\n",
    " **Models** - Standardized interface across providers  \n",
    " **Messages** - Building block of conversations  \n",
    " **Tools** - Extending LLM capabilities  \n",
    " **Agents** - Automated reasoning and action loops  \n",
    " **Memory** - Maintaining context across interactions  \n",
    " **Streaming** - Real-time user experience  \n",
    " **LangGraph** - The foundation powering it all\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Build your own agent** with your specific tools and use case\n",
    "2. **Explore advanced patterns** in the `multi_agent.ipynb` notebook\n",
    "3. **Add debugging** with [LangSmith](https://smith.langchain.com)\n",
    "4. **Deploy to production** using LangGraph's persistence and error recovery\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/introduction/)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangSmith for Debugging](https://smith.langchain.com)\n",
    "- [LangChain Academy](https://academy.langchain.com/)\n",
    "<br> \n",
    "<br> \n",
    "---\n",
    "<br> \n",
    "\n",
    "**Happy building!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
