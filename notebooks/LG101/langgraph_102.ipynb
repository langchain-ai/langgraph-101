{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Advanced Concepts: Middleware & Human-in-the-Loop\n",
    "\n",
    "Welcome to LangGraph Advanced Concepts! This notebook builds on the foundations from LangGraph 101 and introduces two powerful patterns for production agents.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **Human-in-the-Loop** - Pause agents for human review and approval\n",
    "- **Middleware** - Modify agent behavior at key points in execution\n",
    "- **Tool Review** - Add approval workflows to sensitive tools\n",
    "- **Dynamic Behavior** - Adapt agent responses based on context\n",
    "\n",
    "**Prerequisites:** Complete `langgraph_101.ipynb` \n",
    "</br>\n",
    "</br>\n",
    "\n",
    "---\n",
    "</br>\n",
    "\n",
    "> **Note:** These patterns are essential for production agents where safety, compliance, and user control are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's quickly set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Initialize model\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Human-in-the-Loop with Interrupts\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Imagine you're building an agent that can send emails or make purchases. You don't want it to take these actions automatically - you want human approval first!\n",
    "\n",
    "**Human-in-the-loop** lets you:\n",
    "- Pause execution for review\n",
    "- Approve, reject, or edit actions\n",
    "- Add safety controls to sensitive operations\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Agent encounters an `interrupt()` - execution pauses\n",
    "2. System surfaces information to human\n",
    "3. Human provides input (approve/reject/edit)\n",
    "4. Agent resumes with `Command(resume=...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Approval Workflow\n",
    "\n",
    "Let's start with a simple example - asking for approval before sending an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created successfully!\n",
      "Tool name: send_email\n",
      "Tool description: Send an email to a recipient.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import interrupt\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email to a recipient.\"\"\"\n",
    "    \n",
    "    # Pause for human approval\n",
    "    approval = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"Do you want to send this email?\"\n",
    "    })\n",
    "    \n",
    "    if approval.get(\"approved\"): # Will be true if accepted, false if declined\n",
    "        # In production, this would actually send the email\n",
    "        return f\" Email sent to {to} with subject '{subject}'\"\n",
    "    else:\n",
    "        return \"Email cancelled by user\"\n",
    "\n",
    "# Test the tool directly\n",
    "print(\"Tool created successfully!\")\n",
    "print(f\"Tool name: {send_email.name}\")\n",
    "print(f\"Tool description: {send_email.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Agent with Human-in-the-Loop\n",
    "\n",
    "Now let's create an agent that uses this tool. **Remember:** Interrupts require a checkpointer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "# Create checkpointer for persistence\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Create agent with the email tool\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[send_email],\n",
    "    system_prompt=\"You are a helpful email assistant. When asked to send emails, use the send_email tool.\",\n",
    "    checkpointer=checkpointer  # Required for interrupts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Until Interrupt\n",
    "\n",
    "Let's run the agent and see it pause for approval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent paused for approval\n",
      "\n",
      "Interrupt details:\n",
      "  To: alice@example.com\n",
      "  Subject: Meeting Tomorrow\n",
      "  Body: Let's meet at 3pm.\n",
      "  Message: Do you want to send this email?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a unique thread for this conversation\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run the agent and see it pause for approval\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to alice@example.com with subject 'Meeting Tomorrow' and body 'Let's meet at 3pm.'\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Check if we hit an interrupt\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Agent paused for approval\\n\")\n",
    "\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "\n",
    "    print(\"Interrupt details:\")\n",
    "    print(f\"  To: {interrupt_info.value['to']}\")\n",
    "    print(f\"  Subject: {interrupt_info.value['subject']}\")\n",
    "    print(f\"  Body: {interrupt_info.value['body']}\")\n",
    "    print(f\"  Message: {interrupt_info.value['message']}\")\n",
    "else:\n",
    "    print(\"Agent completed without interrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming with Approval\n",
    "\n",
    "Now let's approve the email and let the agent continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "Done. Email sent to alice@example.com with subject \"Meeting Tomorrow\" and body \"Let's meet at 3pm.\"\n",
      "\n",
      "Would you like me to:\n",
      "- set a reminder or calendar event for you,\n",
      "- CC/BCC someone,\n",
      "- or adjust the message (time, location, or add details)?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "# Resume with approval\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": True}),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the final response\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try Rejecting the Email\n",
    "\n",
    "Run the cells again, but this time reject the email by passing `{\"approved\": False}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "I attempted to send the email, but it was canceled. Would you like me to try again with the same content to bob@example.com (Subject: Hello!, Body: Hello!), or would you prefer to change the subject or body?\n",
      "\n",
      "If you want me to proceed as originally requested, reply with: \"Resend.\" If you want changes, tell me the new subject and/or body.\n"
     ]
    }
   ],
   "source": [
    "# New thread for rejection example\n",
    "thread_id_2 = str(uuid.uuid4())\n",
    "config_2 = {\"configurable\": {\"thread_id\": thread_id_2}}\n",
    "\n",
    "# Run until interrupt\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to bob@example.com saying 'Hello!'\")]\n",
    "    },\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "# Resume with rejection\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": False}),  # Reject the email\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Pattern - Edit Before Execution\n",
    "\n",
    "Sometimes you want to **edit** the tool call, not just approve/reject it. Let's enhance our tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def send_email_v2(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email to a recipient.\"\"\"\n",
    "    \n",
    "    # Pause for human review\n",
    "    response = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"Review this email. You can approve, reject, or edit it.\"\n",
    "    })\n",
    "    \n",
    "    # Handle different response types\n",
    "    if response[\"type\"] == \"approve\":\n",
    "        return f\"Email sent to {to} with subject '{subject}'\"\n",
    "\n",
    "    elif response[\"type\"] == \"reject\":\n",
    "        return \"Email cancelled\"\n",
    "\n",
    "    elif response[\"type\"] == \"edit\":\n",
    "        # Use edited values\n",
    "        to = response.get(\"to\", to)\n",
    "        subject = response.get(\"subject\", subject)\n",
    "        body = response.get(\"body\", body)\n",
    "        return f\"\"\"Email sent with edits:\n",
    "                To: {to}\n",
    "                Subject: {subject}\n",
    "                Body: {body}\"\"\"\n",
    "    \n",
    "    return \"Unknown response\"\n",
    "\n",
    "# Create new agent with enhanced tool\n",
    "agent_v2 = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[send_email_v2],\n",
    "    system_prompt=\"You are a helpful email assistant.\",\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused for review...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run and edit the email\n",
    "thread_id_3 = str(uuid.uuid4())\n",
    "config_3 = {\"configurable\": {\"thread_id\": thread_id_3}}\n",
    "\n",
    "# Run until interrupt\n",
    "result = agent_v2.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Send an email to team@example.com about the meeting\")]\n",
    "    },\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"Paused for review...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets edit the email subject to make it URGENT meeting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response:\n",
      "I can draft and send the email, but I need a few details to fill in. Do you want me to proceed with a draft like this, or would you prefer to customize?\n",
      "\n",
      "Option A — Quick draft (no specifics)\n",
      "Subject: Meeting — details to follow\n",
      "Body:\n",
      "Hi Team,\n",
      "This is a reminder about our upcoming meeting. Please look out for the final details (date, time, location/link) and be prepared to discuss the agenda.\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Option B — More complete draft (with placeholders)\n",
      "Subject: Meeting on [date] at [time]\n",
      "Body:\n",
      "Hi Team,\n",
      "This is a reminder about our meeting on [date] at [time]. Location: [location or video call link].\n",
      "Agenda:\n",
      "- [Topic 1]\n",
      "- [Topic 2]\n",
      "Please reply if you have additional topics to add or if you cannot attend.\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Please confirm:\n",
      "- Which draft would you like (A or B), or provide the specifics (date, time, location/link, agenda, your name) and I’ll tailor it.\n",
      "And do you want me to send it now to team@example.com?\n"
     ]
    }
   ],
   "source": [
    "# Resume with edits\n",
    "result = agent_v2.invoke(\n",
    "    Command(resume={\n",
    "        \"type\": \"edit\",\n",
    "        \"subject\": \"URGENT: Meeting Today at 2pm\",  # We have edited the email subject\n",
    "        \"body\": \"This is the edited email body with more details.\"\n",
    "    }),\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Introduction to Middleware\n",
    "\n",
    "**Middleware** provides fine-grained control over the agent loop. It lets you:\n",
    "- Inspect state before/after model calls\n",
    "- Modify model requests dynamically\n",
    "- Add custom logic at key execution points\n",
    "\n",
    "### The Agent Loop\n",
    "\n",
    "```\n",
    "Input --> [before_model] --> [modify_model_request] --> Model --> [after_model] --> Tools --> ...\n",
    "```\n",
    "\n",
    "Middleware hooks into this loop:\n",
    "- **`before_model`** - Runs before model execution, can update state\n",
    "- **`modify_model_request`** - Modifies the request (prompt, model, tools)\n",
    "- **`after_model`** - Runs after model execution, before tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Dynamic System Prompt\n",
    "\n",
    "Let's create middleware that changes the system prompt based on the user's role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware.types import modify_model_request, AgentState, ModelRequest\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define context schema\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "# Create middleware using decorator\n",
    "@modify_model_request\n",
    "def dynamic_prompt_middleware(request: ModelRequest, state: AgentState, runtime: Runtime[Context]) -> ModelRequest:\n",
    "    \"\"\"Adjust system prompt based on user role.\"\"\"\n",
    "    \n",
    "    user_role = runtime.context.get(\"user_role\", \"general\")\n",
    "    \n",
    "    if user_role == \"expert\":\n",
    "        request.system_prompt = \"You are an AI assistant for experts. Provide detailed technical responses with code examples.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        request.system_prompt = \"You are an AI assistant for beginners. Explain concepts simply, avoid jargon.\"\n",
    "    else:\n",
    "        request.system_prompt = \"You are a helpful AI assistant.\"\n",
    "    \n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def explain_concept(concept: str) -> str:\n",
    "    \"\"\"Explain a programming concept.\"\"\"\n",
    "    explanations = {\n",
    "        \"async\": \"Asynchronous programming allows code to run without blocking.\",\n",
    "        \"recursion\": \"Recursion is when a function calls itself.\"\n",
    "    }\n",
    "    return explanations.get(concept.lower(), \"Concept not found.\")\n",
    "\n",
    "# Create agent with middleware\n",
    "agent_with_middleware = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[explain_concept],\n",
    "    middleware=[dynamic_prompt_middleware],\n",
    "    context_schema=Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different User Roles\n",
    "\n",
    "Let's see how the agent responds differently based on user role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EXPERT USER\n",
      "==================================================\n",
      "Here’s a concise but thorough overview of async programming, plus practical code examples in JavaScript (Node.js) and Python (asyncio) to illustrate.\n",
      "\n",
      "What is async programming (at a high level)\n",
      "- Concurrency model: Instead of blocking a thread while waiting for I/O, async code uses non-blocking I/O and a scheduler (event loop) to run other work while the I/O operation completes.\n",
      "- Core ideas:\n",
      "  - Coroutines or async tasks: units of work that can suspend and resume.\n",
      "  - Futures/Promises: placeholders for results that will become available later.\n",
      "  - Awaitables: values that can be awaited (in Python, any object with __await__; in JS, a Promise or thenable).\n",
      "  - Event loop: central loop that dispatches ready I/O events, resumes awaiting coroutines, and runs callbacks.\n",
      "- I/O-bound vs CPU-bound:\n",
      "  - Async is most beneficial for I/O-bound tasks (network, disk, IPC) where the program spends time waiting.\n",
      "  - For CPU-bound work, async alone won’t speed things up; you generally need worker threads/processes or separate processes.\n",
      "- Key tradeoffs:\n",
      "  - Complexity and debugging can be harder (callbacks, state machines, cancellation).\n",
      "  - You gain scalability with fewer threads but must manage asynchronous primitives, cancellation, backpressure, and error propagation.\n",
      "\n",
      "JavaScript / Node.js perspective\n",
      "- Model: single-threaded event loop with asynchronous I/O. Non-blocking APIs and microtask/macrotask queues.\n",
      "- Core concepts:\n",
      "  - Promises: represent future results.\n",
      "  - async/await: syntactic sugar over promises that makes asynchronous code look synchronous.\n",
      "  - Microtasks (Promise callbacks) vs macrotasks (setTimeout, I/O callbacks).\n",
      "  - Error handling: use try/catch with await, or catch on Promises.\n",
      "  - Cancellation: historically via AbortController for fetch and other APIs, or manual cancellation patterns.\n",
      "- Simple example: sequential vs parallel\n",
      "  - Sequential (simulate two I/O tasks)\n",
      "    async function fetchTwoUrls(url1, url2) {\n",
      "      const res1 = await fetch(url1);\n",
      "      const data1 = await res1.text();\n",
      "      const res2 = await fetch(url2);\n",
      "      const data2 = await res2.text();\n",
      "      return [data1.length, data2.length];\n",
      "    }\n",
      "  - Parallel using Promise.all\n",
      "    async function fetchTwoUrlsInParallel(url1, url2) {\n",
      "      const p1 = fetch(url1).then(r => r.text());\n",
      "      const p2 = fetch(url2).then(r => r.text());\n",
      "      const [data1, data2] = await Promise.all([p1, p2]);\n",
      "      return [data1.length, data2.length];\n",
      "    }\n",
      "\n",
      "- Example: using async/await with error handling\n",
      "  async function main() {\n",
      "    try {\n",
      "      const resp = await fetch('https://example.com');\n",
      "      if (!resp.ok) throw new Error('Network response was not ok');\n",
      "      const text = await resp.text();\n",
      "      console.log('Content length:', text.length);\n",
      "    } catch (err) {\n",
      "      console.error('Request failed:', err);\n",
      "    }\n",
      "  }\n",
      "  main();\n",
      "\n",
      "- Practical patterns:\n",
      "  - Parallel I/O: Promise.all, Promise.allSettled, Promise.any.\n",
      "  - Timeouts and cancellation: use AbortController to cancel fetches.\n",
      "  - Streams and backpressure: async iteration with for await (const chunk of stream) { … }.\n",
      "  - Async composition: compose pipelines of async steps.\n",
      "\n",
      "Python asyncio perspective\n",
      "- Model: event loop + coroutines (async def), awaitables, tasks, and futures.\n",
      "- Core concepts:\n",
      "  - asyncio.run(main()): entry point to run an async function.\n",
      "  - async def: define a coroutine.\n",
      "  - await: suspend until the awaited awaitable completes.\n",
      "  - asyncio.create_task / asyncio.gather: schedule concurrent work.\n",
      "  - Cancellation: cancel tasks to stop work early (CancelledError).\n",
      "  - Synchronization: asyncio.Lock, asyncio.Semaphore, asyncio.Event for coordination.\n",
      "- Simple example: two HTTP requests concurrently using aiohttp\n",
      "  import asyncio\n",
      "  import aiohttp\n",
      "\n",
      "  async def fetch(session, url):\n",
      "      async with session.get(url) as resp:\n",
      "          text = await resp.text()\n",
      "          return url, len(text)\n",
      "\n",
      "  async def main():\n",
      "      urls = [\n",
      "          'https://example.com',\n",
      "          'https://httpbin.org/get',\n",
      "          'https://www.python.org',\n",
      "      ]\n",
      "      async with aiohttp.ClientSession() as session:\n",
      "          tasks = [fetch(session, url) for url in urls]\n",
      "          results = await asyncio.gather(*tasks, return_exceptions=True)\n",
      "          for url, length in results:\n",
      "              print(f'{url}: {length} chars')\n",
      "\n",
      "  if __name__ == '__main__':\n",
      "      asyncio.run(main())\n",
      "\n",
      "- Concurrency patterns in asyncio:\n",
      "  - Limiting concurrency:\n",
      "    async def worker(sema, session, queue):\n",
      "        async with sema:\n",
      "            url = await queue.get()\n",
      "            try:\n",
      "                return await fetch(session, url)\n",
      "            finally:\n",
      "                queue.task_done()\n",
      "\n",
      "    async def main():\n",
      "        sema = asyncio.Semaphore(10)  # limit concurrent requests\n",
      "        queue = asyncio.Queue()\n",
      "        for u in urls: queue.put_nowait(u)\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            tasks = [asyncio.create_task(worker(sema, session, queue)) for _ in range(len(urls))]\n",
      "            await asyncio.gather(*tasks)\n",
      "\n",
      "  - Async streams and asynchronous iteration:\n",
      "    async def read_lines_async(path):\n",
      "        async with aiofiles.open(path, 'r') as f:\n",
      "            async for line in f:\n",
      "                yield line.rstrip()\n",
      "\n",
      "  - Task groups (Python 3.11+):\n",
      "    async with asyncio.TaskGroup() as g:\n",
      "        g.create_task(coro1())\n",
      "        g.create_task(coro2())\n",
      "\n",
      "- Error handling:\n",
      "  - Exceptions raised in awaited coroutines propagate to the awaiting task.\n",
      "  - asyncio.gather(..., return_exceptions=True) collects exceptions instead of failing fast.\n",
      "\n",
      "When to use async programming\n",
      "- Use async when your program spends most of its time waiting on I/O (HTTP requests, database calls, file I/O, messaging).\n",
      "- For servers (APIs, workers), async can improve throughput and resource utilization.\n",
      "- For CLI tools or services with heavy CPU work, consider:\n",
      "  - Offloading CPU-bound work to separate processes or worker pools.\n",
      "  - In Python, use multiprocessing or run CPU-heavy tasks in separate processes.\n",
      "  - In Node.js, offload CPU work to worker_threads or child_process.\n",
      "\n",
      "Common pitfalls and best practices\n",
      "- Blocking code kills asynchrony:\n",
      "  - Don’t run sync I/O (e.g., blocking file I/O, sync database drivers) inside an async event loop.\n",
      "  - Use asynchronous libraries (aiohttp instead of requests, aiomysql/aiopg instead of PyMySQL/basics, etc.).\n",
      "- Error handling:\n",
      "  - Always handle exceptions in async paths. Unhandled rejections can crash processes or leave tasks hanging.\n",
      "- Cancellation:\n",
      "  - Support cancellation in long-running coroutines; properly catch asyncio.CancelledError or use AbortController-like patterns in JS.\n",
      "- Resource leaks:\n",
      "  - Ensure resources (sessions, connections) are closed properly via context managers (async with) or finally blocks.\n",
      "- State synchronization:\n",
      "  - Be careful with shared mutable state across coroutines; use locks, queues, or atomic operations to avoid data races.\n",
      "- Backpressure and streaming:\n",
      "  - Use async iterators and async generators for streaming data; apply backpressure to prevent overwhelming consumers.\n",
      "- Testing:\n",
      "  - In Python, use pytest-asyncio or asyncio.run-based tests.\n",
      "  - In JS, use testing frameworks that support async tests (jest, mocha with done/return promises).\n",
      "\n",
      "Quick cheat sheet\n",
      "- JavaScript:\n",
      "  - async function f() { return value; } // returns a Promise\n",
      "  - const x = await f(); // inside another async function\n",
      "  - Promise.all([p1, p2, p3]); // run in parallel\n",
      "  - Use try/catch around awaits for error handling\n",
      "  - Use AbortController to cancel fetch requests\n",
      "- Python:\n",
      "  - async def f(): …; return value\n",
      "  - result = await f()\n",
      "  - asyncio.run(main()) to start\n",
      "  - asyncio.create_task(coro()) to schedule; await asyncio.gather(...)\n",
      "  - Use async with for proper resource management (aiohttp, aiofiles)\n",
      "\n",
      "If you have a specific language, framework, or scenario (e.g., building a fast HTTP API, streaming data processing, or a long-running background task), tell me and I’ll tailor a detailed example and a step-by-step architecture plan.\n",
      "\n",
      "==================================================\n",
      "BEGINNER USER\n",
      "==================================================\n",
      "Async programming is a way to do many things at once without waiting for each task to finish before starting the next one. It helps your programs stay responsive and make good use of time spent waiting for slow operations (like network requests or reading a file).\n",
      "\n",
      "Key ideas, in plain terms:\n",
      "- Blocking vs non-blocking: In traditional (synchronous) code, a task blocks the program until it finishes (you wait idle). Async code starts a task and moves on to do other things.\n",
      "- Concurrency vs parallelism: Concurrency means your program is handling multiple tasks at once. Parallelism means those tasks are actually running at the same time on multiple cores. Async programming is mostly about concurrency and efficient waiting, not magically making CPU work faster.\n",
      "- Non-blocking I/O: Async code is great for I/O tasks (network, disk) because it lets your program do other work while waiting for the I/O to complete, instead of sitting idle.\n",
      "- Event loop: Many async systems use an event loop that keeps track of tasks that are waiting and those that are ready to run. It switches between them as things become ready.\n",
      "\n",
      "Common concepts you’ll see:\n",
      "- Callbacks: functions that run when another task finishes.\n",
      "- Promises/Futures: placeholders for results that aren’t ready yet.\n",
      "- Async/await: syntax that lets you write code that looks sequential but actually yields control while waiting.\n",
      "\n",
      "Simple examples (very brief):\n",
      "\n",
      "JavaScript (async/await)\n",
      "- Goal: fetch data from a URL without blocking the rest of your code.\n",
      "- Concept:\n",
      "  - await pauses the function until the awaited task finishes, but doesn’t block the whole program.\n",
      "  - Meanwhile, other tasks can run.\n",
      "\n",
      "Example:\n",
      "- async function getUser(userId) {\n",
      "-   const res = await fetch(`https://api.example.com/user/${userId}`);\n",
      "-   const data = await res.json();\n",
      "-   return data;\n",
      "- }\n",
      "- // Use:\n",
      "- getUser(123).then(user => console.log(user)).catch(console.error);\n",
      "\n",
      "Python (asyncio with async/await)\n",
      "- Goal: run IO-bound tasks without blocking the event loop.\n",
      "- Concept:\n",
      "  - async defines a function that can pause with await.\n",
      "  - await waits for the result without blocking other tasks.\n",
      "\n",
      "Example:\n",
      "- import asyncio\n",
      "- \n",
      "- async def say_after(delay, what):\n",
      "-     await asyncio.sleep(delay)\n",
      "-     print(what)\n",
      "- \n",
      "- async def main():\n",
      "-     task1 = asyncio.create_task(say_after(1, 'hello'))\n",
      "-     task2 = asyncio.create_task(say_after(2, 'world'))\n",
      "-     await task1\n",
      "-     await task2\n",
      "- \n",
      "- asyncio.run(main())\n",
      "\n",
      "When to use async programming:\n",
      "- Use it for IO-bound tasks (network requests, file access, database calls).\n",
      "- If your program mostly does CPU work (heavy calculations), async may not help much and can even add overhead.\n",
      "\n",
      "Common pitfalls:\n",
      "- Blocking code inside async functions (e.g., a slow, synchronous function) blocks the event loop. Use async equivalents or run blocking code in a separate thread.\n",
      "- Not handling errors properly; asynchronous errors can be trickier to track.\n",
      "- Misunderstanding: async does not automatically make things faster; it makes your program more responsive by not waiting unnecessarily.\n",
      "\n",
      "If you tell me your preferred language (JavaScript, Python, etc.) I can tailor simple, runnable examples for you.\n"
     ]
    }
   ],
   "source": [
    "# Expert user\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERT USER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)\n",
    "print()\n",
    "\n",
    "# Beginner user\n",
    "print(\"=\" * 50)\n",
    "print(\"BEGINNER USER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Explain async programming\")]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Example 2: Custom Middleware - Request Logger\n",
    "\n",
    "  Middleware lets you hook into the agent loop and see what's happening at each step. This is incredibly useful for debugging and understanding how your agent works.\n",
    "\n",
    "  **The Agent Loop:**\n",
    "  User Input --> [before_model] --> [modify_model_request] --> Model --> [after_model] --> Tools --> ...\n",
    "\n",
    "  **What we'll build:**\n",
    "  A logger that prints information at each step:\n",
    "  - **Before model** - How many messages are in the conversation?\n",
    "  - **Model request** - Which model and tools are being used?\n",
    "  - **After model** - Did the model call a tool or give a final answer?\n",
    "\n",
    "  This is like adding debug `print()` statements, but in a clean, reusable way!\n",
    "\n",
    "  Let's create middleware that logs model requests for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware\n",
    "from typing import Any\n",
    "\n",
    "class RequestLoggerMiddleware(AgentMiddleware):\n",
    "    \"\"\"Logs all model requests for debugging.\"\"\"\n",
    "    \n",
    "    name = \"request_logger\"\n",
    "    \n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"Log before model execution.\"\"\"\n",
    "        message_count = len(state.get(\"messages\", []))\n",
    "        print(f\"[BEFORE MODEL] Processing {message_count} messages\")\n",
    "        return None  # Don't modify state\n",
    "    \n",
    "    def modify_model_request(self, request: ModelRequest, state: AgentState) -> ModelRequest:\n",
    "        \"\"\"Log the model request details.\"\"\"\n",
    "        print(f\"  [MODEL REQUEST]\")\n",
    "        print(f\"   Model: {request.model if hasattr(request, 'model') else 'default'}\")\n",
    "        print(f\"   Tools available: {len(request.tools) if request.tools else 0}\")\n",
    "        return request\n",
    "    \n",
    "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"Log after model execution.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(f\" [AFTER MODEL] Model requested {len(last_message.tool_calls)} tool call(s)\")\n",
    "        else:\n",
    "            print(f\" [AFTER MODEL] Model provided final response\")\n",
    "        return None  # Don't modify state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with logger middleware\n",
    "agent_with_logger = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[explain_concept],\n",
    "    middleware=[RequestLoggerMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### What to Expect\n",
    "\n",
    "  When we run the agent with the logger, you'll see the execution flow in real-time:\n",
    "\n",
    "  **First iteration:**\n",
    "  1. `[BEFORE MODEL]` - Shows how many messages we're starting with\n",
    "  2. `[MODEL REQUEST]` - Shows which model and tools are available\n",
    "  3. `[AFTER MODEL]` - The model decides to call the `explain_concept` tool\n",
    "\n",
    "  **Second iteration (after tool execution):**\n",
    "  1. `[BEFORE MODEL]` - Now we have more messages (including tool result)\n",
    "  2. `[MODEL REQUEST]` - Model info again\n",
    "  3. `[AFTER MODEL]` - Model provides the final answer (no more tools needed)\n",
    "\n",
    "  This gives you a detailed view into your agent's decision-making process\n",
    "\n",
    "  Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RUNNING AGENT WITH LOGGER\n",
      "==================================================\n",
      "\n",
      "[BEFORE MODEL] Processing 1 messages\n",
      "  [MODEL REQUEST]\n",
      "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x11de62f10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11e32c270> root_client=<openai.OpenAI object at 0x11de63020> root_async_client=<openai.AsyncOpenAI object at 0x11e32c050> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "   Tools available: 1\n",
      " [AFTER MODEL] Model requested 1 tool call(s)\n",
      "[BEFORE MODEL] Processing 3 messages\n",
      "  [MODEL REQUEST]\n",
      "   Model: client=<openai.resources.chat.completions.completions.Completions object at 0x11de62f10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11e32c270> root_client=<openai.OpenAI object at 0x11de63020> root_async_client=<openai.AsyncOpenAI object at 0x11e32c050> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "   Tools available: 1\n"
     ]
    }
   ],
   "source": [
    "# Run and observe the logs\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RUNNING AGENT WITH LOGGER\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "result = agent_with_logger.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain recursion\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESPONSE\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Combining Middleware and Human-in-the-loop\n",
    "\n",
    "Let's combine human-in-the-loop AND middleware for a production-ready agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitive tool that needs approval\n",
    "@tool\n",
    "def delete_database(database_name: str) -> str:\n",
    "    \"\"\"Delete a database. THIS IS DANGEROUS!\"\"\"\n",
    "    \n",
    "    response = interrupt({\n",
    "        \"action\": \"delete_database\",\n",
    "        \"database_name\": database_name,\n",
    "        \"warning\": \"This will permanently delete the database!\",\n",
    "        \"message\": \"Are you absolutely sure?\"\n",
    "    })\n",
    "    \n",
    "    if response.get(\"confirmed\"):\n",
    "        return f\"Database '{database_name}' has been deleted (simulation)\"\n",
    "    else:\n",
    "        return \"Database deletion cancelled\"\n",
    "\n",
    "# Middleware to track dangerous operations\n",
    "class SafetyMiddleware(AgentMiddleware):\n",
    "    \"\"\"Add safety checks and logging.\"\"\"\n",
    "    \n",
    "    name = \"safety_checker\"\n",
    "    \n",
    "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"Check for dangerous tool calls.\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        \n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            for tool_call in last_message.tool_calls:\n",
    "                if \"delete\" in tool_call[\"name\"].lower():\n",
    "                    print(\"   [SAFETY] Dangerous operation detected!\")\n",
    "                    print(f\"   Tool: {tool_call['name']}\")\n",
    "                    print(f\"   Args: {tool_call['args']}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Create production agent\n",
    "production_agent = create_agent(\n",
    "    model=\"openai:gpt-5-nano\",\n",
    "    tools=[delete_database],\n",
    "    middleware=[SafetyMiddleware()],\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### What to Expect: Layered Safety in Action\n",
    "\n",
    "  When we attempt a dangerous operation, you'll see **both** safety mechanisms activate:\n",
    "\n",
    "  **Layer 1 - Middleware Detection:**\n",
    "  - `[SAFETY] Dangerous operation detected!` - Middleware spots the delete operation\n",
    "  - Logs the tool name and arguments for audit trails\n",
    "\n",
    "  **Layer 2 - Human Approval (Interrupt):**\n",
    "  - Agent execution pauses at the `interrupt()`\n",
    "  - Warning message displayed to human reviewer\n",
    "  - Execution won't continue until explicit approval\n",
    "\n",
    "  **This is defense-in-depth:** Middleware monitors ALL operations, while interrupts enforce human approval for critical actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DANGEROUS OPERATION ATTEMPT\n",
      "==================================================\n",
      "\n",
      "   [SAFETY] Dangerous operation detected!\n",
      "   Tool: delete_database\n",
      "   Args: {'database_name': 'production_db'}\n",
      "\n",
      "  Human approval required:\n",
      "   This will permanently delete the database!\n",
      "   Database: production_db\n",
      "\n",
      "(In a real app, a human would review this before proceeding)\n"
     ]
    }
   ],
   "source": [
    "# Test the combined pattern\n",
    "thread_id_4 = str(uuid.uuid4())\n",
    "config_4 = {\"configurable\": {\"thread_id\": thread_id_4}}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DANGEROUS OPERATION ATTEMPT\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Run until interrupt\n",
    "result = production_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Delete the production_db database\")]\n",
    "    },\n",
    "    config=config_4\n",
    ")\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "    print(\"\\n  Human approval required:\")\n",
    "    print(f\"   {interrupt_info.value['warning']}\")\n",
    "    print(f\"   Database: {interrupt_info.value['database_name']}\")\n",
    "\n",
    "print(\"\\n(In a real app, a human would review this before proceeding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Human-in-the-Loop (Interrupts)\n",
    "-  Use `interrupt()` to pause execution\n",
    "-  Requires a `checkpointer` for persistence\n",
    "-  Resume with `Command(resume=value)`\n",
    "-  Perfect for approval workflows and sensitive operations\n",
    "\n",
    "### Middleware\n",
    "-  `@modify_model_request` - Adjust prompts, models, tools dynamically\n",
    "-  `before_model` / `after_model` - Add custom logic at key points\n",
    "-  Subclass `AgentMiddleware` for reusable components\n",
    "-  Perfect for logging, safety checks, dynamic behavior\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "**Use Interrupts when:**\n",
    "- You need human approval for actions\n",
    "- You want to review/edit tool calls\n",
    "- You need to validate user input\n",
    "\n",
    "**Use Middleware when:**\n",
    "- You need to modify agent behavior dynamically\n",
    "- You want to add logging/monitoring\n",
    "- You need to enforce policies (token limits, safety checks)\n",
    "- You want to personalize responses based on context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise (Optional)\n",
    "\n",
    "Try building an agent that:\n",
    "1. Has a tool to make a purchase\n",
    "2. Uses middleware to check if the purchase amount is over $1000\n",
    "3. If over $1000, uses an interrupt to require approval\n",
    "4. If under $1000, processes automatically\n",
    "\n",
    "Hint: Combine `before_model` middleware with conditional `interrupt()` logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Challenge: Build the purchase approval agent\n",
    "\n",
    "# @tool\n",
    "# def make_purchase(item: str, amount: float) -> str:\n",
    "#     ...\n",
    "\n",
    "# class PurchaseLimitMiddleware(AgentMiddleware):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have powerful tools for building production agents!\n",
    "\n",
    "**Continue your journey:**\n",
    "1.  Check out `multi_agent.ipynb` for multi-agent systems\n",
    "2.  Explore built-in middleware (Summarization, Anthropic Prompt Caching)\n",
    "3.  Build your own custom middleware for your use case\n",
    "4.  Add LangSmith for debugging and monitoring\n",
    "\n",
    "**Resources:**\n",
    "- [Middleware Documentation](https://docs.langchain.com/oss/python/langchain/middleware)\n",
    "- [Human-in-the-Loop Guide](https://docs.langchain.com/oss/python/langchain/human-in-the-loop)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "\n",
    "</br>\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
