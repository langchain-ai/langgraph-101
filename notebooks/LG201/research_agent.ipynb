{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph 201: Building a Deep Research Agent\n",
    "\n",
    "In this notebook, we'll build a multi-agent research system that can conduct comprehensive web research. We'll progressively build up the system in three parts:\n",
    "\n",
    "1. **Part 1**: Single researcher agent that performs web searches\n",
    "2. **Part 2**: Supervisor agent that coordinates multiple researchers\n",
    "3. **Part 3**: Complete workflow with user clarification and report generation\n",
    "\n",
    "![arch](../../images/deep_research.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-work: Setup\n",
    "\n",
    "First, let's install dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API key required for researcher\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path().resolve().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import operator\n",
    "from datetime import datetime\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    filter_messages,\n",
    "    get_buffer_string,\n",
    "    MessageLikeRepresentation\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command, interrupt\n",
    "from utils.utils import show_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date formatted for display.\"\"\"\n",
    "    now = datetime.now()\n",
    "    return f\"{now:%a} {now:%b} {now.day}, {now:%Y}\"\n",
    "\n",
    "def openai_websearch_called(response):\n",
    "    \"\"\"Detect if OpenAI's web search was used.\"\"\"\n",
    "    try:\n",
    "        tool_outputs = response.additional_kwargs.get(\"tool_outputs\")\n",
    "        if not tool_outputs:\n",
    "            return False\n",
    "        for tool_output in tool_outputs:\n",
    "            if tool_output.get(\"type\") == \"web_search_call\":\n",
    "                return True\n",
    "        return False\n",
    "    except (AttributeError, TypeError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "Let's define our hardcoded configuration. In a production system, these would be configurable, but for this educational example we'll keep them simple and visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "RESEARCH_MODEL = \"openai:gpt-4.1-mini\" # Hard requirement for this notebook\n",
    "MAX_OUTPUT_TOKENS = 10000\n",
    "\n",
    "# Research limits\n",
    "MAX_RESEARCHER_ITERATIONS = 3  # How many times supervisor can delegate\n",
    "MAX_REACT_TOOL_CALLS = 10      # Max tool calls per researcher\n",
    "MAX_CONCURRENT_RESEARCH_UNITS = 5  # Max parallel researchers\n",
    "MAX_STRUCTURED_OUTPUT_RETRIES = 3\n",
    "\n",
    "# Initialize model\n",
    "def get_model():\n",
    "    return init_chat_model(\n",
    "        model=RESEARCH_MODEL,\n",
    "        max_tokens=MAX_OUTPUT_TOKENS,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        use_responses_api=True\n",
    "    )\n",
    "\n",
    "print(f\"✓ Configuration set: {RESEARCH_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Single Researcher Agent\n",
    "\n",
    "We'll start by building a single researcher agent that can:\n",
    "1. Receive a research topic\n",
    "2. Use web search to gather information (using OpenAI native web search)\n",
    "3. Compress the findings into a summary\n",
    "\n",
    "![arch](../../images/researcher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define State\n",
    "\n",
    "How does information flow through the steps?  \n",
    "\n",
    "State is the first LangGraph concept we'll cover. **State can be thought of as the memory of the agent - its a shared data structure that’s passed on between the nodes of your graph**, representing the current snapshot of your application. \n",
    "\n",
    "For our Researcher, we'll define 3 fields to track: the conversation history, the topic, and how many API calls it has made. \n",
    "\n",
    "We can also define specific output State to indicate what we expect the researcher to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearcherState(TypedDict):\n",
    "    \"\"\"State for a single researcher agent.\"\"\"\n",
    "    researcher_messages: Annotated[list[MessageLikeRepresentation], operator.add]\n",
    "    research_topic: str        # What to research\n",
    "    tool_call_iterations: int  # How many tool calls made\n",
    "\n",
    "class ResearcherOutputState(TypedDict):\n",
    "    \"\"\"Output from researcher - just the compressed findings.\"\"\"\n",
    "    researcher_messages: Annotated[list[MessageLikeRepresentation], operator.add]\n",
    "    compressed_research: str   # Summary of findings\n",
    "    raw_notes: list           # Raw notes for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Tools\n",
    "\n",
    "Let's define a list of **tools** our agent will have access to. Tools are functionts that can act as extension of the LLM's capabilities. \n",
    "\n",
    "We'll define some no-op tools that will allow our researcher to reflect and indicate when research is complete.\n",
    "\n",
    "Finally, to actually conduct research, our agent needs access to web search. We'll use OpenAI's native web search capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"Signal that research is complete\")\n",
    "def ResearchComplete() -> str:\n",
    "    \"\"\"Call this when you have gathered enough information to answer the research question.\"\"\"\n",
    "    return \"Research marked as complete\"\n",
    "\n",
    "@tool(description=\"Strategic reflection tool for research planning\")\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Use this tool after each search to analyze results and plan next steps.\n",
    "    \n",
    "    Args:\n",
    "        reflection: Detailed reflection on research progress and next steps\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\"\n",
    "\n",
    "def get_all_tools():\n",
    "    \"\"\"Get all available research tools.\"\"\"\n",
    "    tools = [ResearchComplete, think_tool]\n",
    "    # OpenAI's native web search - bind it so model knows it's available, but we don't execute it\n",
    "    tools.append({\"type\": \"web_search_preview\"})\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define Nodes\n",
    "\n",
    "Now that we have a list of tools, we are ready to build nodes that interact with them. \n",
    "\n",
    "Nodes are just python (or JS/TS!) functions. Nodes take in your graph's State as input, execute some logic, and return a new State. \n",
    "\n",
    "Here, we're just going to set up 2 nodes for our ReAct agent:\n",
    "1. **researcher**: Reasoning node that decides which function to invoke \n",
    "2. **researcher_tools**: Node that contains all the available tools and executes the function\n",
    "\n",
    "We'll start with the reasoning node, which will utilize a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_system_prompt = \"\"\"You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your job is to use tools to gather information about the user's input topic.\n",
    "You can use any of the tools provided to you to find resources that can help answer the research question.\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "You have access to:\n",
    "1. **Web search**: For conducting web searches to gather information\n",
    "2. **think_tool**: For reflection and strategic planning during research\n",
    "\n",
    "**CRITICAL: Use think_tool after each search to reflect on results and plan next steps.**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a human researcher with limited time:\n",
    "\n",
    "1. **Read the question carefully** - What specific information does the user need?\n",
    "2. **Start with broader searches** - Use broad, comprehensive queries first\n",
    "3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
    "5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "**Tool Call Budgets**:\n",
    "- **Simple queries**: Use 2-3 search tool calls maximum\n",
    "- **Complex queries**: Use up to 4 search tool calls maximum\n",
    "- **Always stop**: After 4 search tool calls if you cannot find the right sources\n",
    "\n",
    "**Stop Immediately When**:\n",
    "- You can answer the user's question comprehensively\n",
    "- You have 3+ relevant examples/sources for the question\n",
    "- Your last 2 searches returned similar information\n",
    "</Hard Limits>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this prompt in our reasoning node. We'll give our tools to our configured LLM model, and invoke it with our prompt to see what tools it wants to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def researcher(state: ResearcherState, config):\n",
    "    \"\"\"Main researcher node that conducts research.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Get research tools\n",
    "    tools = get_all_tools()    \n",
    "    # Prepare system prompt\n",
    "    researcher_prompt = research_system_prompt.format(date=get_today_str())\n",
    "    \n",
    "    # Configure model with tools\n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .bind_tools(tools)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Generate researcher response\n",
    "    messages = [SystemMessage(content=researcher_prompt)] + researcher_messages\n",
    "    response = await research_model.ainvoke(messages)\n",
    "    \n",
    "    # Update state\n",
    "    return {\n",
    "        \"researcher_messages\": [response],\n",
    "        \"tool_call_iterations\": state.get(\"tool_call_iterations\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the tools node to actually execute the tool calls that the LLM wants to make. We'll also introduce the concept of a Command, which is a special object that allows you to not only update the State, but also determine which node to execute next.\n",
    "\n",
    "Commands serve as an alternative to edges, which we'll cover next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_tool_safely(tool, args):\n",
    "    \"\"\"Safely execute a tool with error handling.\"\"\"\n",
    "    try:\n",
    "        return await tool.ainvoke(args)\n",
    "    except Exception as e:\n",
    "        return f\"Error executing tool: {str(e)}\"\n",
    "\n",
    "async def researcher_tools(state: ResearcherState, config) -> Command[Literal[\"researcher\", \"compress_research\"]]:\n",
    "    \"\"\"Execute tools called by the researcher.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    most_recent_message = researcher_messages[-1]\n",
    "    \n",
    "    # Check if there are tool calls or native search\n",
    "    has_tool_calls = bool(most_recent_message.tool_calls)\n",
    "    has_native_search = openai_websearch_called(most_recent_message)\n",
    "    \n",
    "    if not has_tool_calls and not has_native_search:\n",
    "        return Command(goto=\"compress_research\")\n",
    "    \n",
    "    # Execute tool calls\n",
    "    tools = get_all_tools()\n",
    "    tools_by_name = {\n",
    "        tool.name if hasattr(tool, \"name\") else tool.get(\"name\", \"web_search\"): tool\n",
    "        for tool in tools\n",
    "    }\n",
    "    \n",
    "    tool_calls = most_recent_message.tool_calls\n",
    "    tool_execution_tasks = [\n",
    "        execute_tool_safely(tools_by_name[tc[\"name\"]], tc[\"args\"])\n",
    "        for tc in tool_calls\n",
    "    ]\n",
    "    observations = await asyncio.gather(*tool_execution_tasks)\n",
    "    \n",
    "    # Create tool messages\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tc[\"name\"],\n",
    "            tool_call_id=tc[\"id\"]\n",
    "        )\n",
    "        for observation, tc in zip(observations, tool_calls)\n",
    "    ]\n",
    "    \n",
    "    # Check exit conditions\n",
    "    exceeded_iterations = state.get(\"tool_call_iterations\", 0) >= MAX_REACT_TOOL_CALLS\n",
    "    research_complete = any(\n",
    "        tc[\"name\"] == \"ResearchComplete\"\n",
    "        for tc in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or research_complete:\n",
    "        return Command(\n",
    "            goto=\"compress_research\",\n",
    "            update={\"researcher_messages\": tool_outputs}\n",
    "        )\n",
    "    \n",
    "    # Continue research loop\n",
    "    return Command(\n",
    "        goto=\"researcher\",\n",
    "        update={\"researcher_messages\": tool_outputs}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_research_system_prompt = \"\"\"You are a research assistant that has conducted research on a topic. Your job is to clean up the findings.\n",
    "\n",
    "<Task>\n",
    "Clean up information gathered from tool calls and web searches. All relevant information should be repeated verbatim.\n",
    "The purpose is just to remove obviously irrelevant or duplicative information.\n",
    "</Task>\n",
    "\n",
    "<Guidelines>\n",
    "1. Your output should be fully comprehensive and include ALL information and sources gathered\n",
    "2. Include inline citations [1], [2], etc. for each source\n",
    "3. Include a \"Sources\" section at the end listing all sources with citations\n",
    "4. Make sure to include ALL sources - a later LLM will merge this with other reports\n",
    "</Guidelines>\n",
    "\"\"\"\n",
    "\n",
    "async def compress_research(state: ResearcherState, config):\n",
    "    \"\"\"Compress and synthesize research findings.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Add compression instruction\n",
    "    researcher_messages.append(\n",
    "        HumanMessage(content=\"Please clean up these findings. DO NOT summarize - preserve all relevant information verbatim.\")\n",
    "    )\n",
    "    \n",
    "    # Create compression prompt\n",
    "    compression_prompt = compress_research_system_prompt\n",
    "    messages = [SystemMessage(content=compression_prompt)] + researcher_messages\n",
    "    \n",
    "    # Execute compression\n",
    "    response = await get_model().ainvoke(messages)\n",
    "    \n",
    "    # Extract raw notes\n",
    "    raw_notes_content = \"\\n\".join([\n",
    "        str(message.content)\n",
    "        for message in filter_messages(researcher_messages, include_types=[\"tool\", \"ai\"])\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [raw_notes_content]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define Edges and Build Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edges are connections between nodes. They define the flow of the graph.**\n",
    "* **Normal edges** are deterministic and always go from one node to its defined target\n",
    "* **Conditional edges** are used to dynamically route between nodes, implemented as functions that return the next node to visit based upon some logic. \n",
    "\n",
    "In this case, our conditional routing is handled by Commands, so we can link together the nodes of our graph using normal edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build researcher graph\n",
    "researcher_builder = StateGraph(\n",
    "    ResearcherState,\n",
    "    output_schema=ResearcherOutputState\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "researcher_builder.add_node(\"researcher\", researcher)\n",
    "researcher_builder.add_node(\"researcher_tools\", researcher_tools)\n",
    "researcher_builder.add_node(\"compress_research\", compress_research)\n",
    "\n",
    "# Add edges\n",
    "researcher_builder.add_edge(START, \"researcher\")\n",
    "researcher_builder.add_edge(\"researcher\", \"researcher_tools\")\n",
    "# researcher_tools uses Command to route to either researcher or compress_research\n",
    "researcher_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "# Compile\n",
    "researcher_graph = researcher_builder.compile()\n",
    "researcher_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Running the Single Researcher\n",
    "\n",
    "Let's test our researcher with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the researcher\n",
    "test_query = \"What are the best practices for prompt engineering with LLMs?\"\n",
    "\n",
    "initial_state = {\n",
    "    \"researcher_messages\": [HumanMessage(content=test_query)],\n",
    "    \"research_topic\": test_query,\n",
    "    \"tool_call_iterations\": 0\n",
    "}\n",
    "\n",
    "result = await researcher_graph.ainvoke(initial_state)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESEARCHER MESSAGE HISTORY:\")\n",
    "print(\"=\"*60)\n",
    "for message in result[\"researcher_messages\"]:\n",
    "    message.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESEARCH FINDINGS:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"compressed_research\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Supervisor Agent\n",
    "\n",
    "Now we'll build a supervisor agent that can delegate research to multiple researcher agents in parallel. This will build on the research agent we've created from Part 1.\n",
    "\n",
    "![arch](../../images/research_supervisor.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Supervisor State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define separate state for our Supervisor. This allows it to keep track different items than the Researcher, preventing it from getting distracted by noise. \n",
    "\n",
    "We also define a customer reducer for handling specific state keys. By default state updates overwrite existing values - we've defined a custom reducer to allow for concatenation alongside overwrite behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_reducer(current_value, new_value):\n",
    "    \"\"\"Reducer function that allows overriding values in state.\"\"\"\n",
    "    if isinstance(new_value, dict) and new_value.get(\"type\") == \"override\":\n",
    "        return new_value.get(\"value\", new_value)\n",
    "    else:\n",
    "        return operator.add(current_value, new_value)\n",
    "        \n",
    "class SupervisorState(TypedDict):\n",
    "    \"\"\"State for the supervisor agent.\"\"\"\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]\n",
    "    research_brief: str\n",
    "    notes: Annotated[list[str], override_reducer] = []\n",
    "    research_iterations: int = 0\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Supervisor Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-agent systems, the most common way to coordinate subagents today is to call them as tools. This allows subagents to isolate their context from the supervisor's orchestration, allowing specialization and focus.\n",
    "\n",
    "This is as simple as invoking our researcher subagent within a tool, and giving that tool to our supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"Delegate a research task to a specialized researcher\")\n",
    "async def ConductResearch(research_topic: str) -> dict:\n",
    "    \"\"\"Delegate a specific research topic to a researcher agent.\n",
    "    \n",
    "    Args:\n",
    "        research_topic: Clear, specific research question for the sub-agent\n",
    "    \"\"\"\n",
    "    # Actually invoke the researcher graph\n",
    "    result = await researcher_graph.ainvoke({\n",
    "        \"researcher_messages\": [HumanMessage(content=research_topic)],\n",
    "        \"research_topic\": research_topic,\n",
    "        \"tool_call_iterations\": 0\n",
    "    })\n",
    "    \n",
    "    # Return dictionary directly with both compressed research and raw notes\n",
    "    return {\n",
    "        \"compressed_research\": result.get(\"compressed_research\", \"Error in research\"),\n",
    "        \"raw_notes\": result.get(\"raw_notes\", [])\n",
    "    }\n",
    "\n",
    "@tool(description=\"Signal that all research is complete\")\n",
    "def ResearchComplete() -> str:\n",
    "    \"\"\"Call this when you have gathered all necessary information.\"\"\"\n",
    "    return \"Research marked as complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Supervisor Nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining the prompt for our reasoning node. Like the researcher agent, the reasoning node will rely on calling an LLM with our custom prompt to determine what tools to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_researcher_prompt = \"\"\"You are a research supervisor. Your job is to conduct research by calling the \"ConductResearch\" tool.\n",
    "\n",
    "<Task>\n",
    "Call the \"ConductResearch\" tool to delegate research. When you're satisfied with findings, call \"ResearchComplete\".\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "1. **ConductResearch**: Delegate research tasks to specialized sub-agents\n",
    "2. **ResearchComplete**: Indicate that research is complete\n",
    "3. **think_tool**: For reflection and strategic planning\n",
    "\n",
    "**CRITICAL: Use think_tool before calling ConductResearch to plan, and after to assess progress.**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a research manager:\n",
    "\n",
    "1. **Read the question carefully** - What specific information is needed?\n",
    "2. **Decide how to delegate** - Can multiple independent angles be explored simultaneously?\n",
    "3. **After each ConductResearch call, assess** - Do I have enough? What's missing?\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "- **Limit tool calls** - Stop after {max_researcher_iterations} tool calls if you cannot find the right sources\n",
    "- **Maximum {max_concurrent_research_units} parallel agents per iteration**\n",
    "</Hard Limits>\n",
    "\n",
    "<Scaling Rules>\n",
    "**Simple queries** - Use a single sub-agent\n",
    "**Comparisons** - Use a sub-agent for each element being compared\n",
    "**Important**: Provide complete standalone instructions when calling ConductResearch\n",
    "</Scaling Rules>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def supervisor(state: SupervisorState, config):\n",
    "    \"\"\"Supervisor agent that delegates research.\"\"\"\n",
    "    lead_researcher_tools = [ConductResearch, ResearchComplete, think_tool]\n",
    "    \n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .bind_tools(lead_researcher_tools)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    response = await research_model.ainvoke(supervisor_messages)\n",
    "    \n",
    "    return {\n",
    "        \"supervisor_messages\": [response],\n",
    "        \"research_iterations\": state.get(\"research_iterations\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll customize our tool node implementation to automatically fail tool calls that go above our predefined usage limit. As usual, this node will be responsible for executing the tools the agent wants to call, and updating the state with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_content(messages):\n",
    "    \"\"\"Extract notes from tool call messages.\"\"\"\n",
    "    return [tool_msg.content for tool_msg in filter_messages(messages, include_types=\"tool\")]\n",
    "\n",
    "async def supervisor_tools(state: SupervisorState, config) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Execute tools called by the supervisor.\"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "    \n",
    "    # Check exit conditions\n",
    "    exceeded_iterations = research_iterations > MAX_RESEARCHER_ITERATIONS\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete = any(\n",
    "        tc[\"name\"] == \"ResearchComplete\"\n",
    "        for tc in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or no_tool_calls or research_complete:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"notes\": extract_tool_content(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Process tool calls\n",
    "    all_tool_messages = []\n",
    "    update_payload = {\"supervisor_messages\": []}\n",
    "    \n",
    "    # Handle think_tool\n",
    "    for tc in most_recent_message.tool_calls:\n",
    "        if tc[\"name\"] == \"think_tool\":\n",
    "            all_tool_messages.append(ToolMessage(\n",
    "                content=f\"Reflection recorded: {tc['args']['reflection']}\",\n",
    "                name=\"think_tool\",\n",
    "                tool_call_id=tc[\"id\"]\n",
    "            ))\n",
    "    \n",
    "    # Handle ConductResearch - now executes the actual tool\n",
    "    conduct_research_calls = [\n",
    "        tc for tc in most_recent_message.tool_calls\n",
    "        if tc[\"name\"] == \"ConductResearch\"\n",
    "    ]\n",
    "    \n",
    "    if conduct_research_calls:\n",
    "        try:\n",
    "            allowed_calls = conduct_research_calls[:MAX_CONCURRENT_RESEARCH_UNITS]\n",
    "            overflow_calls = conduct_research_calls[MAX_CONCURRENT_RESEARCH_UNITS:]\n",
    "            \n",
    "            # Execute research tasks in parallel by invoking the tool\n",
    "            research_tasks = [\n",
    "                ConductResearch.ainvoke(tc[\"args\"])\n",
    "                for tc in allowed_calls\n",
    "            ]\n",
    "            \n",
    "            # Get results directly as dictionaries\n",
    "            tool_results = await asyncio.gather(*research_tasks)\n",
    "            \n",
    "            # Create tool messages with results\n",
    "            for observation_dict, tc in zip(tool_results, allowed_calls):\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=observation_dict.get(\"compressed_research\", \"Error in research\"),\n",
    "                    name=tc[\"name\"],\n",
    "                    tool_call_id=tc[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Handle overflow\n",
    "            for overflow_call in overflow_calls:\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=f\"Error: Exceeded max concurrent units ({MAX_CONCURRENT_RESEARCH_UNITS})\",\n",
    "                    name=\"ConductResearch\",\n",
    "                    tool_call_id=overflow_call[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Aggregate raw notes\n",
    "            raw_notes_concat = \"\\n\".join([\n",
    "                \"\\n\".join(obs.get(\"raw_notes\", []))\n",
    "                for obs in tool_results\n",
    "            ])\n",
    "            \n",
    "            if raw_notes_concat:\n",
    "                update_payload[\"raw_notes\"] = [raw_notes_concat]\n",
    "        \n",
    "        except Exception as e:\n",
    "            return Command(\n",
    "                goto=END,\n",
    "                update={\n",
    "                    \"notes\": extract_tool_content(supervisor_messages),\n",
    "                    \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    update_payload[\"supervisor_messages\"] = all_tool_messages\n",
    "    return Command(goto=\"supervisor\", update=update_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Edges and Build Supervisor Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build supervisor graph\n",
    "supervisor_builder = StateGraph(SupervisorState)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_edge(\"supervisor\", \"supervisor_tools\")\n",
    "# supervisor_tools uses Command to route back to supervisor or END\n",
    "\n",
    "supervisor_graph = supervisor_builder.compile()\n",
    "show_graph(supervisor_graph, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test the Supervisor\n",
    "\n",
    "Let's test the supervisor with a research question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the supervisor\n",
    "research_brief = \"Recommend some chinese restaurants and indian restaurants in NYC\"\n",
    "\n",
    "supervisor_system_prompt = lead_researcher_prompt.format(\n",
    "    date=get_today_str(),\n",
    "    max_concurrent_research_units=MAX_CONCURRENT_RESEARCH_UNITS,\n",
    "    max_researcher_iterations=MAX_RESEARCHER_ITERATIONS\n",
    ")\n",
    "\n",
    "initial_state = {\n",
    "    \"supervisor_messages\": [\n",
    "        SystemMessage(content=supervisor_system_prompt),\n",
    "        HumanMessage(content=research_brief)\n",
    "    ],\n",
    "    \"research_brief\": research_brief,\n",
    "    \"research_iterations\": 0,\n",
    "    \"notes\": [],\n",
    "    \"raw_notes\": []\n",
    "}\n",
    "result = await supervisor_graph.ainvoke(initial_state)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUPERVISOR MESSAGE HISTORY:\")\n",
    "print(\"=\"*60)\n",
    "for message in result[\"supervisor_messages\"]:\n",
    "    message.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTED RESEARCH NOTES:\")\n",
    "print(\"=\"*60)\n",
    "for i, note in enumerate(result[\"notes\"], 1):\n",
    "    print(f\"\\n--- Research Finding {i} ---\")\n",
    "    print(note[:500] + \"...\" if len(note) > 500 else note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Concepts (HITL, Context Management)\n",
    "\n",
    "This is the full production-ready research agent. We're going to add a workflow around our supervisor to consolidate our flow, allowing for human feedback on the research plan, as well as scaffolding to manage context.\n",
    "\n",
    "![arch](../../images/deep_research.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Main Agent State\n",
    "\n",
    "We can now put together the State for our overall graph to track. We can share State with the supervisor by including the same keys in our overall State. This allows the supervisor to inherit information gleaned from our human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the complete research agent.\"\"\"\n",
    "    messages: Annotated[list[MessageLikeRepresentation], override_reducer]              # User conversation\n",
    "    need_elaboration: bool                                                              # Whether or not clarification needs to be asked\n",
    "    research_brief: str                                                                 # Processed research goal\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]   # Supervisor conversation\n",
    "    research_iterations: int                                                            # Tracking iterations\n",
    "    notes: Annotated[list[str], override_reducer] = []                                  # Collected findings\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []                              # Raw notes\n",
    "    final_report: str                                                                   # Generated report\n",
    "\n",
    "class AgentInputState(TypedDict):\n",
    "    \"\"\"Input to the agent - just user messages.\"\"\"\n",
    "    messages: list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add Human in the Loop\n",
    "\n",
    "This node uses `interrupt()` to inject human feedback on the research topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClarifyWithUser(TypedDict):\n",
    "    \"\"\"Structured output for clarification.\"\"\"\n",
    "    need_clarification: bool\n",
    "    question: str\n",
    "    verification: str\n",
    "\n",
    "clarify_with_user_instructions = \"\"\"These are the messages exchanged so far:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Assess whether you need to ask a clarifying question.\n",
    "\n",
    "If you need to ask a question:\n",
    "- Be concise while gathering necessary information\n",
    "- Don't ask for information already provided\n",
    "\n",
    "Respond in JSON with these keys:\n",
    "{{\"need_clarification\": boolean, \"question\": \"...\", \"verification\": \"...\"}}\n",
    "\n",
    "If clarification needed:\n",
    "{{\"need_clarification\": true, \"question\": \"<your question>\", \"verification\": \"\"}}\n",
    "\n",
    "If no clarification needed:\n",
    "{{\"need_clarification\": false, \"question\": \"\", \"verification\": \"<acknowledgement message>\"}}\n",
    "\"\"\"\n",
    "\n",
    "async def clarify_with_user(state: AgentState, config):\n",
    "    \"\"\"Ask clarifying questions if needed using human-in-the-loop.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Configure model for structured clarification\n",
    "    clarification_model = (\n",
    "        get_model()\n",
    "        .with_structured_output(ClarifyWithUser)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Analyze whether clarification is needed\n",
    "    prompt_content = clarify_with_user_instructions.format(\n",
    "        messages=get_buffer_string(messages),\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # If clarification needed, use interrupt to pause for user input\n",
    "    if response[\"need_clarification\"]:\n",
    "        return {\"messages\": [AIMessage(content=response[\"question\"])], \"need_elaboration\": True}\n",
    "    else:\n",
    "        # No clarification needed\n",
    "        return {\"messages\": [AIMessage(content=response[\"verification\"])], \"need_elaboration\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def human_input(state: AgentState, config):\n",
    "    ai_question = state[\"messages\"][-1].content\n",
    "    user_response = interrupt(ai_question)\n",
    "    return {\"messages\": [HumanMessage(content=user_response)], \"need_elaboration\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create another node to take the research topic and human feedback. \n",
    "\n",
    "This node will be responsible for generating a high level plan. Keeping this plan available to our agent through the State key research_brief will allow our agent to stay on track even with complicated tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuestion(TypedDict):\n",
    "    \"\"\"Structured research question.\"\"\"\n",
    "    research_brief: str\n",
    "\n",
    "create_research_brief_prompt = \"\"\"Translate these messages into a detailed research question:\n",
    "\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Guidelines:\n",
    "1. Maximize specificity and detail - include all user preferences\n",
    "2. Fill in unstated but necessary dimensions as open-ended\n",
    "3. Avoid unwarranted assumptions\n",
    "4. Use first person (from user's perspective)\n",
    "5. For product/travel research, prefer official sources\n",
    "\"\"\"\n",
    "\n",
    "async def write_research_brief(state: AgentState, config) -> Command[Literal[\"research_supervisor\"]]:\n",
    "    \"\"\"Transform user messages into a structured research brief.\"\"\"\n",
    "    # Configure model for structured output\n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .with_structured_output(ResearchQuestion)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Generate research brief\n",
    "    prompt_content = create_research_brief_prompt.format(\n",
    "        messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # Initialize supervisor\n",
    "    supervisor_system_prompt = lead_researcher_prompt.format(\n",
    "        date=get_today_str(),\n",
    "        max_concurrent_research_units=MAX_CONCURRENT_RESEARCH_UNITS,\n",
    "        max_researcher_iterations=MAX_RESEARCHER_ITERATIONS\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"research_brief\": response[\"research_brief\"],\n",
    "        \"supervisor_messages\": {\n",
    "            \"type\": \"override\",\n",
    "            \"value\": [\n",
    "                SystemMessage(content=supervisor_system_prompt),\n",
    "                HumanMessage(content=response[\"research_brief\"])\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Format Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report_generation_prompt = \"\"\"Based on all research conducted, create a comprehensive answer to:\n",
    "<Research Brief>\n",
    "{research_brief}\n",
    "</Research Brief>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Here are the findings:\n",
    "<Findings>\n",
    "{findings}\n",
    "</Findings>\n",
    "\n",
    "Create a detailed answer that:\n",
    "1. Is well-organized with proper headings (# for title, ## for sections)\n",
    "2. Includes specific facts from the research\n",
    "3. References sources using [Title](URL) format\n",
    "4. Provides comprehensive analysis\n",
    "5. Includes a \"Sources\" section at the end\n",
    "\n",
    "Structure your report appropriately:\n",
    "- For comparisons: intro → overview A → overview B → comparison → conclusion\n",
    "- For lists: just the list with details\n",
    "- For summaries: overview → key concepts → conclusion\n",
    "\n",
    "Use ## for section titles. Be thorough - users expect deep research.\n",
    "\"\"\"\n",
    "\n",
    "async def final_report_generation(state: AgentState, config):\n",
    "    \"\"\"Generate the final comprehensive research report.\"\"\"\n",
    "    notes = state.get(\"notes\", [])\n",
    "    findings = \"\\n\".join(notes)\n",
    "    \n",
    "    # Create report prompt\n",
    "    final_report_prompt = final_report_generation_prompt.format(\n",
    "        research_brief=state.get(\"research_brief\", \"\"),\n",
    "        messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "        findings=findings,\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    \n",
    "    # Generate report\n",
    "    final_report = await get_model().ainvoke([HumanMessage(content=final_report_prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": final_report.content,\n",
    "        \"messages\": [final_report],\n",
    "        \"notes\": {\"type\": \"override\", \"value\": []}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Define Edges & Compile with Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put together our entire workflow using edges! Note that the research supervisor has been added as a node in this workflow, demonstrating how to create a subgraph as a single node.\n",
    "\n",
    "We also compile our agent with a checkpointer, which enables LangGraph's built in persistence. This enables our agent to remember previous conversations and roll back execution if a node fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_needed(state: AgentState):\n",
    "    need_elaboration = state.get(\"need_elaboration\", False)\n",
    "    if need_elaboration:\n",
    "        return \"human_input\"\n",
    "    else:\n",
    "        return \"write_research_brief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpointer for persistence\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Build complete research workflow\n",
    "deep_researcher_builder = StateGraph(\n",
    "    AgentState,\n",
    "    input_schema=AgentInputState\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "deep_researcher_builder.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "deep_researcher_builder.add_node(\"human_input\", human_input)\n",
    "deep_researcher_builder.add_node(\"write_research_brief\", write_research_brief)\n",
    "deep_researcher_builder.add_node(\"research_supervisor\", supervisor_graph)\n",
    "deep_researcher_builder.add_node(\"final_report_generation\", final_report_generation)\n",
    "\n",
    "# Add edges\n",
    "deep_researcher_builder.add_edge(START, \"clarify_with_user\")\n",
    "deep_researcher_builder.add_conditional_edges(\n",
    "    \"clarify_with_user\",\n",
    "    human_feedback_needed,\n",
    "    {\n",
    "        \"human_input\": \"human_input\",\n",
    "        \"write_research_brief\": \"write_research_brief\",\n",
    "    },\n",
    ")\n",
    "deep_researcher_builder.add_edge(\"human_input\", \"clarify_with_user\")\n",
    "deep_researcher_builder.add_edge(\"write_research_brief\", \"research_supervisor\")\n",
    "deep_researcher_builder.add_edge(\"research_supervisor\", \"final_report_generation\")\n",
    "deep_researcher_builder.add_edge(\"final_report_generation\", END)\n",
    "\n",
    "# Compile with checkpointer for interrupt/resume support\n",
    "deep_research_graph = deep_researcher_builder.compile(checkpointer=checkpointer)\n",
    "show_graph(deep_research_graph, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Test HITL and Persistence\n",
    "\n",
    "Let's test the full research workflow! We'll start by giving the agent a vague query that should trigger our agent to ask for clarifying information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete workflow\n",
    "import uuid\n",
    "\n",
    "test_query = \"Recommend best Chinese restaurants in Manhattan\"\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=test_query)]\n",
    "}\n",
    "\n",
    "# Create config with thread_id for checkpointing\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# First invocation - will pause at interrupt if clarification needed\n",
    "print(\"Starting research workflow...\")\n",
    "result = await deep_research_graph.ainvoke(initial_state, config=config)\n",
    "\n",
    "# Check if we hit an interrupt\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERRUPT: Clarification question detected\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the state to see the interrupt details\n",
    "    state_snapshot = deep_research_graph.get_state(config)\n",
    "    \n",
    "    # Show the interrupt value (the clarification question)\n",
    "    if hasattr(state_snapshot, 'tasks') and state_snapshot.tasks:\n",
    "        for task in state_snapshot.tasks:\n",
    "            if hasattr(task, 'interrupts') and task.interrupts:\n",
    "                for interrupt_info in task.interrupts:\n",
    "                    print(f\"Question: {interrupt_info.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our agent is paused mid-execution waiting for human feedback! To resume from our paused execution, we can use the same Command object we've used in our graph implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from interrupt with our response\n",
    "result = await deep_research_graph.ainvoke(Command(resume=\"No preferences. Use your best judgement, and don't ask any further questions.\"), config=config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESEARCH REPORT:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
