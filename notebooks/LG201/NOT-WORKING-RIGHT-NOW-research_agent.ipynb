{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph 201: Building a Deep Research Agent\n",
    "\n",
    "In this notebook, we'll build a multi-agent research system that can conduct comprehensive web research. We'll progressively build up the system in three parts:\n",
    "\n",
    "1. **Part 1**: Single researcher agent that performs web searches\n",
    "2. **Part 2**: Supervisor agent that coordinates multiple researchers\n",
    "3. **Part 3**: Complete workflow with user clarification and report generation\n",
    "\n",
    "![arch](../../images/deep_research.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-work: Setup\n",
    "\n",
    "First, let's install dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "project_root = Path().resolve().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import operator\n",
    "from datetime import datetime\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from aipe.llm import init_payx_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    filter_messages,\n",
    "    get_buffer_string,\n",
    "    MessageLikeRepresentation\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command, interrupt\n",
    "from utils.utils import show_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date formatted for display.\"\"\"\n",
    "    now = datetime.now()\n",
    "    return f\"{now:%a} {now:%b} {now.day}, {now:%Y}\"\n",
    "\n",
    "def openai_websearch_called(response):\n",
    "    \"\"\"Detect if OpenAI's web search was used.\"\"\"\n",
    "    try:\n",
    "        tool_outputs = response.additional_kwargs.get(\"tool_outputs\")\n",
    "        if not tool_outputs:\n",
    "            return False\n",
    "        for tool_output in tool_outputs:\n",
    "            if tool_output.get(\"type\") == \"web_search_call\":\n",
    "                return True\n",
    "        return False\n",
    "    except (AttributeError, TypeError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "Let's define our hardcoded configuration. In a production system, these would be configurable, but for this educational example we'll keep them simple and visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration set: gpt-41-mini\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "RESEARCH_MODEL = \"gpt-41-mini\" # Hard requirement for this notebook\n",
    "MAX_OUTPUT_TOKENS = 10000\n",
    "\n",
    "# Research limits\n",
    "MAX_RESEARCHER_ITERATIONS = 3  # How many times supervisor can delegate\n",
    "MAX_REACT_TOOL_CALLS = 10      # Max tool calls per researcher\n",
    "MAX_CONCURRENT_RESEARCH_UNITS = 5  # Max parallel researchers\n",
    "MAX_STRUCTURED_OUTPUT_RETRIES = 3\n",
    "\n",
    "# Initialize model\n",
    "def get_model():\n",
    "    return init_payx_chat_model(\n",
    "        model=RESEARCH_MODEL,\n",
    "        model_provider=\"azure_openai\",\n",
    "        max_tokens=MAX_OUTPUT_TOKENS\n",
    "    )\n",
    "\n",
    "print(f\"✓ Configuration set: {RESEARCH_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Single Researcher Agent\n",
    "\n",
    "We'll start by building a single researcher agent that can:\n",
    "1. Receive a research topic\n",
    "2. Use web search to gather information (using OpenAI native web search)\n",
    "3. Compress the findings into a summary\n",
    "\n",
    "![arch](../../images/researcher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define State\n",
    "\n",
    "How does information flow through the steps?  \n",
    "\n",
    "State is the first LangGraph concept we'll cover. **State can be thought of as the memory of the agent - its a shared data structure that’s passed on between the nodes of your graph**, representing the current snapshot of your application. \n",
    "\n",
    "For our Researcher, we'll define 3 fields to track: the conversation history, the topic, and how many API calls it has made. \n",
    "\n",
    "We can also define specific output State to indicate what we expect the researcher to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearcherState(TypedDict):\n",
    "    \"\"\"State for a single researcher agent.\"\"\"\n",
    "    researcher_messages: Annotated[list[MessageLikeRepresentation], operator.add]\n",
    "    research_topic: str        # What to research\n",
    "    tool_call_iterations: int  # How many tool calls made\n",
    "\n",
    "class ResearcherOutputState(TypedDict):\n",
    "    \"\"\"Output from researcher - just the compressed findings.\"\"\"\n",
    "    researcher_messages: Annotated[list[MessageLikeRepresentation], operator.add]\n",
    "    compressed_research: str   # Summary of findings\n",
    "    raw_notes: list           # Raw notes for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Tools\n",
    "\n",
    "Let's define a list of **tools** our agent will have access to. Tools are functionts that can act as extension of the LLM's capabilities. \n",
    "\n",
    "We'll define some no-op tools that will allow our researcher to reflect and indicate when research is complete.\n",
    "\n",
    "Finally, to actually conduct research, our agent needs access to web search. We'll use OpenAI's native web search capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"Signal that research is complete\")\n",
    "def ResearchComplete() -> str:\n",
    "    \"\"\"Call this when you have gathered enough information to answer the research question.\"\"\"\n",
    "    return \"Research marked as complete\"\n",
    "\n",
    "@tool(description=\"Strategic reflection tool for research planning\")\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Use this tool after each search to analyze results and plan next steps.\n",
    "    \n",
    "    Args:\n",
    "        reflection: Detailed reflection on research progress and next steps\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\"\n",
    "\n",
    "def get_all_tools():\n",
    "    \"\"\"Get all available research tools.\"\"\"\n",
    "    tools = [ResearchComplete, think_tool]\n",
    "    # OpenAI's native web search - bind it so model knows it's available, but we don't execute it\n",
    "    tools.append({\"type\": \"web_search_preview\"})\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define Nodes\n",
    "\n",
    "Now that we have a list of tools, we are ready to build nodes that interact with them. \n",
    "\n",
    "Nodes are just python (or JS/TS!) functions. Nodes take in your graph's State as input, execute some logic, and return a new State. \n",
    "\n",
    "Here, we're just going to set up 2 nodes for our ReAct agent:\n",
    "1. **researcher**: Reasoning node that decides which function to invoke \n",
    "2. **researcher_tools**: Node that contains all the available tools and executes the function\n",
    "\n",
    "We'll start with the reasoning node, which will utilize a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_system_prompt = \"\"\"You are a research assistant conducting research on the user's input topic. For context, today's date is {date}.\n",
    "\n",
    "<Task>\n",
    "Your job is to use tools to gather information about the user's input topic.\n",
    "You can use any of the tools provided to you to find resources that can help answer the research question.\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "You have access to:\n",
    "1. **Web search**: For conducting web searches to gather information\n",
    "2. **think_tool**: For reflection and strategic planning during research\n",
    "\n",
    "**CRITICAL: Use think_tool after each search to reflect on results and plan next steps.**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a human researcher with limited time:\n",
    "\n",
    "1. **Read the question carefully** - What specific information does the user need?\n",
    "2. **Start with broader searches** - Use broad, comprehensive queries first\n",
    "3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "4. **Execute narrower searches as you gather information** - Fill in the gaps\n",
    "5. **Stop when you can answer confidently** - Don't keep searching for perfection\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "**Tool Call Budgets**:\n",
    "- **Simple queries**: Use 2-3 search tool calls maximum\n",
    "- **Complex queries**: Use up to 4 search tool calls maximum\n",
    "- **Always stop**: After 4 search tool calls if you cannot find the right sources\n",
    "\n",
    "**Stop Immediately When**:\n",
    "- You can answer the user's question comprehensively\n",
    "- You have 3+ relevant examples/sources for the question\n",
    "- Your last 2 searches returned similar information\n",
    "</Hard Limits>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this prompt in our reasoning node. We'll give our tools to our configured LLM model, and invoke it with our prompt to see what tools it wants to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def researcher(state: ResearcherState, config):\n",
    "    \"\"\"Main researcher node that conducts research.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Get research tools\n",
    "    tools = get_all_tools()    \n",
    "    # Prepare system prompt\n",
    "    researcher_prompt = research_system_prompt.format(date=get_today_str())\n",
    "    \n",
    "    # Configure model with tools\n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .bind_tools(tools)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Generate researcher response\n",
    "    messages = [SystemMessage(content=researcher_prompt)] + researcher_messages\n",
    "    response = await research_model.ainvoke(messages)\n",
    "    \n",
    "    # Update state\n",
    "    return {\n",
    "        \"researcher_messages\": [response],\n",
    "        \"tool_call_iterations\": state.get(\"tool_call_iterations\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the tools node to actually execute the tool calls that the LLM wants to make. We'll also introduce the concept of a Command, which is a special object that allows you to not only update the State, but also determine which node to execute next.\n",
    "\n",
    "Commands serve as an alternative to edges, which we'll cover next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_tool_safely(tool, args):\n",
    "    \"\"\"Safely execute a tool with error handling.\"\"\"\n",
    "    try:\n",
    "        return await tool.ainvoke(args)\n",
    "    except Exception as e:\n",
    "        return f\"Error executing tool: {str(e)}\"\n",
    "\n",
    "async def researcher_tools(state: ResearcherState, config) -> Command[Literal[\"researcher\", \"compress_research\"]]:\n",
    "    \"\"\"Execute tools called by the researcher.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    most_recent_message = researcher_messages[-1]\n",
    "    \n",
    "    # Check if there are tool calls or native search\n",
    "    has_tool_calls = bool(most_recent_message.tool_calls)\n",
    "    has_native_search = openai_websearch_called(most_recent_message)\n",
    "    \n",
    "    if not has_tool_calls and not has_native_search:\n",
    "        return Command(goto=\"compress_research\")\n",
    "    \n",
    "    # Execute tool calls\n",
    "    tools = get_all_tools()\n",
    "    tools_by_name = {\n",
    "        tool.name if hasattr(tool, \"name\") else tool.get(\"name\", \"web_search\"): tool\n",
    "        for tool in tools\n",
    "    }\n",
    "    \n",
    "    tool_calls = most_recent_message.tool_calls\n",
    "    tool_execution_tasks = [\n",
    "        execute_tool_safely(tools_by_name[tc[\"name\"]], tc[\"args\"])\n",
    "        for tc in tool_calls\n",
    "    ]\n",
    "    observations = await asyncio.gather(*tool_execution_tasks)\n",
    "    \n",
    "    # Create tool messages\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tc[\"name\"],\n",
    "            tool_call_id=tc[\"id\"]\n",
    "        )\n",
    "        for observation, tc in zip(observations, tool_calls)\n",
    "    ]\n",
    "    \n",
    "    # Check exit conditions\n",
    "    exceeded_iterations = state.get(\"tool_call_iterations\", 0) >= MAX_REACT_TOOL_CALLS\n",
    "    research_complete = any(\n",
    "        tc[\"name\"] == \"ResearchComplete\"\n",
    "        for tc in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or research_complete:\n",
    "        return Command(\n",
    "            goto=\"compress_research\",\n",
    "            update={\"researcher_messages\": tool_outputs}\n",
    "        )\n",
    "    \n",
    "    # Continue research loop\n",
    "    return Command(\n",
    "        goto=\"researcher\",\n",
    "        update={\"researcher_messages\": tool_outputs}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_research_system_prompt = \"\"\"You are a research assistant that has conducted research on a topic. Your job is to clean up the findings.\n",
    "\n",
    "<Task>\n",
    "Clean up information gathered from tool calls and web searches. All relevant information should be repeated verbatim.\n",
    "The purpose is just to remove obviously irrelevant or duplicative information.\n",
    "</Task>\n",
    "\n",
    "<Guidelines>\n",
    "1. Your output should be fully comprehensive and include ALL information and sources gathered\n",
    "2. Include inline citations [1], [2], etc. for each source\n",
    "3. Include a \"Sources\" section at the end listing all sources with citations\n",
    "4. Make sure to include ALL sources - a later LLM will merge this with other reports\n",
    "</Guidelines>\n",
    "\"\"\"\n",
    "\n",
    "async def compress_research(state: ResearcherState, config):\n",
    "    \"\"\"Compress and synthesize research findings.\"\"\"\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Add compression instruction\n",
    "    researcher_messages.append(\n",
    "        HumanMessage(content=\"Please clean up these findings. DO NOT summarize - preserve all relevant information verbatim.\")\n",
    "    )\n",
    "    \n",
    "    # Create compression prompt\n",
    "    compression_prompt = compress_research_system_prompt\n",
    "    messages = [SystemMessage(content=compression_prompt)] + researcher_messages\n",
    "    \n",
    "    # Execute compression\n",
    "    response = await get_model().ainvoke(messages)\n",
    "    \n",
    "    # Extract raw notes\n",
    "    raw_notes_content = \"\\n\".join([\n",
    "        str(message.content)\n",
    "        for message in filter_messages(researcher_messages, include_types=[\"tool\", \"ai\"])\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [raw_notes_content]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define Edges and Build Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edges are connections between nodes. They define the flow of the graph.**\n",
    "* **Normal edges** are deterministic and always go from one node to its defined target\n",
    "* **Conditional edges** are used to dynamically route between nodes, implemented as functions that return the next node to visit based upon some logic. \n",
    "\n",
    "In this case, our conditional routing is handled by Commands, so we can link together the nodes of our graph using normal edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f32c146ee90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build researcher graph\n",
    "researcher_builder = StateGraph(\n",
    "    ResearcherState,\n",
    "    output_schema=ResearcherOutputState\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "researcher_builder.add_node(\"researcher\", researcher)\n",
    "researcher_builder.add_node(\"researcher_tools\", researcher_tools)\n",
    "researcher_builder.add_node(\"compress_research\", compress_research)\n",
    "\n",
    "# Add edges\n",
    "researcher_builder.add_edge(START, \"researcher\")\n",
    "researcher_builder.add_edge(\"researcher\", \"researcher_tools\")\n",
    "# researcher_tools uses Command to route to either researcher or compress_research\n",
    "researcher_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "# Compile\n",
    "researcher_graph = researcher_builder.compile()\n",
    "researcher_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Running the Single Researcher\n",
    "\n",
    "Let's test our researcher with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"request body doesn't contain valid inputs\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the best practices for prompt engineering with LLMs?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m initial_state = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresearcher_messages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=test_query)],\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresearch_topic\u001b[39m\u001b[33m\"\u001b[39m: test_query,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtool_call_iterations\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m researcher_graph.ainvoke(initial_state)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRESEARCHER MESSAGE HISTORY:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mresearcher\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Generate researcher response\u001b[39;00m\n\u001b[32m     18\u001b[39m messages = [SystemMessage(content=researcher_prompt)] + researcher_messages\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m research_model.ainvoke(messages)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresearcher_messages\u001b[39m\u001b[33m\"\u001b[39m: [response],\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtool_call_iterations\u001b[39m\u001b[33m\"\u001b[39m: state.get(\u001b[33m\"\u001b[39m\u001b[33mtool_call_iterations\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) + \u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5570\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5563\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5564\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5569\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5571\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5573\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5574\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:225\u001b[39m, in \u001b[36mRunnableRetry.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Input, config: RunnableConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m    224\u001b[39m ) -> Output:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall_with_config(\u001b[38;5;28mself\u001b[39m._ainvoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:2109\u001b[39m, in \u001b[36mRunnable._acall_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2105\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2106\u001b[39m         coro = acall_func_with_variable_args(\n\u001b[32m   2107\u001b[39m             func, input_, config, run_manager, **kwargs\n\u001b[32m   2108\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2109\u001b[39m         output: Output = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(coro, context)\n\u001b[32m   2110\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2111\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:210\u001b[39m, in \u001b[36mRunnableRetry._ainvoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ainvoke\u001b[39m(\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    205\u001b[39m     input_: Input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     **kwargs: Any,\n\u001b[32m    209\u001b[39m ) -> Output:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retrying(reraise=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m    212\u001b[39m             result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\n\u001b[32m    213\u001b[39m                 input_,\n\u001b[32m    214\u001b[39m                 \u001b[38;5;28mself\u001b[39m._patch_config(config, run_manager, attempt.retry_state),\n\u001b[32m    215\u001b[39m                 **kwargs,\n\u001b[32m    216\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:166\u001b[39m, in \u001b[36mAsyncRetrying.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AttemptManager:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=\u001b[38;5;28mself\u001b[39m._retry_state)\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m do \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/retry.py:212\u001b[39m, in \u001b[36mRunnableRetry._ainvoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retrying(reraise=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\n\u001b[32m    213\u001b[39m             input_,\n\u001b[32m    214\u001b[39m             \u001b[38;5;28mself\u001b[39m._patch_config(config, run_manager, attempt.retry_state),\n\u001b[32m    215\u001b[39m             **kwargs,\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt.retry_state.outcome \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attempt.retry_state.outcome.failed:\n\u001b[32m    218\u001b[39m         attempt.retry_state.set_result(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5570\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5563\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5564\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5569\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5571\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5573\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5574\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:425\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Any,\n\u001b[32m    423\u001b[39m ) -> AIMessage:\n\u001b[32m    424\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    426\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    427\u001b[39m         stop=stop,\n\u001b[32m    428\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    429\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    430\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    431\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    432\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    433\u001b[39m         **kwargs,\n\u001b[32m    434\u001b[39m     )\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1132\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     **kwargs: Any,\n\u001b[32m   1130\u001b[39m ) -> LLMResult:\n\u001b[32m   1131\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1133\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1090\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1079\u001b[39m             *[\n\u001b[32m   1080\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m             ]\n\u001b[32m   1089\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m flattened_outputs = [\n\u001b[32m   1092\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1094\u001b[39m ]\n\u001b[32m   1095\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1343\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1344\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1580\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1579\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1583\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1584\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1585\u001b[39m ):\n\u001b[32m   1586\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1559\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1552\u001b[39m     raw_response = (\n\u001b[32m   1553\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root_async_client.responses.with_raw_response.parse(\n\u001b[32m   1554\u001b[39m             **payload\n\u001b[32m   1555\u001b[39m         )\n\u001b[32m   1556\u001b[39m     )\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root_async_client.responses.with_raw_response.create(\n\u001b[32m   1560\u001b[39m             **payload\n\u001b[32m   1561\u001b[39m         )\n\u001b[32m   1562\u001b[39m     )\n\u001b[32m   1563\u001b[39m response = raw_response.parse()\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_response_headers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/openai/resources/responses/responses.py:2320\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2282\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2283\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2284\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2318\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2319\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2322\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2323\u001b[39m             {\n\u001b[32m   2324\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2325\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2326\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2327\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2328\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2329\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2330\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2332\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2333\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2334\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2335\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2336\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2337\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2338\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2339\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2340\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2341\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2342\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2343\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2344\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2345\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2346\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2347\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2348\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2349\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2350\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2351\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2352\u001b[39m             },\n\u001b[32m   2353\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2354\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2355\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2356\u001b[39m         ),\n\u001b[32m   2357\u001b[39m         options=make_request_options(\n\u001b[32m   2358\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2359\u001b[39m         ),\n\u001b[32m   2360\u001b[39m         cast_to=Response,\n\u001b[32m   2361\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2362\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2363\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/payx-langgraph-101/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"request body doesn't contain valid inputs\"}}",
      "During task with name 'researcher' and id '89588a62-67a6-7c7a-250b-41c5fbf356f7'"
     ]
    }
   ],
   "source": [
    "# Test the researcher\n",
    "test_query = \"What are the best practices for prompt engineering with LLMs?\"\n",
    "\n",
    "initial_state = {\n",
    "    \"researcher_messages\": [HumanMessage(content=test_query)],\n",
    "    \"research_topic\": test_query,\n",
    "    \"tool_call_iterations\": 0\n",
    "}\n",
    "\n",
    "result = await researcher_graph.ainvoke(initial_state)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESEARCHER MESSAGE HISTORY:\")\n",
    "print(\"=\"*60)\n",
    "for message in result[\"researcher_messages\"]:\n",
    "    message.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESEARCH FINDINGS:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"compressed_research\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Supervisor Agent\n",
    "\n",
    "Now we'll build a supervisor agent that can delegate research to multiple researcher agents in parallel. This will build on the research agent we've created from Part 1.\n",
    "\n",
    "![arch](../../images/research_supervisor.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Supervisor State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define separate state for our Supervisor. This allows it to keep track different items than the Researcher, preventing it from getting distracted by noise. \n",
    "\n",
    "We also define a customer reducer for handling specific state keys. By default state updates overwrite existing values - we've defined a custom reducer to allow for concatenation alongside overwrite behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def override_reducer(current_value, new_value):\n",
    "    \"\"\"Reducer function that allows overriding values in state.\"\"\"\n",
    "    if isinstance(new_value, dict) and new_value.get(\"type\") == \"override\":\n",
    "        return new_value.get(\"value\", new_value)\n",
    "    else:\n",
    "        return operator.add(current_value, new_value)\n",
    "        \n",
    "class SupervisorState(TypedDict):\n",
    "    \"\"\"State for the supervisor agent.\"\"\"\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]\n",
    "    research_brief: str\n",
    "    notes: Annotated[list[str], override_reducer] = []\n",
    "    research_iterations: int = 0\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Supervisor Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-agent systems, the most common way to coordinate subagents today is to call them as tools. This allows subagents to isolate their context from the supervisor's orchestration, allowing specialization and focus.\n",
    "\n",
    "This is as simple as invoking our researcher subagent within a tool, and giving that tool to our supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(description=\"Delegate a research task to a specialized researcher\")\n",
    "async def ConductResearch(research_topic: str) -> dict:\n",
    "    \"\"\"Delegate a specific research topic to a researcher agent.\n",
    "    \n",
    "    Args:\n",
    "        research_topic: Clear, specific research question for the sub-agent\n",
    "    \"\"\"\n",
    "    # Actually invoke the researcher graph\n",
    "    result = await researcher_graph.ainvoke({\n",
    "        \"researcher_messages\": [HumanMessage(content=research_topic)],\n",
    "        \"research_topic\": research_topic,\n",
    "        \"tool_call_iterations\": 0\n",
    "    })\n",
    "    \n",
    "    # Return dictionary directly with both compressed research and raw notes\n",
    "    return {\n",
    "        \"compressed_research\": result.get(\"compressed_research\", \"Error in research\"),\n",
    "        \"raw_notes\": result.get(\"raw_notes\", [])\n",
    "    }\n",
    "\n",
    "@tool(description=\"Signal that all research is complete\")\n",
    "def ResearchComplete() -> str:\n",
    "    \"\"\"Call this when you have gathered all necessary information.\"\"\"\n",
    "    return \"Research marked as complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Supervisor Nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining the prompt for our reasoning node. Like the researcher agent, the reasoning node will rely on calling an LLM with our custom prompt to determine what tools to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_researcher_prompt = \"\"\"You are a research supervisor. Your job is to conduct research by calling the \"ConductResearch\" tool.\n",
    "\n",
    "<Task>\n",
    "Call the \"ConductResearch\" tool to delegate research. When you're satisfied with findings, call \"ResearchComplete\".\n",
    "</Task>\n",
    "\n",
    "<Available Tools>\n",
    "1. **ConductResearch**: Delegate research tasks to specialized sub-agents\n",
    "2. **ResearchComplete**: Indicate that research is complete\n",
    "3. **think_tool**: For reflection and strategic planning\n",
    "\n",
    "**CRITICAL: Use think_tool before calling ConductResearch to plan, and after to assess progress.**\n",
    "</Available Tools>\n",
    "\n",
    "<Instructions>\n",
    "Think like a research manager:\n",
    "\n",
    "1. **Read the question carefully** - What specific information is needed?\n",
    "2. **Decide how to delegate** - Can multiple independent angles be explored simultaneously?\n",
    "3. **After each ConductResearch call, assess** - Do I have enough? What's missing?\n",
    "</Instructions>\n",
    "\n",
    "<Hard Limits>\n",
    "- **Limit tool calls** - Stop after {max_researcher_iterations} tool calls if you cannot find the right sources\n",
    "- **Maximum {max_concurrent_research_units} parallel agents per iteration**\n",
    "</Hard Limits>\n",
    "\n",
    "<Scaling Rules>\n",
    "**Simple queries** - Use a single sub-agent\n",
    "**Comparisons** - Use a sub-agent for each element being compared\n",
    "**Important**: Provide complete standalone instructions when calling ConductResearch\n",
    "</Scaling Rules>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def supervisor(state: SupervisorState, config):\n",
    "    \"\"\"Supervisor agent that delegates research.\"\"\"\n",
    "    lead_researcher_tools = [ConductResearch, ResearchComplete, think_tool]\n",
    "    \n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .bind_tools(lead_researcher_tools)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    response = await research_model.ainvoke(supervisor_messages)\n",
    "    \n",
    "    return {\n",
    "        \"supervisor_messages\": [response],\n",
    "        \"research_iterations\": state.get(\"research_iterations\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll customize our tool node implementation to automatically fail tool calls that go above our predefined usage limit. As usual, this node will be responsible for executing the tools the agent wants to call, and updating the state with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_content(messages):\n",
    "    \"\"\"Extract notes from tool call messages.\"\"\"\n",
    "    return [tool_msg.content for tool_msg in filter_messages(messages, include_types=\"tool\")]\n",
    "\n",
    "async def supervisor_tools(state: SupervisorState, config) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Execute tools called by the supervisor.\"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "    \n",
    "    # Check exit conditions\n",
    "    exceeded_iterations = research_iterations > MAX_RESEARCHER_ITERATIONS\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete = any(\n",
    "        tc[\"name\"] == \"ResearchComplete\"\n",
    "        for tc in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or no_tool_calls or research_complete:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"notes\": extract_tool_content(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Process tool calls\n",
    "    all_tool_messages = []\n",
    "    update_payload = {\"supervisor_messages\": []}\n",
    "    \n",
    "    # Handle think_tool\n",
    "    for tc in most_recent_message.tool_calls:\n",
    "        if tc[\"name\"] == \"think_tool\":\n",
    "            all_tool_messages.append(ToolMessage(\n",
    "                content=f\"Reflection recorded: {tc['args']['reflection']}\",\n",
    "                name=\"think_tool\",\n",
    "                tool_call_id=tc[\"id\"]\n",
    "            ))\n",
    "    \n",
    "    # Handle ConductResearch - now executes the actual tool\n",
    "    conduct_research_calls = [\n",
    "        tc for tc in most_recent_message.tool_calls\n",
    "        if tc[\"name\"] == \"ConductResearch\"\n",
    "    ]\n",
    "    \n",
    "    if conduct_research_calls:\n",
    "        try:\n",
    "            allowed_calls = conduct_research_calls[:MAX_CONCURRENT_RESEARCH_UNITS]\n",
    "            overflow_calls = conduct_research_calls[MAX_CONCURRENT_RESEARCH_UNITS:]\n",
    "            \n",
    "            # Execute research tasks in parallel by invoking the tool\n",
    "            research_tasks = [\n",
    "                ConductResearch.ainvoke(tc[\"args\"])\n",
    "                for tc in allowed_calls\n",
    "            ]\n",
    "            \n",
    "            # Get results directly as dictionaries\n",
    "            tool_results = await asyncio.gather(*research_tasks)\n",
    "            \n",
    "            # Create tool messages with results\n",
    "            for observation_dict, tc in zip(tool_results, allowed_calls):\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=observation_dict.get(\"compressed_research\", \"Error in research\"),\n",
    "                    name=tc[\"name\"],\n",
    "                    tool_call_id=tc[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Handle overflow\n",
    "            for overflow_call in overflow_calls:\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=f\"Error: Exceeded max concurrent units ({MAX_CONCURRENT_RESEARCH_UNITS})\",\n",
    "                    name=\"ConductResearch\",\n",
    "                    tool_call_id=overflow_call[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Aggregate raw notes\n",
    "            raw_notes_concat = \"\\n\".join([\n",
    "                \"\\n\".join(obs.get(\"raw_notes\", []))\n",
    "                for obs in tool_results\n",
    "            ])\n",
    "            \n",
    "            if raw_notes_concat:\n",
    "                update_payload[\"raw_notes\"] = [raw_notes_concat]\n",
    "        \n",
    "        except Exception as e:\n",
    "            return Command(\n",
    "                goto=END,\n",
    "                update={\n",
    "                    \"notes\": extract_tool_content(supervisor_messages),\n",
    "                    \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    update_payload[\"supervisor_messages\"] = all_tool_messages\n",
    "    return Command(goto=\"supervisor\", update=update_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Edges and Build Supervisor Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build supervisor graph\n",
    "supervisor_builder = StateGraph(SupervisorState)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_edge(\"supervisor\", \"supervisor_tools\")\n",
    "# supervisor_tools uses Command to route back to supervisor or END\n",
    "\n",
    "supervisor_graph = supervisor_builder.compile()\n",
    "show_graph(supervisor_graph, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test the Supervisor\n",
    "\n",
    "Let's test the supervisor with a research question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the supervisor\n",
    "research_brief = \"Recommend some chinese restaurants and indian restaurants in NYC\"\n",
    "\n",
    "supervisor_system_prompt = lead_researcher_prompt.format(\n",
    "    date=get_today_str(),\n",
    "    max_concurrent_research_units=MAX_CONCURRENT_RESEARCH_UNITS,\n",
    "    max_researcher_iterations=MAX_RESEARCHER_ITERATIONS\n",
    ")\n",
    "\n",
    "initial_state = {\n",
    "    \"supervisor_messages\": [\n",
    "        SystemMessage(content=supervisor_system_prompt),\n",
    "        HumanMessage(content=research_brief)\n",
    "    ],\n",
    "    \"research_brief\": research_brief,\n",
    "    \"research_iterations\": 0,\n",
    "    \"notes\": [],\n",
    "    \"raw_notes\": []\n",
    "}\n",
    "result = await supervisor_graph.ainvoke(initial_state)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUPERVISOR MESSAGE HISTORY:\")\n",
    "print(\"=\"*60)\n",
    "for message in result[\"supervisor_messages\"]:\n",
    "    message.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTED RESEARCH NOTES:\")\n",
    "print(\"=\"*60)\n",
    "for i, note in enumerate(result[\"notes\"], 1):\n",
    "    print(f\"\\n--- Research Finding {i} ---\")\n",
    "    print(note[:500] + \"...\" if len(note) > 500 else note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Concepts (HITL, Context Management)\n",
    "\n",
    "This is the full production-ready research agent. We're going to add a workflow around our supervisor to consolidate our flow, allowing for human feedback on the research plan, as well as scaffolding to manage context.\n",
    "\n",
    "![arch](../../images/deep_research.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Main Agent State\n",
    "\n",
    "We can now put together the State for our overall graph to track. We can share State with the supervisor by including the same keys in our overall State. This allows the supervisor to inherit information gleaned from our human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the complete research agent.\"\"\"\n",
    "    messages: Annotated[list[MessageLikeRepresentation], override_reducer]              # User conversation\n",
    "    need_elaboration: bool                                                              # Whether or not clarification needs to be asked\n",
    "    research_brief: str                                                                 # Processed research goal\n",
    "    supervisor_messages: Annotated[list[MessageLikeRepresentation], override_reducer]   # Supervisor conversation\n",
    "    research_iterations: int                                                            # Tracking iterations\n",
    "    notes: Annotated[list[str], override_reducer] = []                                  # Collected findings\n",
    "    raw_notes: Annotated[list[str], override_reducer] = []                              # Raw notes\n",
    "    final_report: str                                                                   # Generated report\n",
    "\n",
    "class AgentInputState(TypedDict):\n",
    "    \"\"\"Input to the agent - just user messages.\"\"\"\n",
    "    messages: list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Add Human in the Loop\n",
    "\n",
    "This node uses `interrupt()` to inject human feedback on the research topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClarifyWithUser(TypedDict):\n",
    "    \"\"\"Structured output for clarification.\"\"\"\n",
    "    need_clarification: bool\n",
    "    question: str\n",
    "    verification: str\n",
    "\n",
    "clarify_with_user_instructions = \"\"\"These are the messages exchanged so far:\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Assess whether you need to ask a clarifying question.\n",
    "\n",
    "If you need to ask a question:\n",
    "- Be concise while gathering necessary information\n",
    "- Don't ask for information already provided\n",
    "\n",
    "Respond in JSON with these keys:\n",
    "{{\"need_clarification\": boolean, \"question\": \"...\", \"verification\": \"...\"}}\n",
    "\n",
    "If clarification needed:\n",
    "{{\"need_clarification\": true, \"question\": \"<your question>\", \"verification\": \"\"}}\n",
    "\n",
    "If no clarification needed:\n",
    "{{\"need_clarification\": false, \"question\": \"\", \"verification\": \"<acknowledgement message>\"}}\n",
    "\"\"\"\n",
    "\n",
    "async def clarify_with_user(state: AgentState, config):\n",
    "    \"\"\"Ask clarifying questions if needed using human-in-the-loop.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Configure model for structured clarification\n",
    "    clarification_model = (\n",
    "        get_model()\n",
    "        .with_structured_output(ClarifyWithUser)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Analyze whether clarification is needed\n",
    "    prompt_content = clarify_with_user_instructions.format(\n",
    "        messages=get_buffer_string(messages),\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # If clarification needed, use interrupt to pause for user input\n",
    "    if response[\"need_clarification\"]:\n",
    "        return {\"messages\": [AIMessage(content=response[\"question\"])], \"need_elaboration\": True}\n",
    "    else:\n",
    "        # No clarification needed\n",
    "        return {\"messages\": [AIMessage(content=response[\"verification\"])], \"need_elaboration\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def human_input(state: AgentState, config):\n",
    "    ai_question = state[\"messages\"][-1].content\n",
    "    user_response = interrupt(ai_question)\n",
    "    return {\"messages\": [HumanMessage(content=user_response)], \"need_elaboration\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create another node to take the research topic and human feedback. \n",
    "\n",
    "This node will be responsible for generating a high level plan. Keeping this plan available to our agent through the State key research_brief will allow our agent to stay on track even with complicated tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuestion(TypedDict):\n",
    "    \"\"\"Structured research question.\"\"\"\n",
    "    research_brief: str\n",
    "\n",
    "create_research_brief_prompt = \"\"\"Translate these messages into a detailed research question:\n",
    "\n",
    "<Messages>\n",
    "{messages}\n",
    "</Messages>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Guidelines:\n",
    "1. Maximize specificity and detail - include all user preferences\n",
    "2. Fill in unstated but necessary dimensions as open-ended\n",
    "3. Avoid unwarranted assumptions\n",
    "4. Use first person (from user's perspective)\n",
    "5. For product/travel research, prefer official sources\n",
    "\"\"\"\n",
    "\n",
    "async def write_research_brief(state: AgentState, config) -> Command[Literal[\"research_supervisor\"]]:\n",
    "    \"\"\"Transform user messages into a structured research brief.\"\"\"\n",
    "    # Configure model for structured output\n",
    "    research_model = (\n",
    "        get_model()\n",
    "        .with_structured_output(ResearchQuestion)\n",
    "        .with_retry(stop_after_attempt=MAX_STRUCTURED_OUTPUT_RETRIES)\n",
    "    )\n",
    "    \n",
    "    # Generate research brief\n",
    "    prompt_content = create_research_brief_prompt.format(\n",
    "        messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # Initialize supervisor\n",
    "    supervisor_system_prompt = lead_researcher_prompt.format(\n",
    "        date=get_today_str(),\n",
    "        max_concurrent_research_units=MAX_CONCURRENT_RESEARCH_UNITS,\n",
    "        max_researcher_iterations=MAX_RESEARCHER_ITERATIONS\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"research_brief\": response[\"research_brief\"],\n",
    "        \"supervisor_messages\": {\n",
    "            \"type\": \"override\",\n",
    "            \"value\": [\n",
    "                SystemMessage(content=supervisor_system_prompt),\n",
    "                HumanMessage(content=response[\"research_brief\"])\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Format Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report_generation_prompt = \"\"\"Based on all research conducted, create a comprehensive answer to:\n",
    "<Research Brief>\n",
    "{research_brief}\n",
    "</Research Brief>\n",
    "\n",
    "Today's date is {date}.\n",
    "\n",
    "Here are the findings:\n",
    "<Findings>\n",
    "{findings}\n",
    "</Findings>\n",
    "\n",
    "Create a detailed answer that:\n",
    "1. Is well-organized with proper headings (# for title, ## for sections)\n",
    "2. Includes specific facts from the research\n",
    "3. References sources using [Title](URL) format\n",
    "4. Provides comprehensive analysis\n",
    "5. Includes a \"Sources\" section at the end\n",
    "\n",
    "Structure your report appropriately:\n",
    "- For comparisons: intro → overview A → overview B → comparison → conclusion\n",
    "- For lists: just the list with details\n",
    "- For summaries: overview → key concepts → conclusion\n",
    "\n",
    "Use ## for section titles. Be thorough - users expect deep research.\n",
    "\"\"\"\n",
    "\n",
    "async def final_report_generation(state: AgentState, config):\n",
    "    \"\"\"Generate the final comprehensive research report.\"\"\"\n",
    "    notes = state.get(\"notes\", [])\n",
    "    findings = \"\\n\".join(notes)\n",
    "    \n",
    "    # Create report prompt\n",
    "    final_report_prompt = final_report_generation_prompt.format(\n",
    "        research_brief=state.get(\"research_brief\", \"\"),\n",
    "        messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "        findings=findings,\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    \n",
    "    # Generate report\n",
    "    final_report = await get_model().ainvoke([HumanMessage(content=final_report_prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"final_report\": final_report.content,\n",
    "        \"messages\": [final_report],\n",
    "        \"notes\": {\"type\": \"override\", \"value\": []}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Define Edges & Compile with Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put together our entire workflow using edges! Note that the research supervisor has been added as a node in this workflow, demonstrating how to create a subgraph as a single node.\n",
    "\n",
    "We also compile our agent with a checkpointer, which enables LangGraph's built in persistence. This enables our agent to remember previous conversations and roll back execution if a node fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_needed(state: AgentState):\n",
    "    need_elaboration = state.get(\"need_elaboration\", False)\n",
    "    if need_elaboration:\n",
    "        return \"human_input\"\n",
    "    else:\n",
    "        return \"write_research_brief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpointer for persistence\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Build complete research workflow\n",
    "deep_researcher_builder = StateGraph(\n",
    "    AgentState,\n",
    "    input_schema=AgentInputState\n",
    ")\n",
    "\n",
    "# Add nodes\n",
    "deep_researcher_builder.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "deep_researcher_builder.add_node(\"human_input\", human_input)\n",
    "deep_researcher_builder.add_node(\"write_research_brief\", write_research_brief)\n",
    "deep_researcher_builder.add_node(\"research_supervisor\", supervisor_graph)\n",
    "deep_researcher_builder.add_node(\"final_report_generation\", final_report_generation)\n",
    "\n",
    "# Add edges\n",
    "deep_researcher_builder.add_edge(START, \"clarify_with_user\")\n",
    "deep_researcher_builder.add_conditional_edges(\n",
    "    \"clarify_with_user\",\n",
    "    human_feedback_needed,\n",
    "    {\n",
    "        \"human_input\": \"human_input\",\n",
    "        \"write_research_brief\": \"write_research_brief\",\n",
    "    },\n",
    ")\n",
    "deep_researcher_builder.add_edge(\"human_input\", \"clarify_with_user\")\n",
    "deep_researcher_builder.add_edge(\"write_research_brief\", \"research_supervisor\")\n",
    "deep_researcher_builder.add_edge(\"research_supervisor\", \"final_report_generation\")\n",
    "deep_researcher_builder.add_edge(\"final_report_generation\", END)\n",
    "\n",
    "# Compile with checkpointer for interrupt/resume support\n",
    "deep_research_graph = deep_researcher_builder.compile(checkpointer=checkpointer)\n",
    "show_graph(deep_research_graph, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Test HITL and Persistence\n",
    "\n",
    "Let's test the full research workflow! We'll start by giving the agent a vague query that should trigger our agent to ask for clarifying information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete workflow\n",
    "import uuid\n",
    "\n",
    "test_query = \"Recommend best Chinese restaurants in Manhattan\"\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=test_query)]\n",
    "}\n",
    "\n",
    "# Create config with thread_id for checkpointing\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# First invocation - will pause at interrupt if clarification needed\n",
    "print(\"Starting research workflow...\")\n",
    "result = await deep_research_graph.ainvoke(initial_state, config=config)\n",
    "\n",
    "# Check if we hit an interrupt\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERRUPT: Clarification question detected\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the state to see the interrupt details\n",
    "    state_snapshot = deep_research_graph.get_state(config)\n",
    "    \n",
    "    # Show the interrupt value (the clarification question)\n",
    "    if hasattr(state_snapshot, 'tasks') and state_snapshot.tasks:\n",
    "        for task in state_snapshot.tasks:\n",
    "            if hasattr(task, 'interrupts') and task.interrupts:\n",
    "                for interrupt_info in task.interrupts:\n",
    "                    print(f\"Question: {interrupt_info.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our agent is paused mid-execution waiting for human feedback! To resume from our paused execution, we can use the same Command object we've used in our graph implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from interrupt with our response\n",
    "result = await deep_research_graph.ainvoke(Command(resume=\"No preferences. Use your best judgement, and don't ask any further questions.\"), config=config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESEARCH REPORT:\")\n",
    "print(\"=\"*60)\n",
    "print(result[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-101 (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
